
@article{wager_confidence_2014,
	title = {Confidence {Intervals} for {Random} {Forests}: {The} {Jackknife} and the {Infinitesimal} {Jackknife}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Confidence {Intervals} for {Random} {Forests}},
	url = {http://jmlr.org/papers/v15/wager14a.html},
	abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number 
B
B
 of bootstrap replicates, and working with a large 
B
B
 can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require 
B=Θ(
n
1.5
)
B=Θ(n1.5)
 bootstrap replicates to converge, where 
n
n
 is the size of the training set. We propose improved versions that only require 
B=Θ(n)
B=Θ(n)
 replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.},
	number = {48},
	urldate = {2025-01-19},
	journal = {Journal of Machine Learning Research},
	author = {Wager, Stefan and Hastie, Trevor and Efron, Bradley},
	year = {2014},
	pages = {1625--1651},
}

@article{arvesen_jackknifing_1969,
	title = {Jackknifing \${U}\$-{Statistics}},
	volume = {40},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177697287},
	doi = {10.1214/aoms/1177697287},
	language = {en},
	number = {6},
	urldate = {2025-01-19},
	journal = {The Annals of Mathematical Statistics},
	author = {Arvesen, James N.},
	month = dec,
	year = {1969},
	pages = {2076--2100},
}

@article{arcones_bootstrap_1992,
	title = {On the {Bootstrap} of \${U}\$ and \${V}\$ {Statistics}},
	volume = {20},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-20/issue-2/On-the-Bootstrap-of-U-and-V-Statistics/10.1214/aos/1176348650.full},
	doi = {10.1214/aos/1176348650},
	number = {2},
	urldate = {2025-01-19},
	journal = {The Annals of Statistics},
	author = {Arcones, Miguel A. and Gine, Evarist},
	month = jun,
	year = {1992},
}

@article{wager_estimation_2018,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	volume = {113},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839},
	doi = {10.1080/01621459.2017.1319839},
	language = {en},
	number = {523},
	urldate = {2025-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Wager, Stefan and Athey, Susan},
	month = jul,
	year = {2018},
	pages = {1228--1242},
}

@article{steele_exact_2009,
	title = {Exact bootstrap k-nearest neighbor learners},
	volume = {74},
	copyright = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-008-5096-0},
	doi = {10.1007/s10994-008-5096-0},
	language = {en},
	number = {3},
	urldate = {2025-01-19},
	journal = {Machine Learning},
	author = {Steele, Brian M.},
	month = mar,
	year = {2009},
	pages = {235--255},
}

@article{shao_general_1989,
	title = {A {General} {Theory} for {Jackknife} {Variance} {Estimation}},
	volume = {17},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-17/issue-3/A-General-Theory-for-Jackknife-Variance-Estimation/10.1214/aos/1176347263.full},
	doi = {10.1214/aos/1176347263},
	number = {3},
	urldate = {2025-01-19},
	journal = {The Annals of Statistics},
	author = {Shao, Jun and Wu, C. F. J.},
	month = sep,
	year = {1989},
}

@book{politis_subsampling_1999,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Subsampling},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-7190-1 978-1-4612-1554-7},
	url = {http://link.springer.com/10.1007/978-1-4612-1554-7},
	urldate = {2025-01-19},
	publisher = {Springer New York},
	author = {Politis, Dimitris N. and Romano, Joseph P. and Wolf, Michael},
	year = {1999},
	doi = {10.1007/978-1-4612-1554-7},
}

@book{politis_subsampling_1999-1,
	address = {New York},
	series = {Springer series in statistics},
	title = {Subsampling},
	isbn = {978-1-4612-1554-7 978-1-4612-7190-1},
	language = {eng},
	publisher = {Springer},
	author = {Politis, Dimitris N. and Romano, Joseph P. and Wolf, Michael},
	year = {1999},
}

@book{politis_subsampling_1999-2,
	address = {New York},
	series = {Springer series in statistics},
	title = {Subsampling},
	isbn = {978-1-4612-7190-1},
	language = {eng},
	publisher = {Springer},
	author = {Politis, Dimitris N. and Romano, Joseph P. and Wolf, Michael},
	year = {1999},
}

@book{politis_subsampling_1999-3,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Subsampling},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-7190-1 978-1-4612-1554-7},
	url = {http://link.springer.com/10.1007/978-1-4612-1554-7},
	urldate = {2025-01-19},
	publisher = {Springer New York},
	author = {Politis, Dimitris N. and Romano, Joseph P. and Wolf, Michael},
	year = {1999},
	doi = {10.1007/978-1-4612-1554-7},
}

@article{peng_rates_2022,
	title = {Rates of convergence for random forests via generalized {U}-statistics},
	volume = {16},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-16/issue-1/Rates-of-convergence-for-random-forests-via-generalized-U-statistics/10.1214/21-EJS1958.full},
	doi = {10.1214/21-EJS1958},
	number = {1},
	urldate = {2025-01-19},
	journal = {Electronic Journal of Statistics},
	author = {Peng, Wei and Coleman, Tim and Mentch, Lucas},
	month = jan,
	year = {2022},
}

@misc{peng_bias_2021,
	title = {Bias, {Consistency}, and {Alternative} {Perspectives} of the {Infinitesimal} {Jackknife}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2106.05918},
	doi = {10.48550/ARXIV.2106.05918},
	abstract = {Though introduced nearly 50 years ago, the infinitesimal jackknife (IJ) remains a popular modern tool for quantifying predictive uncertainty in complex estimation settings. In particular, when supervised learning ensembles are constructed via bootstrap samples, recent work demonstrated that the IJ estimate of variance is particularly convenient and useful. However, despite the algebraic simplicity of its final form, its derivation is rather complex. As a result, studies clarifying the intuition behind the estimator or rigorously investigating its properties have been severely lacking. This work aims to take a step forward on both fronts. We demonstrate that surprisingly, the exact form of the IJ estimator can be obtained via a straightforward linear regression of the individual bootstrap estimates on their respective weights or via the classical jackknife. The latter realization is particularly useful as it allows us to formally investigate the bias of the IJ variance estimator and better characterize the settings in which its use is appropriate. Finally, we extend these results to the case of U-statistics where base models are constructed via subsampling rather than bootstrapping and provide a consistent estimate of the resulting variance.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Peng, Wei and Mentch, Lucas and Stefanski, Leonard},
	year = {2021},
	note = {Version Number: 1},
	keywords = {FOS: Mathematics, Statistics Theory (math.ST)},
}

@article{ouimet_general_2021,
	title = {General {Formulas} for the {Central} and {Non}-{Central} {Moments} of the {Multinomial} {Distribution}},
	volume = {4},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2571-905X},
	url = {https://www.mdpi.com/2571-905X/4/1/2},
	doi = {10.3390/stats4010002},
	abstract = {We present the first general formulas for the central and non-central moments of the multinomial distribution, using a combinatorial argument and the factorial moments previously obtained in Mosimann (1962). We use the formulas to give explicit expressions for all the non-central moments up to order 8 and all the central moments up to order 4. These results expand significantly on those in Newcomer (2008) and Newcomer et al. (2008), where the non-central moments were calculated up to order 4.},
	language = {en},
	number = {1},
	urldate = {2025-01-19},
	journal = {Stats},
	author = {Ouimet, Frédéric},
	month = jan,
	year = {2021},
	pages = {18--27},
}

@article{lin_random_2006,
	title = {Random {Forests} and {Adaptive} {Nearest} {Neighbors}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1198/016214505000001230},
	doi = {10.1198/016214505000001230},
	language = {en},
	number = {474},
	urldate = {2025-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Yi and Jeon, Yongho},
	month = jun,
	year = {2006},
	pages = {578--590},
}

@book{lee_u-statistics_2019,
	edition = {0},
	title = {U-{Statistics}},
	isbn = {978-0-203-73452-0},
	url = {https://www.taylorfrancis.com/books/9781351405867},
	language = {en},
	urldate = {2025-01-19},
	publisher = {Routledge},
	author = {Lee, A J.},
	month = mar,
	year = {2019},
	doi = {10.1201/9780203734520},
}

@book{koroljuk_theory_1994,
	address = {Dordrecht},
	series = {Mathematics and {Its} {Applications}},
	title = {Theory of {U}-{Statistics}},
	isbn = {978-94-017-3515-5},
	abstract = {This monograph contains, for the first time, a systematic presentation of the theory of U-statistics. On the one hand, this theory is an extension of summation theory onto classes of dependent (in a special manner) random variables. On the other hand, the theory involves various statistical applications. The construction of the theory is concentrated around the main asymptotic problems, namely, around the law of large numbers, the central limit theorem, the convergence of distributions of U-statistics with degenerate kernels, functional limit theorems, estimates for convergence rates, and asymptotic expansions. Probabilities of large deviations and laws of iterated logarithm are also considered. The connection between the asymptotics of U-statistics destributions and the convergence of distributions in infinite-dimensional spaces are discussed. Various generalizations of U-statistics for dependent multi-sample variables and for varying kernels are examined. When proving limit theorems and inequalities for the moments and characteristic functions the martingale structure of U-statistics and orthogonal decompositions are used. The book has ten chapters and concludes with an extensive reference list. For researchers and students of probability theory and mathematical statistics},
	language = {eng},
	number = {273},
	publisher = {Springer},
	author = {Koroljuk, Volodymyr S. and Borovskich, Jurij V.},
	year = {1994},
	doi = {10.1007/978-94-017-3515-5},
}

@book{durrett_probability_2019,
	address = {Cambridge},
	edition = {5th ed},
	title = {Probability: {Theory} and {Examples}},
	isbn = {978-1-108-47368-2 978-1-108-59103-4},
	shorttitle = {Probability},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Durrett, Rick},
	year = {2019},
}

@book{durrett_probability_2019-1,
	address = {Cambridge},
	edition = {5th ed},
	title = {Probability: {Theory} and {Examples}},
	isbn = {978-1-108-59103-4},
	shorttitle = {Probability},
	language = {eng},
	publisher = {Cambridge University Press},
	author = {Durrett, Rick},
	year = {2019},
}

@misc{durrett_probability_2019-2,
	title = {Probability: {Theory} and {Examples}},
	shorttitle = {Probability},
	url = {https://www.cambridge.org/core/books/probability/DD9A1907F810BB14CCFF022CDFC5677A},
	abstract = {This lively introduction to measure-theoretic probability theory covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. Concentrating on results that are the most useful for applications, this comprehensive treatment is a rigorous graduate text and reference. Operating under the philosophy that the best way to learn probability is to see it in action, the book contains extended examples that apply the theory to concrete applications. This fifth edition contains a new chapter on multidimensional Brownian motion and its relationship to partial differential equations (PDEs), an advanced topic that is finding new applications. Setting the foundation for this expansion, Chapter 7 now features a proof of Itô's formula. Key exercises that previously were simply proofs left to the reader have been directly inserted into the text as lemmas. The new edition re-instates discussion about the central limit theorem for martingales and stationary sequences.},
	language = {en},
	urldate = {2025-01-19},
	journal = {Cambridge Core},
	author = {Durrett, Rick},
	month = apr,
	year = {2019},
	doi = {10.1017/9781108591034},
	note = {ISBN: 9781108591034 9781108473682
Publisher: Cambridge University Press},
}

@incollection{durrett_frontmatter_2019,
	title = {Frontmatter},
	url = {https://www.cambridge.org/core/books/probability/frontmatter/40EACB653FFB101788B7D40A8C529C48},
	abstract = {Probability - April 2019},
	language = {en},
	urldate = {2025-01-19},
	booktitle = {Probability: {Theory} and {Examples}},
	publisher = {Cambridge University Press},
	author = {Durrett, Rick},
	month = apr,
	year = {2019},
	pages = {i--vi},
}

@article{chen_randomized_2019,
	title = {Randomized incomplete \${U}\$-statistics in high dimensions},
	volume = {47},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-6/Randomized-incomplete-U-statistics-in-high-dimensions/10.1214/18-AOS1773.full},
	doi = {10.1214/18-AOS1773},
	abstract = {This paper studies inference for the mean vector of a high-dimensional \$U\$-statistic. In the era of big data, the dimension \$d\$ of the \$U\$-statistic and the sample size \$n\$ of the observations tend to be both large, and the computation of the \$U\$-statistic is prohibitively demanding. Data-dependent inferential procedures such as the empirical bootstrap for \$U\$-statistics is even more computationally expensive. To overcome such a computational bottleneck, incomplete \$U\$-statistics obtained by sampling fewer terms of the \$U\$-statistic are attractive alternatives. In this paper, we introduce randomized incomplete \$U\$-statistics with sparse weights whose computational cost can be made independent of the order of the \$U\$-statistic. We derive nonasymptotic Gaussian approximation error bounds for the randomized incomplete \$U\$-statistics in high dimensions, namely in cases where the dimension \$d\$ is possibly much larger than the sample size \$n\$, for both nondegenerate and degenerate kernels. In addition, we propose generic bootstrap methods for the incomplete \$U\$-statistics that are computationally much less demanding than existing bootstrap methods, and establish finite sample validity of the proposed bootstrap methods. Our methods are illustrated on the application to nonparametric testing for the pairwise independence of a high-dimensional random vector under weaker assumptions than those appearing in the literature.},
	number = {6},
	urldate = {2025-01-19},
	journal = {The Annals of Statistics},
	author = {Chen, Xiaohui and Kato, Kengo},
	month = dec,
	year = {2019},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62E17, 62F40, 62H15, Bernoulli sampling, Divide and conquer, Gaussian approximation, bootstrap, incomplete \$U\$-statistics, randomized inference, sampling with replacement},
	pages = {3127--3156},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2025-01-19},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {Artificial Intelligence, classification, ensemble, regression},
	pages = {5--32},
}

@book{breiman_classification_2017,
	address = {New York},
	title = {Classification and {Regression} {Trees}},
	isbn = {978-1-315-13947-0},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
	publisher = {Chapman and Hall/CRC},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, R. A. and Stone, Charles J.},
	month = oct,
	year = {2017},
	doi = {10.1201/9781315139470},
}

@article{biau_rate_2010,
	title = {On the {Rate} of {Convergence} of the {Bagged} {Nearest} {Neighbor} {Estimate}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/biau10a.html},
	language = {en},
	number = {22},
	urldate = {2025-01-19},
	journal = {Journal of Machine Learning Research},
	author = {Biau, Gérard and Cérou, Frédéric and Guyader, Arnaud},
	year = {2010},
	pages = {687--712},
}

@book{biau_lectures_2015,
	address = {Cham},
	series = {Springer {Series} in the {Data} {Sciences}},
	title = {Lectures on the {Nearest} {Neighbor} {Method}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-25386-2 978-3-319-25388-6},
	url = {http://link.springer.com/10.1007/978-3-319-25388-6},
	urldate = {2025-01-19},
	publisher = {Springer International Publishing},
	author = {Biau, Gérard and Devroye, Luc},
	year = {2015},
	doi = {10.1007/978-3-319-25388-6},
	keywords = {Density Estimation, Nearest Neighbor Method, Order Statistics, Regression Estimation, Stone's Theorem},
}

@article{biau_layered_2010,
	title = {On the layered nearest neighbour estimate, the bagged nearest neighbour estimate and the random forest method in regression and classification},
	volume = {101},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X10001387},
	doi = {10.1016/j.jmva.2010.06.019},
	abstract = {Let X1,…,Xn be identically distributed random vectors in Rd, independently drawn according to some probability density. An observation Xi is said to be a layered nearest neighbour (LNN) of a point x if the hyperrectangle defined by x and Xi contains no other data points. We first establish consistency results on Ln(x), the number of LNN of x. Then, given a sample (X,Y),(X1,Y1),…,(Xn,Yn) of independent identically distributed random vectors from Rd×R, one may estimate the regression function r(x)=E[Y{\textbar}X=x] by the LNN estimate rn(x), defined as an average over the Yi’s corresponding to those Xi which are LNN of x. Under mild conditions on r, we establish the consistency of E{\textbar}rn(x)−r(x){\textbar}p towards 0 as n→∞, for almost all x and all p≥1, and discuss the links between rn and the random forest estimates of Breiman (2001) [8]. We finally show the universal consistency of the bagged (bootstrap-aggregated) nearest neighbour method for regression and classification.},
	number = {10},
	urldate = {2025-01-19},
	journal = {Journal of Multivariate Analysis},
	author = {Biau, Gérard and Devroye, Luc},
	month = nov,
	year = {2010},
	keywords = {Bagging, Layered nearest neighbours, One nearest neighbour estimate, Random forests, Regression estimation},
	pages = {2499--2518},
}

@article{demirkaya_optimal_2024,
	title = {Optimal {Nonparametric} {Inference} with {Two}-{Scale} {Distributional} {Nearest} {Neighbors}},
	volume = {119},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2022.2115375},
	doi = {10.1080/01621459.2022.2115375},
	abstract = {The weighted nearest neighbors (WNN) estimator has been popularly used as a flexible and easy-to-implement nonparametric tool for mean regression estimation. The bagging technique is an elegant way to form WNN estimators with weights automatically generated to the nearest neighbors (Steele 2009; Biau, Cérou, and Guyader 2010); we name the resulting estimator as the distributional nearest neighbors (DNN) for easy reference. Yet, there is a lack of distributional results for such estimator, limiting its application to statistical inference. Moreover, when the mean regression function has higher-order smoothness, DNN does not achieve the optimal nonparametric convergence rate, mainly because of the bias issue. In this work, we provide an in-depth technical analysis of the DNN, based on which we suggest a bias reduction approach for the DNN estimator by linearly combining two DNN estimators with different subsampling scales, resulting in the novel two-scale DNN (TDNN) estimator. The two-scale DNN estimator has an equivalent representation of WNN with weights admitting explicit forms and some being negative. We prove that, thanks to the use of negative weights, the two-scale DNN estimator enjoys the optimal nonparametric rate of convergence in estimating the regression function under the fourth-order smoothness condition. We further go beyond estimation and establish that the DNN and two-scale DNN are both asymptotically normal as the subsampling scales and sample size diverge to infinity. For the practical implementation, we also provide variance estimators and a distribution estimator using the jackknife and bootstrap techniques for the two-scale DNN. These estimators can be exploited for constructing valid confidence intervals for nonparametric inference of the regression function. The theoretical results and appealing finite-sample performance of the suggested two-scale DNN method are illustrated with several simulation examples and a real data application.},
	number = {545},
	urldate = {2025-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Demirkaya, Emre and Fan, Yingying and Gao, Lan and Lv, Jinchi and Vossler, Patrick and Wang, Jingbo},
	month = jan,
	year = {2024},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2022.2115375},
	keywords = {Bagging, Bootstrap and jackknife, Nonparametric estimation and inference, Two-scale distributional nearest neighbors, Weighted nearest neighbors, k-nearest neighbors},
	pages = {297--307},
}

@article{lalonde_evaluating_1986,
	title = {Evaluating the {Econometric} {Evaluations} of {Training} {Programs} with {Experimental} {Data}},
	volume = {76},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/1806062},
	abstract = {This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric procedures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.},
	number = {4},
	urldate = {2025-01-19},
	journal = {The American Economic Review},
	author = {LaLonde, Robert J.},
	year = {1986},
	note = {Publisher: American Economic Association},
	pages = {604--620},
}

@article{chernozhukov_simple_2022,
	title = {A simple and general debiased machine learning theorem with finite-sample guarantees},
	volume = {110},
	issn = {1464-3510},
	url = {https://doi.org/10.1093/biomet/asac033},
	doi = {10.1093/biomet/asac033},
	abstract = {Debiased machine learning is a meta-algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e., scalar summaries, of machine learning algorithms. For example, an analyst may seek the confidence interval for a treatment effect estimated with a neural network. We present a non-asymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation and semiparametric efficiency by finite-sample arguments. The rate of convergence is {\textbackslash}n{\textasciicircum}\{-1/2\}{\textbackslash} for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill-posed inverse problems.},
	number = {1},
	journal = {Biometrika},
	author = {Chernozhukov, V and Newey, W K and Singh, R},
	month = jun,
	year = {2022},
	note = {\_eprint: https://academic.oup.com/biomet/article-pdf/110/1/257/49160070/asac033.pdf},
	pages = {257--264},
}

@misc{noauthor_simple_nodate,
	title = {simple and general debiased machine learning theorem with finite-sample guarantees {\textbar} {Biometrika} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/biomet/article/110/1/257/6608087},
	urldate = {2024-12-04},
}

@article{semenova_debiased_2021,
	title = {Debiased machine learning of conditional average treatment effects and other causal functions},
	volume = {24},
	issn = {1368-4221},
	url = {https://doi.org/10.1093/ectj/utaa027},
	doi = {10.1093/ectj/utaa027},
	abstract = {This paper provides estimation and inference methods for the best linear predictor (approximation) of a structural function, such as conditional average structural and treatment effects, and structural derivatives, based on modern machine learning tools. We represent this structural function as a conditional expectation of an unbiased signal that depends on a nuisance parameter, which we estimate by modern machine learning techniques. We first adjust the signal to make it insensitive (Neyman-orthogonal) with respect to the first-stage regularisation bias. We then project the signal onto a set of basis functions, which grow with sample size, to get the best linear predictor of the structural function. We derive a complete set of results for estimation and simultaneous inference on all parameters of the best linear predictor, conducting inference by Gaussian bootstrap. When the structural function is smooth and the basis is sufficiently rich, our estimation and inference results automatically target this function. When basis functions are group indicators, the best linear predictor reduces to the group average treatment/structural effect, and our inference automatically targets these parameters. We demonstrate our method by estimating uniform confidence bands for the average price elasticity of gasoline demand conditional on income.},
	number = {2},
	urldate = {2024-12-01},
	journal = {The Econometrics Journal},
	author = {Semenova, Vira and Chernozhukov, Victor},
	month = may,
	year = {2021},
	pages = {264--289},
}

@misc{hirano_asymptotic_2023,
	title = {Asymptotic {Representations} for {Sequential} {Decisions}, {Adaptive} {Experiments}, and {Batched} {Bandits}},
	url = {http://arxiv.org/abs/2302.03117},
	doi = {10.48550/arXiv.2302.03117},
	abstract = {We develop asymptotic approximation results that can be applied to sequential estimation and inference problems, adaptive randomized controlled trials, and other statistical decision problems that involve multiple decision nodes with structured and possibly endogenous information sets. Our results extend the classic asymptotic representation theorem used extensively in efficiency bound theory and local power analysis. In adaptive settings where the decision at one stage can affect the observation of variables in later stages, we show that a limiting data environment characterizes all limit distributions attainable through a joint choice of an adaptive design rule and statistics applied to the adaptively generated data, under local alternatives. We illustrate how the theory can be applied to study the choice of adaptive rules and end-of-sample statistical inference in batched (groupwise) sequential adaptive experiments.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Hirano, Keisuke and Porter, Jack R.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03117},
	keywords = {Economics - Econometrics},
}

@article{semenova_debiased_2021-1,
	title = {Debiased machine learning of conditional average treatment effects and other causal functions},
	volume = {24},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {1368-4221, 1368-423X},
	url = {https://academic.oup.com/ectj/article/24/2/264/5899048},
	doi = {10.1093/ectj/utaa027},
	abstract = {This paper provides estimation and inference methods for the best linear predictor (approximation) of a structural function, such as conditional average structural and treatment effects, and structural derivatives, based on modern machine learning tools. We represent this structural function as a conditional expectation of an unbiased signal that depends on a nuisance parameter, which we estimate by modern machine learning techniques. We ﬁrst adjust the signal to make it insensitive (Neyman-orthogonal) with respect to the ﬁrst-stage regularisation bias. We then project the signal onto a set of basis functions, which grow with sample size, to get the best linear predictor of the structural function. We derive a complete set of results for estimation and simultaneous inference on all parameters of the best linear predictor, conducting inference by Gaussian bootstrap. When the structural function is smooth and the basis is sufﬁciently rich, our estimation and inference results automatically target this function. When basis functions are group indicators, the best linear predictor reduces to the group average treatment/structural effect, and our inference automatically targets these parameters. We demonstrate our method by estimating uniform conﬁdence bands for the average price elasticity of gasoline demand conditional on income.},
	language = {en},
	number = {2},
	urldate = {2024-10-24},
	journal = {The Econometrics Journal},
	author = {Semenova, Vira and Chernozhukov, Victor},
	month = jun,
	year = {2021},
	pages = {264--289},
}

@article{chernozhukov_doubledebiased_2018,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	issn = {1368-4221},
	url = {https://doi.org/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	number = {1},
	urldate = {2024-10-24},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	month = feb,
	year = {2018},
	pages = {C1--C68},
}

@misc{ritzwoller_simultaneous_2024,
	title = {Simultaneous {Inference} for {Local} {Structural} {Parameters} with {Random} {Forests}},
	url = {http://arxiv.org/abs/2405.07860},
	doi = {10.48550/arXiv.2405.07860},
	abstract = {We construct simultaneous confidence intervals for solutions to conditional moment equations. The intervals are built around a class of nonparametric regression algorithms based on subsampled kernels. This class encompasses various forms of subsampled random forest regression, including Generalized Random Forests (Athey et al., 2019). Although simultaneous validity is often desirable in practice -- for example, for fine-grained characterization of treatment effect heterogeneity -- only confidence intervals that confer pointwise guarantees were previously available. Our work closes this gap. As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional U-statistics.},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Ritzwoller, David M. and Syrgkanis, Vasilis},
	month = sep,
	year = {2024},
	note = {arXiv:2405.07860},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Statistics Theory},
}

@article{biau_layered_2010-1,
	title = {On the layered nearest neighbour estimate, the bagged nearest neighbour estimate and the random forest method in regression and classification},
	volume = {101},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X10001387},
	doi = {10.1016/j.jmva.2010.06.019},
	abstract = {Let X1,…,Xn be identically distributed random vectors in Rd, independently drawn according to some probability density. An observation Xi is said to be a layered nearest neighbour (LNN) of a point x if the hyperrectangle defined by x and Xi contains no other data points. We first establish consistency results on Ln(x), the number of LNN of x. Then, given a sample (X,Y),(X1,Y1),…,(Xn,Yn) of independent identically distributed random vectors from Rd×R, one may estimate the regression function r(x)=E[Y{\textbar}X=x] by the LNN estimate rn(x), defined as an average over the Yi’s corresponding to those Xi which are LNN of x. Under mild conditions on r, we establish the consistency of E{\textbar}rn(x)−r(x){\textbar}p towards 0 as n→∞, for almost all x and all p≥1, and discuss the links between rn and the random forest estimates of Breiman (2001) [8]. We finally show the universal consistency of the bagged (bootstrap-aggregated) nearest neighbour method for regression and classification.},
	number = {10},
	urldate = {2024-10-05},
	journal = {Journal of Multivariate Analysis},
	author = {Biau, Gérard and Devroye, Luc},
	month = nov,
	year = {2010},
	keywords = {Bagging, Layered nearest neighbours, One nearest neighbour estimate, Random forests, Regression estimation},
	pages = {2499--2518},
}

@article{lin_random_2006-1,
	title = {Random {Forests} and {Adaptive} {Nearest} {Neighbors}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214505000001230},
	doi = {10.1198/016214505000001230},
	abstract = {In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-PNNs) and show that random forests can be viewed as adaptively weighted k-PNN methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-PNNs in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-PNNs of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results.},
	number = {474},
	urldate = {2024-10-05},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Yi and Jeon, Yongho},
	month = jun,
	year = {2006},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/016214505000001230},
	keywords = {Adaptive estimation, Boosting, Classification trees, Randomized trees, Regression trees},
	pages = {578--590},
}

@article{wager_condence_nodate,
	title = {Conﬁdence {Intervals} for {Random} {Forests}: {The} {Jackknife} and the {Inﬁnitesimal} {Jackknife}},
	abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the inﬁnitesimal jackknife (IJ). In practice, bagged predictors are computed using a ﬁnite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = Θ(n1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = Θ(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our ﬁndings with multiple experiments and simulation studies.},
	language = {en},
	author = {Wager, Stefan and Hastie, Trevor and Efron, Bradley},
}

@article{wager_condence_nodate-1,
	title = {Conﬁdence {Intervals} for {Random} {Forests}: {The} {Jackknife} and the {Inﬁnitesimal} {Jackknife}},
	abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the inﬁnitesimal jackknife (IJ). In practice, bagged predictors are computed using a ﬁnite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = Θ(n1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = Θ(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our ﬁndings with multiple experiments and simulation studies.},
	language = {en},
	author = {Wager, Stefan and Hastie, Trevor and Efron, Bradley},
}

@article{wager_condence_nodate-2,
	title = {Conﬁdence {Intervals} for {Random} {Forests}: {The} {Jackknife} and the {Inﬁnitesimal} {Jackknife}},
	abstract = {We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the inﬁnitesimal jackknife (IJ). In practice, bagged predictors are computed using a ﬁnite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = Θ(n1.5) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = Θ(n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our ﬁndings with multiple experiments and simulation studies.},
	language = {en},
	author = {Wager, Stefan and Hastie, Trevor and Efron, Bradley},
}

@article{song_approximating_2019,
	title = {Approximating high-dimensional infinite-order \${U}\$-statistics: {Statistical} and computational guarantees},
	volume = {13},
	issn = {1935-7524, 1935-7524},
	shorttitle = {Approximating high-dimensional infinite-order \${U}\$-statistics},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-13/issue-2/Approximating-high-dimensional-infinite-order-U-statistics--Statistical-and/10.1214/19-EJS1643.full},
	doi = {10.1214/19-EJS1643},
	abstract = {We study the problem of distributional approximations to high-dimensional non-degenerate \$U\$-statistics with random kernels of diverging orders. Infinite-order \$U\$-statistics (IOUS) are a useful tool for constructing simultaneous prediction intervals that quantify the uncertainty of ensemble methods such as subbagging and random forests. A major obstacle in using the IOUS is their computational intractability when the sample size and/or order are large. In this article, we derive non-asymptotic Gaussian approximation error bounds for an incomplete version of the IOUS with a random kernel. We also study data-driven inferential methods for the incomplete IOUS via bootstraps and develop their statistical and computational guarantees.},
	number = {2},
	urldate = {2024-10-05},
	journal = {Electronic Journal of Statistics},
	author = {Song, Yanglei and Chen, Xiaohui and Kato, Kengo},
	month = jan,
	year = {2019},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {Gaussian approximation, Infinite-order \$U\$-statistics, bootstrap, incomplete \$U\$ statistics, random forests, uncertainty quantification},
	pages = {4794--4848},
}

@article{hoeffding_class_1948,
	title = {A {Class} of {Statistics} with {Asymptotically} {Normal} {Distribution}},
	volume = {19},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-19/issue-3/A-Class-of-Statistics-with-Asymptotically-Normal-Distribution/10.1214/aoms/1177730196.full},
	doi = {10.1214/aoms/1177730196},
	abstract = {Let \$X\_1, {\textbackslash}cdot, X\_n\$ be \$n\$ independent random vectors, \$X\_{\textbackslash}nu = (X{\textasciicircum}\{(1)\}\_{\textbackslash}nu, {\textbackslash}cdots, X{\textasciicircum}\{(r)\}\_{\textbackslash}nu),\$ and \${\textbackslash}Phi(x\_1, {\textbackslash}cdots, x\_m)\$ a function of \$m({\textbackslash}leq n)\$ vectors \$x\_{\textbackslash}nu = (x{\textasciicircum}\{(1)\}\_{\textbackslash}nu , {\textbackslash}cdots, x{\textasciicircum}\{(r)\}\_{\textbackslash}nu)\$. A statistic of the form \$U = {\textbackslash}sum"{\textbackslash}Phi(X\_\{{\textbackslash}alpha 1\}, {\textbackslash}cdots, X\_\{{\textbackslash}alpha\_m\})/n(n - 1) {\textbackslash}cdots (n - m + 1),\$ where the sum \${\textbackslash}sum"\$ is extended over all permutations \$({\textbackslash}alpha\_1, {\textbackslash}cdots, {\textbackslash}alpha\_m)\$ of \$m\$ different integers, \$1 {\textbackslash}leq {\textbackslash}alpha\_i {\textbackslash}leq n\$, is called a \$U\$-statistic. If \$X\_1, {\textbackslash}cdots, X\_n\$ have the same (cumulative) distribution function (d.f.) \$F(x), U\$ is an unbiased estimate of the population characteristic \${\textbackslash}theta(F) = {\textbackslash}int {\textbackslash}cdots {\textbackslash}int{\textbackslash}Phi(x\_1, {\textbackslash}cdots, x\_m) dF(x\_1) {\textbackslash}cdots dF(x\_m). {\textbackslash}theta(F)\$ is called a regular functional of the d.f. \$F(x)\$. Certain optimal properties of \$U\$-statistics as unbiased estimates of regular functionals have been established by Halmos [9] (cf. Section 4). The variance of a \$U\$-statistic as a function of the sample size \$n\$ and of certain population characteristics is studied in Section 5. It is shown that if \$X\_1, {\textbackslash}cdots, X\_n\$ have the same distribution and \${\textbackslash}Phi(x\_1, {\textbackslash}cdots, x\_m)\$ is independent of \$n\$, the d.f. of \${\textbackslash}sqrt n(U - {\textbackslash}theta)\$ tends to a normal d.f. as \$n {\textbackslash}rightarrow {\textbackslash}infty\$ under the sole condition of the existence of \$E{\textbackslash}Phi{\textasciicircum}2(X\_1, {\textbackslash}cdots, X\_m)\$. Similar results hold for the joint distribution of several \$U\$-statistics (Theorems 7.1 and 7.2), for statistics \$U'\$ which, in a certain sense, are asymptotically equivalent to \$U\$ (Theorems 7.3 and 7.4), for certain functions of statistics \$U\$ or \$U'\$ (Theorem 7.5) and, under certain additional assumptions, for the case of the \$X\_{\textbackslash}nu\$'s having different distributions (Theorems 8.1 and 8.2). Results of a similar character, though under different assumptions, are contained in a recent paper by von Mises [18] (cf. Section 7). Examples of statistics of the form \$U\$ or \$U'\$ are the moments, Fisher's \$k\$-statistics, Gini's mean difference, and several rank correlation statistics such as Spearman's rank correlation and the difference sign correlation (cf. Section 9). Asymptotic power functions for the non-parametric tests of independence based on these rank statistics are obtained. They show that these tests are not unbiased in the limit (Section 9f). The asymptotic distribution of the coefficient of partial difference sign correlation which has been suggested by Kendall also is obtained (Section 9h).},
	number = {3},
	urldate = {2024-10-03},
	journal = {The Annals of Mathematical Statistics},
	author = {Hoeffding, Wassily},
	month = sep,
	year = {1948},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {293--325},
}

@book{biau_lectures_2015-1,
	address = {Cham},
	series = {Springer {Series} in the {Data} {Sciences}},
	title = {Lectures on the {Nearest} {Neighbor} {Method}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-25386-2 978-3-319-25388-6},
	url = {http://link.springer.com/10.1007/978-3-319-25388-6},
	urldate = {2024-10-03},
	publisher = {Springer International Publishing},
	author = {Biau, Gérard and Devroye, Luc},
	year = {2015},
	doi = {10.1007/978-3-319-25388-6},
	keywords = {Density Estimation, Nearest Neighbor Method, Order Statistics, Regression Estimation, Stone's Theorem},
}

@article{thompson_likelihood_1933,
	title = {On the {Likelihood} that {One} {Unknown} {Probability} {Exceeds} {Another} in {View} of the {Evidence} of {Two} {Samples}},
	volume = {25},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2332286},
	doi = {10.2307/2332286},
	number = {3/4},
	urldate = {2024-09-23},
	journal = {Biometrika},
	author = {Thompson, William R.},
	year = {1933},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {285--294},
}

@article{russo_tutorial_nodate,
	title = {A {Tutorial} on {Thompson} {Sampling}},
	abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally eﬃcient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not eﬀective and relations to alternative algorithms.},
	language = {en},
	author = {Russo, Daniel J and Roy, Benjamin Van and Kazerouni, Abbas and Wen, Zheng},
}

@book{vaart_asymptotic_1998,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Asymptotic {Statistics}},
	isbn = {978-0-521-78450-4},
	url = {https://www.cambridge.org/core/books/asymptotic-statistics/A3C7DAD3F7E66A1FA60E9C8FE132EE1D},
	abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
	urldate = {2024-09-23},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	year = {1998},
	doi = {10.1017/CBO9780511802256},
}

@book{cam_asymptotic_1986,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Asymptotic {Methods} in {Statistical} {Decision} {Theory}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-9369-9 978-1-4612-4946-7},
	url = {http://link.springer.com/10.1007/978-1-4612-4946-7},
	urldate = {2024-09-23},
	publisher = {Springer},
	author = {Cam, Lucien Le},
	year = {1986},
	doi = {10.1007/978-1-4612-4946-7},
	keywords = {Likelihood, Random variable, Variance, decision theory, statistics},
}

@misc{hirano_asymptotic_2023-1,
	title = {Asymptotic {Representations} for {Sequential} {Decisions}, {Adaptive} {Experiments}, and {Batched} {Bandits}},
	url = {http://arxiv.org/abs/2302.03117},
	doi = {10.48550/arXiv.2302.03117},
	abstract = {We develop asymptotic approximation results that can be applied to sequential estimation and inference problems, adaptive randomized controlled trials, and other statistical decision problems that involve multiple decision nodes with structured and possibly endogenous information sets. Our results extend the classic asymptotic representation theorem used extensively in efficiency bound theory and local power analysis. In adaptive settings where the decision at one stage can affect the observation of variables in later stages, we show that a limiting data environment characterizes all limit distributions attainable through a joint choice of an adaptive design rule and statistics applied to the adaptively generated data, under local alternatives. We illustrate how the theory can be applied to study the choice of adaptive rules and end-of-sample statistical inference in batched (groupwise) sequential adaptive experiments.},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Hirano, Keisuke and Porter, Jack R.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03117 [econ]},
	keywords = {Economics - Econometrics},
}

@article{hirano_asymptotics_2009,
	title = {Asymptotics for {Statistical} {Treatment} {Rules}},
	volume = {77},
	copyright = {© 2009 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA6630},
	doi = {10.3982/ECTA6630},
	abstract = {This paper develops asymptotic optimality theory for statistical treatment rules in smooth parametric and semiparametric models. Manski (2000, 2002, 2004) and Dehejia (2005) have argued that the problem of choosing treatments to maximize social welfare is distinct from the point estimation and hypothesis testing problems usually considered in the treatment effects literature, and advocate formal analysis of decision procedures that map empirical data into treatment choices. We develop large-sample approximations to statistical treatment assignment problems using the limits of experiments framework. We then consider some different loss functions and derive treatment assignment rules that are asymptotically optimal under average and minmax risk criteria.},
	language = {en},
	number = {5},
	urldate = {2024-09-23},
	journal = {Econometrica},
	author = {Hirano, Keisuke and Porter, Jack R.},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA6630},
	keywords = {Bayes rules, Statistical decision theory, minmax, minmax regret, semiparametric models, treatment assignment},
	pages = {1683--1701},
}

@article{mansour_inequalities_nodate,
	title = {{INEQUALITIES} {FOR} {WEIERSTRASS} {PRODUCTS}},
	language = {en},
	author = {Mansour, Touﬁk},
}

@article{wang_variance_2014,
	title = {Variance {Estimation} of a {General} {U}-{Statistic} with {Application} to {Cross}-{Validation}},
	volume = {24},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/24310980},
	abstract = {This paper addresses the problem of variance estimation for a general U-statistic. U-statistics form a class of unbiased estimators for those parameters of interest that can be written as E\{ϕ(X1,...,Xk)\}, where ϕ is a symmetric kernel function with k arguments. Although estimating the variance of a U-statistic is clearly of interest, asymptotic results for a general U-statistic are not necessarily reliable when the kernel size k is not negligible compared with the sample size n. Such situations arise in cross-validation and other nonparametric risk estimation problems. On the other hand, the exact closed form variance is complicated in form, especially when both k and n are large. We have devised an unbiased variance estimator for a general U-statistic. It can be written as a quadratic form of the kernel function ϕ and is applicable as long as k ≤ n/2. In addition, it can be represented in a familiar analysis of variance form as a contrast of between-class and within-class variation. As a further step to make the proposed variance estimator more practical, we developed a partition resampling scheme that can be used to realize the U-statistic and its variance estimator simultaneously with high computational efficiency. A data example in the context of model selection is provided. To study our estimator, we construct a U-statistic cross-validation tool, akin to the BIC criterion for model selection. With our variance estimator we can test which model has the smallest risk.},
	number = {3},
	urldate = {2024-09-05},
	journal = {Statistica Sinica},
	author = {Wang, Qing and Lindsay, Bruce},
	year = {2014},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {1117--1141},
}

@article{wang_variance_2014-1,
	title = {Variance estimation of a general u-statistic with appllication to cross-validation},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/statistica/J24N3/J24N34/J24N34.html},
	doi = {10.5705/ss.2012.215},
	abstract = {This paper addresses the problem of variance estimation for a general U-statistic. U-statistics form a class of unbiased estimators for those parameters of interest that can be written as E \{ϕ(X1, . . . , Xk)\}, where ϕ is a symmetric kernel function with k arguments. Although estimating the variance of a U-statistic is clearly of interest, asymptotic results for a general U-statistic are not necessarily reliable when the kernel size k is not negligible compared with the sample size n. Such situations arise in cross-validation and other nonparametric risk estimation problems. On the other hand, the exact closed form variance is complicated in form, especially when both k and n are large. We have devised an unbiased variance estimator for a general U-statistic. It can be written as a quadratic form of the kernel function ϕ and is applicable as long as k ≤ n/2. In addition, it can be represented in a familiar analysis of variance form as a contrast of between-class and within-class variation. As a further step to make the proposed variance estimator more practical, we developed a partition resampling scheme that can be used to realize the U-statistic and its variance estimator simultaneously with high computational eﬃciency. A data example in the context of model selection is provided. To study our estimator, we construct a U-statistic cross-validation tool, akin to the bic criterion for model selection. With our variance estimator we can test which model has the smallest risk.},
	language = {en},
	urldate = {2024-09-05},
	journal = {Statistica Sinica},
	author = {Wang, Qing and Lindsay, Bruce G.},
	year = {2014},
}

@article{ouimet_general_2021-1,
	title = {General {Formulas} for the {Central} and {Non}-{Central} {Moments} of the {Multinomial} {Distribution}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2571-905X},
	url = {https://www.mdpi.com/2571-905X/4/1/2},
	doi = {10.3390/stats4010002},
	abstract = {We present the first general formulas for the central and non-central moments of the multinomial distribution, using a combinatorial argument and the factorial moments previously obtained in Mosimann (1962). We use the formulas to give explicit expressions for all the non-central moments up to order 8 and all the central moments up to order 4. These results expand significantly on those in Newcomer (2008) and Newcomer et al. (2008), where the non-central moments were calculated up to order 4.},
	language = {en},
	number = {1},
	urldate = {2024-09-02},
	journal = {Stats},
	author = {Ouimet, Frédéric},
	month = mar,
	year = {2021},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {central moments, higher moments, multinomial distribution, non-central moments},
	pages = {18--27},
}
