\section{Stuff}

\begin{thm}[\citet{ritzwoller_uniform_2024} - Corollary 4.1]
	Let $\Sigma$ be the diagonal matrix with components $\sigma_{b, j}^2$.
	\begin{enumerate}
		\item Under the same conditions as Theorem \ref{ritzwoller_Thm4_2}, we have that
		      \begin{equation}
			      \sqrt{\frac{n}{b^2}} \Sigma^{-1 / 2} \bar{U}_{n, b}\left(\mathbf{x}^{(p)}\right) \lesssim \log ^{1 / 2}(d n)+\frac{\phi \log ^2(d n)}{\underline{\sigma}_b n^{1 / 2}}+\xi_{n, b}
		      \end{equation}
		      with probability greater than $1-C / n$.
		\item Let $Z$ denote a centered Gaussian random vector with covariance matrix $\Var\left(\tilde{u}^{(1)}\left(\boldsymbol{x}^{(d)}, D_i\right)\right)$.
		      Under the same conditions as Theorem \ref{ritzwoller_Thm4_2}, we have that
		      \begin{equation}
			      \sup _{\mathrm{R} \in \mathcal{R}}\left|P\left\{\sqrt{\frac{n}{b^2}} \Sigma^{-1 / 2} \bar{U}_{n, b}\left(\boldsymbol{x}^{(d)}\right) \in \mathrm{R}\right\}-P\left\{\Sigma^{-1 / 2} Z \in \mathrm{R}\right\}\right|
			      \lesssim\left(\frac{\phi^2 \log ^5(d n)}{\sigma_b^2 n}\right)^{1 / 4}+\xi_{n, b} \sqrt{\log (d)},
		      \end{equation}
		      where $\mathcal{R}$ denotes the set of hyper-rectangles in $\mathbb{R}^d$.
	\end{enumerate}
\end{thm}

\citet{peng_rates_2022}
{\color{red} Combine Theorem 1 and a variation of the argument for Proposition 1 to improve rate for point-wise inference.
	Better rate of subsampling scale with sample size.}


\begin{thm}[Theorem 1 - \citet{peng_rates_2022}]\label{PengThm1}
	Let $\mathbf{Z}_1, \ldots, \mathbf{Z}_n$ be i.i.d. from $P$ and $U_{n, s, \omega}$ be a generalized complete $U$-statistic with kernel $h\left(\mathbf{Z}_1, \ldots, \mathbf{Z}_s ; \omega\right)$.
	Let $\theta=\E[h], \zeta_{1, \omega}=\Var\left(\E\left[h \mid Z_1\right]\right)$ and $\zeta_s=\operatorname{var}(h)$.

	If $\frac{s}{n} \frac{\zeta_s}{s \zeta_{1, \omega}} \rightarrow 0$, then
	\begin{equation}
		\frac{U_{n, s, \omega}-\theta}{\sqrt{s^2 \zeta_{1, \omega} / n}} \rightsquigarrow N(0,1) .
	\end{equation}
\end{thm}
In a remark to this Theorem, the authors state that when the variance ratio $\frac{\zeta_s}{s\zeta_{1,\omega}}$ is bounded, choosing $s = o(n)$ is sufficient to ensure that Theorem \ref{PengThm1} applies.
A more directly applicable result comes from Proposition 2 of the same paper.


\begin{thm}[Proposition 2 - \citet{peng_rates_2022}]
	Let $\mathbf{Z}_1, \ldots, \mathbf{Z}_s$ denote i.i.d. pairs of random variables $\left(\mathbf{X}_i, Y_i\right)$.
	Suppose $Y_i=\mu\left(X_i\right)+\varepsilon_i$ where $\mu$ is bounded, $\varepsilon$ has mean 0 and variance $\sigma^2$, and $\mathbf{X}_i$ and $\varepsilon_i$ are independent.
	Let $\varphi=\sum_{i=1}^s w(\mathbf{x}, \mathbf{X}_i, \mathfrak{X}) Y_i$ such that $\sum_{i=1}^s w(\mathbf{x}, \mathbf{X}_i, \mathfrak{X})=1$, where $\mathfrak{X}$ denotes $\left\{X_i\right\}_{i=1}^m$.
	Then
	\begin{equation}
		\limsup _{s \rightarrow \infty} \frac{\zeta_s}{s^2 \zeta_1}
		< \infty.
	\end{equation}
\end{thm}

Following from the representation of the WNN estimator as an L-statistic in \citet{steele_exact_2009}, both the DNN and TDNN estimators are of this form.
Thus, we can deduce the following.
\begin{equation}
	\limsup _{s \rightarrow \infty} \frac{1}{s^2} \frac{\Var\left(h_{s}\left(\mathbf{x}; D_{\ell}\right)\right)}{\Cov\left(h_s\left(\mathbf{x}; D_{\ell}\right), h_s\left(\mathbf{x}; D_{\ell'}\right)\right)}
	< \infty
\end{equation}
where $D_{\ell}$ and $D_{\ell'}$ only share exactly one observation, say $\mathbf{Z}_1$.

\section{Hoeffding-Decompositions}



\hrule

{\color{red} Idea 2:}

By restricting Theorem 4.2 from \citet{ritzwoller_uniform_2024} to the univariate case, i.e. considering only a single point of interest, we obtain the following corollary.
\begin{cor}\mbox{}\\*
	Consider a generic order $s$ U-statistic
	\begin{equation}
		U_{s}(\mathbf{x}; \mathbf{D}_n)
		= \binom{n}{s}^{-1} \sum_{\ell \in L_{n,s}}u(\mathbf{x}; D_{\ell})
		\quad \text{with} \quad
		U_{s}(\mathbf{x}; \mathbf{D}_n) = \theta(\mathbf{x})
	\end{equation}
	and the Kernel of its Haj\'ek projection
	\begin{equation}
		u^{(1)}\left(\mathbf{x}; \mathbf{z}\right)
		= \E\left[u(\mathbf{x}; D_{[s]}) \, | \, \mathbf{Z}_1 = \mathbf{z}\right] - \theta(\mathbf{x}).
	\end{equation}
	Furthermore, define the following variances.
	\begin{equation}
		\nu^2 = \Var\left(u(\mathbf{x}; D_{[s]})\right), \quad
		\sigma_{s}^2 = \Var\left(u^{(1)}(\mathbf{x}; \mathbf{Z})\right), \quad
		\psi_{s}^2 = \nu^2 - s \sigma_{s}^2
	\end{equation}
	If the kernel function $u\left(\mathbf{x} ; D_{\ell}\right)$ satisfies the bound
	$\left\|u(\mathbf{x}; D_{\ell})\right\|_{\psi_1} \leq \phi,$
	then
	\begin{align}
		\sqrt{\frac{n}{{s}^2 \sigma_{s}^2}}
		\left(u(\mathbf{x}; \mathbf{D}_n) - \theta(\mathbf{x}) - \frac{b}{n} \sum_{i=1}^n u^{(1)}(\mathbf{x}; \mathbf{Z}_{i})\right)
		%
		= \sqrt{\frac{n}{{s}^2 \sigma_{s}^2}} \operatorname{HR}(\mathbf{x}; \mathbf{D}_n)
		%
		\lesssim \xi_{n, s},
		\quad \text {where} \\
		\xi_{n, s}
		= \left(\frac{C s \log(n)}{n}\right)^{s / 2}\left(\left(\frac{n \psi_{s}^2}{{s}^2 \sigma_{s}^2}\right)^{1 / 2}+\left(\frac{\phi^2 s \log ^4(n)}{\sigma_{s}^2}\right)^{1 / 2}\right),
	\end{align}
	with probability greater than $1-C / n$.
\end{cor}

We want to use this bound to improve on the subsampling bounds derived in the proofs of Theorems 5 and 6 from \cite{demirkaya_optimal_2024}.
These can be found in their Supplementary Material, parts D.5 and D.6.

\hrule

First, as introduced by \citet{peng_rates_2022}, the concept of generalized U-statistics.
\begin{dfn}[Generalized U-Statistic]\label{Gen_UStat}\mbox{}\\*
	Suppose $Z_1, \ldots, Z_n$ are i.i.d. samples from $F_Z$ and let $h$ denote a (possibly randomized) real-valued function utilizing $s$ of these samples that is permutation symmetric in those $s$ arguments. A generalized $U$-statistic with kernel $h$ of order (rank) s refers to any estimator of the form
	\begin{equation}
		U_{n, s, N, \omega}=\frac{1}{\hat{N}} \sum_{(n, s)} \rho h\left(Z_{i 1}, \ldots, Z_{i s} ; \omega\right)
	\end{equation}
	where $\omega$ denotes i.i.d. randomness, independent of the original data.
	The $\rho$ denotes i.i.d. Bernoulli random variables determining which subsamples are selected and $\operatorname{Pr}(\rho=1)=N /\binom{n}{s}$.
	The actual number of subsamples selected is given by $\hat{N}=\sum \rho$ where $\E[\hat{N}]=N$.
	When $N=\binom{n}{s}$, the estimator in Eq. (2.4) is a generalized complete $U$-statistic and is denoted as $U_{n, s, \omega}$.
	When $N<\binom{n}{s}$, these estimators are generalized incomplete $U$-statistics.
\end{dfn}

\hrule

\begin{proof}[Proof of Lemma \ref{lem:Cov_TDNN_k}]
	Following the idea of the proof of Lemma 7 in \citet{demirkaya_optimal_2024}, we want to find an upper and lower asymptotic bound for $\zeta_{s_1, s_2}^{1}$ to pin down its asymptotic order.
	To construct a lower bound we again follow a similar strategy as used in the authors' proof.
	Recalling Lemma \ref{lem:psi_s1s2_1} and the conditional independence properties of the individual summands we can observe the following.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{1}
			 & = \Var_{1}(\psi_{s_1, s_2}^{1}\left(\mathbf{x}; \mathbf{Z}_1\right))                                                                                                               \\
			%
			 & \geq \Var\left(w_{1}^{*}\frac{s_2}{s_1}\varepsilon_1 \E_{D'}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1^{\prime}, D'\right)\, \Big| \, \mathbf{X}_1^{\prime} = \mathbf{X}_1 \right]
			+ w_{2}^{*}\varepsilon_1 \E_D\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, \Big| \, \mathbf{X}_1\right]\right)                                                               \\
			%
			 & = \sigma_{\varepsilon}^{2}\cdot
			\Var\left(w_{1}^{*}\frac{s_2}{s_1}\E_{D'}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1^{\prime}, D'\right)\, \Big| \, \mathbf{X}_1^{\prime} = \mathbf{X}_1 \right]
			+ w_{2}^{*}\E_D\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, \Big| \, \mathbf{X}_1\right]\right)                                                                             \\
		\end{aligned}
	\end{equation}
	Let now $B_D$ denote the event that observation $\mathbf{Z}_1$ is the closest to the point of interest $\mathbf{x}$ in a data set $D$, i.e. $\rk(\mathbf{x}; \mathbf{Z}_1, D) = 1$.
	Then we can continue as follows.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{1}
			\geq \sigma_{\varepsilon}^{2}\cdot \Var\left(w_{1}^{*}\frac{s_2}{s_1}\E_{D'}\left[B_{D'}\, \Big| \, \mathbf{X}_1^{\prime} = \mathbf{X}_1 \right]
			+ w_{2}^{*}\E_D\left[B_D\, \Big| \, \mathbf{X}_1\right]\right)\\
			%
			\geq \sigma_{\varepsilon}^{2}\cdot \Var\left(w_{1}^{*}\E_{D'}\left[B_{D'}\, \Big| \, \mathbf{X}_1^{\prime} = \mathbf{X}_1 \right]
			+ w_{2}^{*}\E_D\left[B_D\, \Big| \, \mathbf{X}_1\right]\right)
		\end{aligned}
	\end{equation}
	Next, notice the following fact.
	\begin{equation}
		\forall \mathbf{x}_1 \in \mathbb{R}^d: \quad 
		0
		\leq \E_D\left[B_D\, \Big| \, \mathbf{x}_1\right] 
		\leq \E_D\left[B_{D_{[s_1]}}\, \Big| \, \mathbf{x}_1\right] 
		= \E_{D'}\left[B_{D'}\, \Big| \, \mathbf{X}_1^{\prime} = \mathbf{x}_1\right] 
		\leq 1
	\end{equation}
	Thus, we can find the following, where $D^{\prime \prime}$ denotes a third independent data set of i.i.d. observations drawn from $P$ of size $2s_2 - 1$.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{1}
			& \geq \sigma_{\varepsilon}^{2}\cdot \Var\left(\E_D\left[B_D\, \Big| \, \mathbf{X}_1\right]\right)
			= \sigma_{\varepsilon}^{2}\cdot \left(
				\E\left[\left(\E_D\left[B_D\, \Big| \, \mathbf{X}_1\right]\right)^2\right]
				- \left(\E\left[\E\left[B_D\, \Big| \, \mathbf{X}_1\right]\right]\right)^2
			\right)\\
			%
			& = \sigma_{\varepsilon}^{2}\cdot \left(
				\E\left[\E_{D^{\prime \prime}}\left[B_{D^{\prime \prime}}\, \Big| \, \mathbf{X}_1\right]\right]
				- s_{2}^{-2}
			\right)
			= \sigma_{\varepsilon}^{2}\cdot \left(\frac{1}{2s_2 - 1} - \frac{1}{s_2^2}\right) 
			\sim s_2^{-1}.
		\end{aligned}
	\end{equation}

	To construct an upper bound, consider the following.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right)
			 & = \Cov\left(
			h_{s_1, s_2}\left(\mathbf{x}; \mathbf{Z}_1, \mathbf{Z}_2, \ldots, \mathbf{Z}_{s_2}\right),
			h_{s_1, s_2}\left(\mathbf{x}; \mathbf{Z}_1, \mathbf{Z}_{2}^{\prime}, \ldots, \mathbf{Z}_{s_2}^{\prime}\right)
			\right)                                                  \\
			%
			 & = \E_{1}\left[\Cov_{(2:s_2), (2:s_2)^{\prime}}\left(
				h_{s_1, s_2}\left(\mathbf{x}; D\right),
				h_{s_1, s_2}\left(\mathbf{x}; D^{\prime}\right)
				\, \Big| \, D_1 = D_{1}^{\prime} = \mathbf{Z}_1\right)
			\right]                                                  \\
			%
			 & \leq \E_{1}\left[\E_{(2:s_2), (2:s_2)^{\prime}}\left[
			|h_{s_1, s_2}\left(\mathbf{x}; D\right)| \cdot
			|h_{s_1, s_2}\left(\mathbf{x}; D^{\prime}\right)|
			\, \Big| \, D_1 = D_{1}^{\prime} = \mathbf{Z}_1\right]
			\right]                                                  \\
			%
			 & = \E_{1}\left[\left(\E_{2:s_2}\left[
					h_{s_1, s_2}\left(\mathbf{x}; D\right)
					\, \Big| \, D_1 = \mathbf{Z}_1\right]\right)^{2}
			\right]                                                  \\
			%
			 & = \E_{1}\left[\left(\E_{2:s_2}\left[
					w_{1}^{*} \tilde{\mu}_{s_1}\left(\mathbf{x}; D\right) + w_{2}^{*} h_{s_2}\left(\mathbf{x}; D\right)
					\, \Big| \, D_1 = \mathbf{Z}_1\right]\right)^{2}
			\right]                                                  \\
			%
			 & = \E_{1}\left[\left(
				w_{1}^{*} \E_{2:s_2}\left[\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
				+ w_{2}^{*} \E_{2:s_2}\left[h_{s_2}\left(\mathbf{x}; D\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
				\right)^{2}
				\right]
		\end{aligned}
	\end{equation}
	This gives us two conditional expectations to analyze individually.
	\begin{equation}
		A := \E_{2:s_2}\left[\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
		\quad \text{and} \quad
		B := \E_{2:s_2}\left[h_{s_2}\left(\mathbf{x}; D\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
	\end{equation}
	We thus obtain the following expression.
	\begin{equation}
		\zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right)
		\leq \E_{1}\left[\left(w_{1}^{*} A	+ w_{2}^{*} B\right)^{2}\right]
		= \left(w_{1}^{*}\right)^2 \E_{1}\left[A^2\right]
		+ 2 w_{1}^{*}w_{2}^{*} \E_{1}\left[AB\right]
		+ \left(w_{2}^{*}\right)^2 \E_{1}\left[B^2\right]
	\end{equation}
	We can use this to find an upper bound for $\zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right)$ by considering the terms individually.
	\begin{equation}
		\begin{aligned}
			A
			 & := \E_{2:s_2}\left[\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
			= \E_{2:s_2}\left[
			\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}}h_{s_1}\left(\mathbf{x}; D_{\ell}\right)\, \Big| \, D_1 = \mathbf{Z}_1
			\right]                                                                                                                                                                     \\
			%
			 & = \binom{s_2}{s_1}^{-1}\left(\E_{2:s_2}\left[
				\sum_{\ell \in L_{s_1}\left([s_2] \backslash \{1\}\right)}h_{s_1}\left(\mathbf{x}; D_{\ell}\right)\right]
			+ \E_{2:s_2}\left[
				\sum_{\ell \in L_{s_1 - 1}\left([s_2] \backslash \{1\}\right)}h_{s_1}\left(\mathbf{x}; D_{\ell \cap 1}\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
			\right)                                                                                                                                                                     \\
			%
			 & = \binom{s_2}{s_1}^{-1}\left(
			\binom{s_2 - 1}{s_1}\E_{2:(s_1 + 1)}\left[h_{s_1}\left(\mathbf{x}; D_{2:(s_1 + 1)}\right)\right]
			+ \binom{s_2 - 1}{s_1 - 1}\E_{2:s_1}\left[h_{s_1}\left(\mathbf{x}; D_{[s_1]}\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
			\right)                                                                                                                                                                     \\
			%
			 & = \binom{s_2}{s_1}^{-1}\left(
			\binom{s_2 - 1}{s_1}\E_{2:(s_1 + 1)}\left[\sum_{i = 2}^{s_1 + 1}\kappa(\mathbf{x}; \mathbf{Z}_i, D_{2:(s_1 + 1)}) Y_i\right]
			+ \binom{s_2 - 1}{s_1 - 1}\E_{2:s_1}\left[\sum_{i = 1}^{s_1}\kappa(\mathbf{x}; \mathbf{Z}_i, D_{[s_1]}) Y_i\, \Big| \, D_1 = \mathbf{Z}_1\right]
			\right)                                                                                                                                                                     \\
			%
			 & = \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1} s_1 \E_{2:(s_1 + 1)}\left[\kappa(\mathbf{x}; \mathbf{Z}_2, D_{2:(s_1 + 1)}) Y_2\right]                                       \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}\left(
			(s_1 - 1)\E_{2:s_1}\left[\kappa(\mathbf{x}; \mathbf{Z}_2, D_{[s_1]}) Y_2\, \Big| \, D_1 = \mathbf{Z}_1\right]
			+ \mu(\mathbf{X}_1)\E_{2:s_1}\left[\kappa(\mathbf{x}; \mathbf{Z}_1, D_{[s_1]})\, \Big| \, D_1 = \mathbf{Z}_1\right]
			\right)                                                                                                                                                                     \\
			%
			 & = \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1}
			\E_{2}\left[\mu(\mathbf{X}_2) \, s_1 \, \E_{3:(s_1+1)}\left[\kappa(\mathbf{x}; \mathbf{Z}_2, D_{2:(s_1 + 1)}) \, | \, \mathbf{Z}_2\right]\right]                            \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}
			\E_{2}\left[\mu(\mathbf{X}_2) \, (s_1 - 1) \, \E_{3:s_1}\left[\kappa(\mathbf{x}; \mathbf{Z}_2, D_{[s_1]}) \, | \, \mathbf{Z}_2\right] \, \Big| \, D_1 = \mathbf{Z}_1\right] \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}\mu(\mathbf{X}_1)
			\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}                                                                    \\
			%
			 & = \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1}
			\E_{2}\left[\mu(\mathbf{X}_2) \, s_1 \, \left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}\right]                     \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}
			\E_{2}\left[\mu(\mathbf{X}_2) \, (s_1 - 1) \,
				\1\left(\|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right)
			\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_1 - 2} \, \Big| \, D_1 = \mathbf{Z}_1\right]                              \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}\mu(\mathbf{X}_1)
			\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}                                                                    \\
			%
			 & = \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1}
			\E_{2}\left[\mu(\mathbf{X}_2) \, s_1 \, \left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}\right]                     \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}
			\P\left(\|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right)                                                                                                \\
			 & \quad \quad \cdot
			\E_{2}\left[\mu(\mathbf{X}_2) \, (s_1 - 1) \,
				\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_1 - 2}
			\, \Big| \, \|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right]                                                                                            \\
			 & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}\mu(\mathbf{X}_1)
			\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}
		\end{aligned}
	\end{equation}
	Concerning the second conditional expectation, we find the following.
	\begin{equation}
		\begin{aligned}
			B
			 & := \E_{2:s_2}\left[h_{s_2}\left(\mathbf{x}; D\right)\, \Big| \, D_1 = \mathbf{Z}_1\right]
			= \E_{2:s_2}\left[\sum_{i = 1}^{s_2} \kappa(\mathbf{x}; \mathbf{Z}_i, D) Y_i \, \Big| \, D_1 = \mathbf{Z}_1\right]              \\
			%
			 & = \E_{2:s_2}\left[\kappa(\mathbf{x}; \mathbf{Z}_1, D) Y_1 \, \Big| \, D_1 = \mathbf{Z}_1\right]
			+ \E_{2:s_2}\left[\sum_{i = 2}^{s_2} \kappa(\mathbf{x}; \mathbf{Z}_i, D) Y_i \, \Big| \, D_1 = \mathbf{Z}_1\right]              \\
			%
			 & = \mu(\mathbf{X}_1) \left(1 - \varphi\left(B\left(\mathbf{X}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_2 - 1}
			+ (s_2 - 1)\E_{2:s_2}\left[\kappa(\mathbf{x}; \mathbf{Z}_2, D) Y_2 \, \Big| \, D_1 = \mathbf{Z}_1\right]                        \\
			%
			 & = \mu(\mathbf{X}_1) \left(1 - \varphi\left(B\left(\mathbf{X}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_2 - 1}
			+ \E_{2}\left[
				\mu(\mathbf{X}_2) \, (s_2 - 1) \, \E_{3:s_2}\left[\kappa(\mathbf{x}; \mathbf{Z}_2, D)\, | \, \mathbf{Z}_2\right]
			\, \Big| \, D_1 = \mathbf{Z}_1\right]                                                                                           \\
			%
			 & = \mu(\mathbf{X}_1) \left(1 - \varphi\left(B\left(\mathbf{X}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_2 - 1} \\
			 & \quad + \E_{2}\left[
				\mu(\mathbf{X}_2)\, (s_2 - 1) \,
				\1\left(\|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right)
				\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_2 - 2}
			\, \Big| \, D_1 = \mathbf{Z}_1\right]                                                                                           \\
			%
			 & = \mu(\mathbf{X}_1) \left(1 - \varphi\left(B\left(\mathbf{X}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_2 - 1} \\
			 & \quad + \P\left(\|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right)\E_{2}\left[
				\mu(\mathbf{X}_2)\, (s_2 - 1) \,
				\left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_2 - 2}
				\, \Big| \, \|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right]
		\end{aligned}
	\end{equation}

	Thus, the terms of interest take the following forms.
	\begin{equation}
		\begin{aligned}
			A^2
			 & = \left(\binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1}
			 \E_{2}\left[\mu(\mathbf{X}_2) \, s_1 \, \left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}\right] \right.                    \\ 
			  & \quad + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}
			 \P\left(\|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right)                                                                                                \\
			  &  \quad \quad \cdot
			 \E_{2}\left[\mu(\mathbf{X}_2) \, (s_1 - 1) \,
				 \left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_2 - \mathbf{x} \|\right)\right)\right)^{s_1 - 2}
			 \, \Big| \, \|\mathbf{X}_2 - \mathbf{x}\| < \|\mathbf{X}_1 - \mathbf{x}\|\right]                                                                                            \\
			  & \quad \left. + \binom{s_2}{s_1}^{-1}\binom{s_2 - 1}{s_1 - 1}\mu(\mathbf{X}_1)
			 \left(1 - \varphi\left(B\left(\mathbf{x}, \| \mathbf{X}_1 - \mathbf{x} \|\right)\right)\right)^{s_1 - 1}         \right)^2               \\
			%
			& = \binom{s_2}{s_1}^{-2}\\
			%
			 & {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}
	\begin{equation}
		\begin{aligned}
			AB
			 & =                         \\
			%
			 & {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}
	\begin{equation}
		\begin{aligned}
			B^2
			 & =                         \\
			%
			 & {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}

	It remains to analyze the expectation with respect to the first observation and its asymptotic bahvior.
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[A^2\right]
			 & =                         \\
			%
			 & {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[AB\right]
			 & =                         \\
			%
			 & {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[B^2\right]
			 & =                         \\
			%
			 & {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}

	{\color{red} LOREM IPSUM}
\end{proof}