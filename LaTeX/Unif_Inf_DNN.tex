\documentclass[letterpaper,10pt]{article}
\usepackage[letterpaper, hmargin=1.8cm, vmargin = 2.5cm]{geometry} %Sets the page geometry
\usepackage{url}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage[english]{babel}
\usepackage{setspace}
\setlength{\parskip}{1em} % Set space when paragraphs are used
\setlength{\parindent}{0in}

\usepackage{xcolor}
\usepackage{multicol}
\usepackage{float}
\usepackage{graphicx} % Package for \includegraphics
\usepackage{wrapfig} % Figure wrapping
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{amssymb}
\usepackage{amsmath}
\numberwithin{equation}{section}
%%%%% Boxes Setup %%%%%
\usepackage[many]{tcolorbox}    	% for COLORED BOXES (tikz and xcolor included)
\definecolor{main}{HTML}{dedcdc}    % setting main color to be used
\definecolor{sub}{HTML}{dedcdc}     % setting sub color to be used
\tcbset{
    sharp corners,
    colback = white,
    before skip = 0.3cm,    % add extra space before the box
    after skip = 0.3cm      % add extra space after the box
}                           % setting global options for tcolorbox
\newtcolorbox{boxD}{
    colback = sub, 
    colframe = main, 
    boxrule = 0pt, 
    toprule = 0pt, % top rule weight
    bottomrule = 0pt % bottom rule weight
}

\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{bbm}
\usepackage[autostyle]{csquotes}
\usepackage{doi}
\usepackage[
    backend=biber,
    style=authoryear-comp,
    natbib=true,
    sortlocale=en_US,
    url=false, 
    doi=true,
    eprint=false
]{biblatex}

\addbibresource{bibliography.bib}

% Other
\newtheorem{thm}{Theorem}
\numberwithin{thm}{section}
\newtheorem{dfn}{Definition}
\newtheorem{lem}{Lemma}
\numberwithin{lem}{section}
\newtheorem{rmk}{Remark}
\newtheorem{exmp}{Example}
\newtheorem{cor}{Corollary}
\numberwithin{cor}{section}
% \newtheorem{prop}{Proposition}
\newtheorem{asm}{Assumption}

\renewcommand\qedsymbol{$\blacksquare$}

\newcommand{\sumi}{\sum_{i=1}^n}
\newcommand{\calE}{\mathcal{E}}
\makeatother
\renewcommand{\hat}{\widehat}
\newcommand{\E}{\mathbb{E}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\rk}{\text{rk}}
\renewcommand{\P}{\mathbbm{P}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\DeclarePairedDelimiter\floor{\lfloor}{\rfloor} %Floor function
\DeclarePairedDelimiter\ceil{\lceil}{\rceil} %Ceil function
\DeclareMathOperator*{\argmax}{arg\,max} % argmax
\DeclareMathOperator*{\argmin}{arg\,min} % argmin
\newcommand{\indep}{\perp\!\!\!\!\perp} 


\begin{document}
\singlespacing
\title{Simultaneous Inference for Conditional Average Treatment Effects using \\ Distributional Nearest Neighbor Estimation}
\date{Last edited: \today}
\author{Jakob R. Juergens \\ University of Wisconsin - Madison}
\maketitle
\hrule
\onehalfspacing
\begin{abstract}
	This paper presents a computationally simple method of estimating heterogeneous treatment effects based on the Two-Scale Distributional Nearest Neighbor (TDNN) estimator of \citet{demirkaya_optimal_2024}.
	As part of this analysis, I improve on conditions required for consistent variance estimation presented in the original paper and provide results for asymptotically valid pointwise inference in a nonparametric regression setup and extend the analysis to the estimation of conditional average treatment effects.
	Building on the framework of \citet{ritzwoller_uniform_2024}, I develop uniformly valid confidence bands for the TDNN estimator.
	I then show how to apply these to perform uniformly valid inference in both the nonparametric regression setup and the heterogeneous treatment effect setup.
	A main contribution is the development of a computationally simple method that leverages the theoretical results of the aforementioned papers.
	% \blfootnote{The author thanks Harold D. Chiang, Jack Porter {\color{red} ADD OTHERS} for invaluable feedback.}
\end{abstract}
\vspace{0.3cm}
\hrule
\singlespacing

\vspace{-0.3cm}
\begin{center}
	{\small Supplementary Material and R Package available at: {\color{red} Work in Progress}
	% \url{https://github.com/JakobJuergens/Unif_Inf_TDNN}
	}
\end{center}
\vspace{0.3cm}
\hrule
\singlespacing
% {\small \tableofcontents}
\thispagestyle{empty}

\pagenumbering{arabic}
\onehalfspacing

\newpage
\input{Parts/01_Introduction.tex}

\newpage
\input{Parts/02_Setup.tex}

\newpage
\input{Parts/03_TDNN.tex}

\newpage
\input{Parts/04_PW_Inf.tex}

\newpage
\input{Parts/05_Unif_Inf.tex}

\newpage
\input{Parts/06_Simulations.tex}

\newpage
\input{Parts/07_Application.tex}

\newpage
\input{Parts/08_Conclusion.tex}

\newpage
\printbibliography

\newpage
\appendix

\newpage
\input{Parts/A1_UStat.tex}

\newpage
\input{Parts/A2_Helpful_Results.tex}


\newpage
\section{Proofs for Results in Section \ref{sec:TDNN}}
\hrule

\newpage
\section{Proofs for Results in Section \ref{sec:pw_inf}}
\hrule

\subsection{Closed Form Representations}
\hrule

\begin{proof}[Proof of Theorem \ref{thm:JK_closed_form}]\mbox{}\\*
	Recall the closed form representation of the DNN estimator as presented in Equation \ref{eq:DNN_closed_form}.
	\begin{equation}
		\tilde{\mu}_{s}(\mathbf{x}; \mathbf{D}_n)
		= \binom{n}{s}^{-1} \sum_{i = 1}^{n - s + 1}\binom{n - i}{s - 1}Y_{(i)}
	\end{equation}
	Plugging into the Jackknife variance estimator for the DNN estimator now gives us the following where we assume that $n$ is sufficiently large for $n - s + 1$ to be larger than $s$.
	\begin{equation}
		\begin{aligned}
			\hat{\omega}_{\text{JK}}^{2}
			 & = \frac{n - 1}{n}\sum_{i = 1}^{n} \left(\tilde{\mu}_{s}(\mathbf{x}; \mathbf{D}_{n, -i}) - \tilde{\mu}_{s}(\mathbf{x}; \mathbf{D}_n)\right)^2 \\
			%
			 & = \frac{n - 1}{n}\left\{\sum_{i = 1}^{s} \left(
			\binom{n - 1}{s}^{-1}\left(\sum_{j = 1}^{i - 1} \binom{n - j - 1}{s - 1}Y_{(j)}
			+ \sum_{j = i + 1}^{n - s + 1}\binom{n - j}{s - 1}Y_{(j)}\right)
			- \binom{n}{s}^{-1} \sum_{j = 1}^{n - s + 1} \binom{n - j}{s - 1}Y_{(j)}
			\right)^2 \right.                                                                                                                               \\
			 & \left. \quad \quad +
			\sum_{i = s + 1}^{n} \left(
			\binom{n - 1}{s}^{-1}\sum_{j = 1}^{n - s + 1} \binom{n - j - 1}{s - 1}Y_{(j)}
			- \binom{n}{s}^{-1} \sum_{j = 1}^{n - s + 1} \binom{n - j}{s - 1}Y_{(j)}
			\right)^2\right\}                                                                                                                               \\
			%
			& = 
		\end{aligned}
	\end{equation}
	The closed form of the Jackknife variance estimator for the TDNN estimator follows from the same approach.

	{\color{red} LOREM IPSUM}
\end{proof}

\newpage
\subsection{Kernel (Conditional) Expectations}
\hrule

\begin{lem}[DNN Kernel Expecctation]\label{lem:DNN_k_exp}\mbox{}\\*
	Let $\mathbf{x}$ denote a point of interest.
	Then
	\begin{equation}
		\E_D\left[h_s\left(\mathbf{x}; D\right)\right]
		= \E_{1}\left[\mu\left(\mathbf{X}_1\right) s\left(1 - \psi\left(B\left(\mathbf{x}, \|\mathbf{X}_1 - \mathbf{x}\|\right)\right)\right)^{s-1}\right]\\
		\longrightarrow \mu(\mathbf{x}) \quad \text{as} \quad s \rightarrow \infty
	\end{equation}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:DNN_k_exp}]
	\begin{equation}
		\begin{aligned}
			\E_D\left[h_s\left(\mathbf{x}; D\right)\right]
			 & = \E_D\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) Y_i\right]
			= s \E_{1}\left[Y_1 \E_{2:s}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, | \, \mathbf{X}_1 \right]\right]                                                                \\
			%
			 & = \E_{1}\left[\left(\mu\left(\mathbf{X}_1\right) + \varepsilon_1\right) s\left(1 - \psi\left(B\left(\mathbf{x}, \|\mathbf{X}_1 - \mathbf{x}\|\right)\right)\right)^{s-1}\right] \\
			%
			 & = \E_{1}\left[\mu\left(\mathbf{X}_1\right) s\left(1 - \psi\left(B\left(\mathbf{x}, \|\mathbf{X}_1 - \mathbf{x}\|\right)\right)\right)^{s-1}\right]                              \\
			%
			 & \longrightarrow \mu(\mathbf{x}) \quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

\begin{lem}[DNN Haj\'ek Kernel Expectation]\label{lem:psi_s_1}\mbox{}\\*
	Let $\mathbf{z}_1 = (\mathbf{x}_1, y_1)$ denote a specific realization of $\mathbf{Z}$ and $\mathbf{x}$ denote a point of interest.
	Then
	\begin{equation}
		\psi_{s}^{1}\left(\mathbf{x}; \mathbf{z}_1\right)
		= \varepsilon_1 \E_D\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
		+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) \mu(\mathbf{X}_i)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
	\end{equation}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:psi_s_1}]
	\begin{equation}
		\begin{aligned}
			\psi_{s}^{1}\left(\mathbf{x}; \mathbf{z}_1\right)
			 & = \E_{D}\left[h_{s}\left(\mathbf{x}; D\right) \, | \, \mathbf{Z}_1 = \mathbf{z}_1 \right]
			= \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) Y_i \, \Big| \, \mathbf{Z}_1 = \mathbf{z}_1 \right]  \\
			%
			 & = \E_{D}\left[\left(\mu(\mathbf{x}_1) + \varepsilon_1\right)\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)
			+ \sum_{i = 2}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) \mu(\mathbf{X}_i)\, \Big| \, \mathbf{Z}_1 = \mathbf{z}_1 \right] \\
			%
			 & = \varepsilon_1 \E_D\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) \mu(\mathbf{X}_i)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

% \begin{lem}[TDNN Haj\'ek Kernel Expectation]\label{lem:psi_s1s2_1}\mbox{}\\*
% 	Let $\mathbf{z}_1 = (\mathbf{x}_1, y_1)$ denote a specific realization of $\mathbf{Z}$ and $\mathbf{x}$ denote a point of interest.
% 	Let $D = \left\{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{s_2} \right\}$ and $D' = \left\{\mathbf{Z}_1^{\prime}, \dotsc, \mathbf{Z}_{s_1}^{\prime} \right\}$ denote two independent and i.i.d. samples drawn from $P$.
% 	Furthermore, let $\mathbf{X} \sim P$ and $\mathbf{X} \indep D,D'$.
% 	Then
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(\mathbf{x}; \mathbf{z}_1\right)
% 			 & = w_{1}^{*} \left(
% 			\frac{s_2}{s_1}\left(\varepsilon_1 \E_{D'}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1^{\prime}, D'\right)\, \Big| \, \mathbf{X}_1^{\prime} = \mathbf{x}_1 \right]
% 				+ \E_{D'}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i^{\prime}, D'\right) \mu(\mathbf{X}_i^{\prime})\, \Big| \, \mathbf{X}_1 ^{\prime}= \mathbf{x}_1 \right]\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(\mathbf{X})\right]\right)                                                                                          \\
% 			 & \quad + w_{2}^{*} \left(\varepsilon_1 \E_D\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
% 			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) \mu(\mathbf{X}_i)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]\right) \\
% 		\end{aligned}
% 	\end{equation}
% \end{lem}

% \begin{proof}[Proof of Lemma \ref{lem:psi_s1s2_1}]
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(\mathbf{x}; \mathbf{z}_1\right)
% 			 & = \E_{D}\left[w_{1}^{*} \tilde{\mu}_{s_1}\left(\mathbf{x}; D\right)
% 				+ w_{2}^{*} h_{s_2}\left(\mathbf{x}; D\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]
% 			= \E_{D}\left[h_{s_1, s_2}\left(\mathbf{x}; D\right) \, | \, \mathbf{Z}_1 = \mathbf{z}_1 \right]                                                             \\
% 			%
% 			 & = w_{1}^{*} \E_{D}\left[\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]
% 			+ w_{2}^{*} \E_D\left[h_{s_2}\left(\mathbf{x}; D\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]                                                            \\
% 			%
% 			 & = w_{1}^{*} \E_{D}\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}}h_{s_1}\left(\mathbf{x}; D_\ell\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]
% 			+ w_{2}^{*} \E_D\left[h_{s_2}\left(\mathbf{x}; D\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]                                                            \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(\E_{D}\left[
% 				\sum_{\ell \in L_{s_1 - 1}\left([s_2]\backslash \{1\}\right)}h_{s_1}\left(\mathbf{x}; D_{\ell \cup 1}\right)
% 				\sum_{\ell \in L_{s_1}\left([s_2]\backslash \{1\}\right)}h_{s_1}\left(\mathbf{x}; D_\ell\right)
% 				\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]
% 			\right)                                                                                                                                                      \\
% 			 & \quad + w_{2}^{*} \E_D\left[h_{s_2}\left(\mathbf{x}; D\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]                                                   \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(
% 			\binom{s_2-1}{s_1-1}\E_{1:s_1}\left[h_{s_1}\left(\mathbf{x}; D_{[s_1]}\right) \, | \, \mathbf{Z}_1 = \mathbf{z}_1 \right]
% 			+ \binom{s_2-1}{s_1}\E_{2:(s_1+1)}\left[h_{s_1}\left(\mathbf{x}; D_{2:(s_1+1)}\right)\right]
% 			\right)                                                                                                                                                      \\
% 			 & \quad + w_{2}^{*} \E_D\left[h_{s_2}\left(\mathbf{x}; D\right)\, | \, \mathbf{Z}_1 = \mathbf{z}_1\right]                                                   \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(
% 			\binom{s_2-1}{s_1-1}\psi_{s_1}^{1}\left(\mathbf{x}; \mathbf{z_1}\right)
% 			+ \binom{s_2-1}{s_1}\E_{2:(s_1+1)}\left[h_{s_1}\left(\mathbf{x}; D_{2:(s_1+1)}\right)\right]
% 			\right) + w_{2}^{*} \psi_{s_2}^{1}\left(\mathbf{x}; \mathbf{z_1}\right)
% 		\end{aligned}
% 	\end{equation}
% 	Using Lemmas \ref{lem:DNN_k_exp} and \ref{lem:psi_s_1}, we can further simplify this term significantly.
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(\mathbf{x}; \mathbf{z}_1\right)
% 			 & = w_{1}^{*} \left(\frac{s_2}{s_1}\psi_{s_1}^{1}\left(\mathbf{x}; \mathbf{z_1}\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(\mathbf{X})\right]\right)
% 			+ w_{2}^{*} \psi_{s_2}^{1}\left(\mathbf{x}; \mathbf{z_1}\right)                                                                                       \\
% 			%
% 			 & = w_{1}^{*} \left(
% 			\frac{s_2}{s_1}\left(\varepsilon_1 \E_{D'}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D'\right)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
% 				+ \E_{D'}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D'\right) \mu(\mathbf{X}_i)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(\mathbf{X})\right]\right)                                                                                          \\
% 			 & \quad + w_{2}^{*} \left(\varepsilon_1 \E_D\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]
% 			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(\mathbf{x}; \mathbf{Z}_i, D\right) \mu(\mathbf{X}_i)\, \Big| \, \mathbf{X}_1 = \mathbf{x}_1 \right]\right) \\
% 			%
% 			 & \quad = {\color{red} LOREM IPSUM}
% 		\end{aligned}
% 	\end{equation}
% \end{proof}

\newpage
\subsection{Kernel Variances \& Covariances}
\hrule

\begin{lem}[Adapted from \citet{demirkaya_optimal_2024}]\label{lem:omega_s}\mbox{}\\*
	Let $D = \{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{s}\}$ be a vector of i.i.d. random variables drawn from $P$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}\left(\mathbf{x}\right)
		= \E\left[h_{s}^{2}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,  \mathbf{Z}_{s}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}\left(\mathbf{x}\right)
		= \E_1\left[\mu^2\left(\mathbf{X}_1\right) s \E_{2:s}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_1, D\right)\right]\right] + \sigma_{\varepsilon}^2
		\lesssim \mu^2(\mathbf{x}) + \sigma_{\varepsilon}^2 + o(1)
		\quad \text{as} \quad s \rightarrow \infty.
	\end{equation}
\end{lem}

\hrule

\begin{lem}\label{lem:omega_sc}\mbox{}\\*
	Let $D = \{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{s}\}$ be a vector of i.i.d. random variables drawn from $P$.
	Let $D^{\prime} = \{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{c}, \mathbf{Z}_{c+1}^{\prime}, \dotsc,  \mathbf{Z}_{s}^{\prime}\}$ where $\mathbf{Z}_{c+1}^{\prime}, \dotsc,  \mathbf{Z}_{s}^{\prime}$ are i.i.d. draws from $P$ that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}^{c}\left(\mathbf{x}\right)
		= \E\left[h_{s}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_{c}, \mathbf{Z}_{c+1}, \ldots, \mathbf{Z}_{s}\right) \cdot
			h_{s}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,\mathbf{Z}_{c}, \mathbf{Z}_{c+1}^{\prime}, \ldots, \mathbf{Z}_{s}^{\prime}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}^{c}\left(\mathbf{x}\right)
		\lesssim \frac{s^2 + cs  - c^2}{s^2} \mu^2(\mathbf{x}) + (c/s) \sigma_{\varepsilon}^2 + o(1)
		\quad \text{for} \quad s \quad \text{sufficiently large}
	\end{equation}
	and thus
	\begin{equation}
		\Omega_{s}^{c}\left(\mathbf{x}\right)
		\lesssim \mu^2(\mathbf{x}) + o(1)
		\quad \text{as} \quad s \rightarrow \infty.
	\end{equation}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:omega_sc}]
	\begin{equation}
		\begin{aligned}
			\Omega_{s}^{c}\left(\mathbf{x}\right)
			 & = \E\left[h_{s}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_{c}, \mathbf{Z}_{c+1}, \ldots, \mathbf{Z}_{s}\right) \cdot
			h_{s}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,\mathbf{Z}_{c}, \mathbf{Z}_{c+1}^{\prime}, \ldots, \mathbf{Z}_{s}^{\prime}\right)\right]                                                                \\
			%
			 & = \E_{D, D'}\left[
				\left(\sum_{i = 1}^{s}\kappa\left(\mathbf{x}; \mathbf{Z}_{i}, D\right)Y_{i}\right)
				\left(\sum_{j = 1}^{c}\kappa\left(\mathbf{x}; \mathbf{Z}_{j}, D^{\prime}\right)Y_{j}
				+ \sum_{j = c+1}^{s}\kappa\left(\mathbf{x}; \mathbf{Z}_{j}^{\prime}, D^{\prime}\right)Y_{j}^{\prime}\right)
			\right]                                                                                                                                                                                             \\
			%
			 & = \E_{D, D'}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c}\kappa\left(\mathbf{x}; \mathbf{Z}_{i}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{j}, D^{\prime}\right)Y_{i}Y_{j}\right]
			+  \E_{D, D'}\left[\sum_{i = 1}^{c}\sum_{j = c+1}^{s}\kappa\left(\mathbf{x}; \mathbf{Z}_{i}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{j}^{\prime}, D^{\prime}\right)Y_{i}Y_{j}^{\prime}\right]   \\
			 & \quad + \E_{D, D'}\left[\sum_{i = c+1}^{s}\sum_{j = 1}^{c}\kappa\left(\mathbf{x}; \mathbf{Z}_{i}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{j}, D^{\prime}\right)Y_{i}Y_{j}\right]
			+  \E_{D, D'}\left[\sum_{i = c+1}^{s}\sum_{j = c+1}^{s}\kappa\left(\mathbf{x}; \mathbf{Z}_{i}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{j}^{\prime}, D^{\prime}\right)Y_{i}Y_{j}^{\prime}\right] \\
			%
			 & = \E_{D, D'}\left[c \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D^{\prime}\right)Y_{1}^{2}\right]
			+ \E_{D, D'}\left[c(s-c) \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)Y_{1}Y_{c+1}^{\prime}\right]                           \\
			 & \quad + \E_{D, D'}\left[c(s-c) \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D^{\prime}\right)Y_{c+1}Y_{1}\right]                                    \\
			 & \quad + \E_{D, D'}\left[(s-c)^2 \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)Y_{c+1}Y_{c+1}^{\prime}\right]
		\end{aligned}
	\end{equation}
	Starting from this decomposition, we will analyze the terms one by one.
	First, by honesty and an application of Lemma \ref{lem:dem12}, we find the following.
	\begin{equation}
		\begin{aligned}
			\E_{D, D'}\left[c\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D^{\prime}\right)Y_{1}^{2}\right]
			 & = \E_{D, D'}\left[c\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D^{\prime}\right)\left(\mu^2(\mathbf{X}_1) + \sigma_{\varepsilon}^{2}\right)\right]                   \\
			%
			 & = \E_{1}\left[\left(\mu^2(\mathbf{X}_1) + \sigma_{\varepsilon}^{2}\right) c\E_{2:s}\left[\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D^{\prime}\right)\right]\right] \\
			%
			 & = \E_{1}\left[\left(\mu^2(\mathbf{X}_1) + \sigma_{\varepsilon}^{2}\right) c\left\{1 - \varphi\left(B\left(\mathbf{x}, \|\mathbf{X}_1 - \mathbf{x}\|\right)\right)\right\}^{2s-c-1}\right]                        \\
			%
			 & \leq (c/s) \E_{1}\left[\left(\mu^2(\mathbf{X}_1) + \sigma_{\varepsilon}^{2}\right) s\left\{1 - \varphi\left(B\left(\mathbf{x}, \|\mathbf{X}_1 - \mathbf{x}\|\right)\right)\right\}^{s-1}\right]                  \\
			%
			 & \lesssim (c/s)\left(\mu^2(\mathbf{x}) + \sigma_{\varepsilon}\right) + o(1)
		\end{aligned}
	\end{equation}
	Similarly, we can find that:
	\begin{equation}
		\begin{aligned}
			 & \E_{D, D'}\left[c(s-c) \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)Y_{1}Y_{c+1}^{\prime}\right]                                          \\
			%
			 & \quad = \E_{D, D'}\left[\mu(\mathbf{X}_1)\mu(\mathbf{X}_{c+1}^{\prime}) \, c(s-c) \, \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)\right] \\
			%
			 & \quad \leq \E_{D}\left[|\mu(\mathbf{X}_1)| \, c \, \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\right]
			\E_{D'}\left[|\mu(\mathbf{X}_{c+1}^{\prime})| \, (s-c) \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                               \\
			% 
			 & \quad = \frac{c (s-c)}{s^2} \leq \E_{D}\left[|\mu(\mathbf{X}_1)| \, s \, \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\right]
			\E_{D'}\left[|\mu(\mathbf{X}_{c+1}^{\prime})| \, s \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                   \\
			%
			 & \quad = \frac{c (s-c)}{s^2} \left(\E_{D}\left[|\mu(\mathbf{X}_1)| \, s \, \kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D\right)\right]\right)^2                                                                       \\
			% 
			 & \quad = \frac{c (s-c)}{s^2} \left(\E_{1}\left[|\mu(\mathbf{X}_1)| s\left\{1 - \varphi\left(B\left(\mathbf{x}, \|\mathbf{X}_1 - \mathbf{x}\|\right)\right)\right\}^{s-1}\right]\right)^2                          \\
			%
			 & \quad \lesssim \frac{c(s-c)}{s^2}\mu^2(\mathbf{x}) + o(1)
		\end{aligned}
	\end{equation}
	Following analogous steps, we find the same result for the third term.
	\begin{equation}
		\begin{aligned}
			\E_{D, D'}\left[c(s-c) \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{1}, D^{\prime}\right)Y_{c+1}Y_{1}\right]
			\lesssim \frac{c(s-c)}{s^2}\mu^2(\mathbf{x}) + o(1)
		\end{aligned}
	\end{equation}
	The fourth term can be asymptotically bounded in the following way.
	\begin{equation}
		\begin{aligned}
			 & \E_{D, D'}\left[(s-c)^2 \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)Y_{c+1}Y_{c+1}^{\prime}\right]                                           \\
			%
			 & \quad = \E_{D, D'}\left[\mu(\mathbf{X}_{c+1})\mu(\mathbf{X}_{c+1}^{\prime})\, (s-c)^2 \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)\right] \\
			%
			 & \quad \leq \E_{D}\left[|\mu(\mathbf{X}_{c+1})|\, (s-c) \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\right]
			\E_{D'}\left[|\mu(\mathbf{X}_{c+1}^{\prime})|\, (s-c) \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                      \\
			%
			 & \quad = \frac{(s-c)^2}{s^2} \E_{D}\left[|\mu(\mathbf{X}_{c+1})|\, s \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\right]
			\E_{D'}\left[|\mu(\mathbf{X}_{c+1}^{\prime})|\, s \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                          \\
			%
			 & \quad = \frac{(s-c)^2}{s^2} \left(\E_{D}\left[|\mu(\mathbf{X}_{c+1})|\, s \, \kappa\left(\mathbf{x}; \mathbf{Z}_{c+1}, D\right)\right]\right)^2                                                                        \\
			%
			 & \quad \lesssim \frac{(s-c)^2}{s^2}\mu^2(\mathbf{x}) + o(1)
		\end{aligned}
	\end{equation}
	The result of Lemma \ref{lem:omega_sc} follows immediately.
\end{proof}

\newpage
\begin{lem}\label{lem:upsilon_s}\mbox{}\\*
	Let $D = \{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{s_2}\}$ be a vector of i.i.d. random variables drawn from $P$ for $s_2 > s_1$.
	Furthermore, let
	\begin{equation}
		\Upsilon_{s_1, s_2}\left(\mathbf{x}\right)
		= \E\left[h_{s_1}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,  \mathbf{Z}_{s_1}\right) \cdot
			h_{s_2}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,\mathbf{Z}_{s_1}, \ldots, \mathbf{Z}_{s_2}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Upsilon_{s_1, s_2}\left(\mathbf{x}\right)
		\lesssim \mu^{2}\left(\mathbf{x}\right) + \sigma^2_{\varepsilon} + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < c_1 \leq s_1 / s_2 \leq c_2 < 1.
	\end{equation}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:upsilon_s}]
	\begin{equation}
		\begin{aligned}
			\Upsilon_{s_1, s_2}\left(\mathbf{x}\right)
			 & = \E\left[h_{s_1}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,  \mathbf{Z}_{s_1}\right) \cdot
			h_{s_2}\left(\mathbf{x}; \mathbf{Z}_1, \ldots,\mathbf{Z}_{s_1}, \ldots, \mathbf{Z}_{s_2}\right)\right]                                                                             \\
			%
			 & = \E_{D}\left[
				\left(\sum_{i = 1}^{s_1} \kappa(\mathbf{x}; \mathbf{Z}_i, D_{[s_1]})Y_i\right)
				\left(\sum_{j = 1}^{s_1}\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_j + \sum_{j = s_1 + 1}^{s_2}\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_j\right)
			\right]                                                                                                                                                                            \\
			%
			 & = \E_{D}\left[\sum_{i = 1}^{s_1} \kappa(\mathbf{x}; \mathbf{Z}_i, D) Y_i^2\right]
			+ \E_{D}\left[\sum_{i = 1}^{s_1}\sum_{j = s_1 + 1}^{s_2}\kappa(\mathbf{x}; \mathbf{Z}_i, D_{[s_1]})\kappa(\mathbf{x}; \mathbf{Z}_j, D) Y_i Y_j\right]                              \\
			%
			 & = \E_{D}\left[Y_1^2 \, s_1 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]
			+ \E_{D}\left[Y_{1} Y_{s_2} \, s_1 (s_2 - s_1) \, \kappa(\mathbf{x}; \mathbf{Z}_1, D_{[s_1]})\kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right]                                        \\
			%
			 & = \E_{D}\left[\left(\mu^2(\mathbf{X}_1) + \sigma^2_{\varepsilon}\right) \, s_1 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]
			+ \E_{D}\left[\mu(\mathbf{X}_1) \mu(\mathbf{X}_{s_2}) \, s_1 (s_2 - s_1) \, \kappa(\mathbf{x}; \mathbf{Z}_1, D_{[s_1]})\kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right]              \\
			%
			 & = \frac{s_1}{s_2}\E_{D}\left[\left(\mu^2(\mathbf{X}_1) + \sigma^2_{\varepsilon}\right) \, s_1 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]
			+ \frac{s_2 - s_1}{s_2}\E_{D}\left[\mu(\mathbf{X}_1) \mu(\mathbf{X}_{s_2}) \, s_1 s_2 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D_{[s_1]})\kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right] \\
			%
			 & \leq \frac{s_1}{s_2} \E_{D}\left[\left(\mu^2(\mathbf{X}_1) + \sigma^2_{\varepsilon}\right) \, s_2 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]                                 \\
			 & \quad \quad + \frac{s_2 - s_1}{s_2}\E_{D}\left[|\mu(\mathbf{X}_1)| \, s_1 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D_{[s_1]})\right]
			\E_{D}\left[|\mu(\mathbf{X}_{s_2})| \, s_2 \, \kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right]                                                                                       \\
			%
			 & \lesssim \mu^{2}\left(\mathbf{x}\right) + \sigma^2_{\varepsilon} + o(1).
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

\begin{lem}\label{lem:upsilon_sc}\mbox{}\\*
	Let $D = \{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{s_2}\}$ be a vector of i.i.d. random variables drawn from $P$ for $s_2 > s_1$.
	Let $D^{\prime} = \{\mathbf{Z}_1, \dotsc, \mathbf{Z}_{c}, \mathbf{Z}_{c+1}^{\prime}, \dotsc,  \mathbf{Z}_{s_1}^{\prime}\}$ where $\mathbf{Z}_{c+1}^{\prime}, \dotsc,  \mathbf{Z}_{s_1}^{\prime}$ are i.i.d. draws from $P$ that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Upsilon_{s_1, s_2}^{c}\left(\mathbf{x}\right)
		= \E\left[h_{s_1}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_c, \mathbf{Z}^{\prime}_{c+1}, \ldots,  \mathbf{Z}^{\prime}_{s_1}\right) \cdot
			h_{s_2}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_{s_2}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\begin{aligned}
			 & \Upsilon_{s_1, s_2}^{c}\left(\mathbf{x}\right)
			\lesssim \frac{c s_2 - c^2 + s_1 s_2}{s_1 s_2}\mu^2(\mathbf{x}) + (c/s_1) \sigma^2_{\varepsilon} + o(1) \\
			%
			 & \text{for} \quad s_1, s_2 \quad \text{sufficiently large}
			\quad \text{with} \quad
			0 < c_1 \leq s_1 / s_2 \leq c_2 < 1
		\end{aligned}
	\end{equation}
	and thus
	\begin{equation}
		\Upsilon_{s_1, s_2}^{c}\left(\mathbf{x}\right)
		\lesssim \mu^2(\mathbf{x}) + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < c_1 \leq s_1 / s_2 \leq c_2 < 1.
	\end{equation}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:upsilon_sc}]
	\begin{equation}
		\begin{aligned}
			\Upsilon_{s_1, s_2}^{c}\left(\mathbf{x}\right)
			 & = \E\left[h_{s_1}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_c, \mathbf{Z}^{\prime}_{c+1}, \ldots,  \mathbf{Z}^{\prime}_{s_1}\right) \cdot
			h_{s_2}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_{s_2}\right)\right]                                                                                                    \\
			%                                                                     
			 & = \E_{D, D'}\left[
				\left(\sum_{i = 1}^{c} \kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime})Y_i + \sum_{i = c+1}^{s_1} \kappa(\mathbf{x}; \mathbf{Z}_i^{\prime}, D^{\prime})Y_i^{\prime}\right)
				\left(\sum_{j = 1}^{c} \kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_j + \sum_{j = c+1}^{s_2}\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_j \right)
			\right]                                                                                                                                                                          \\
			%
			 & = \E_{D, D'}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c} \kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_i Y_j\right]
			+ \E_{D, D'}\left[\left(\sum_{i = 1}^{c}\kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime}) Y_i\right) \left(\sum_{j = c+1}^{s_2} \kappa(\mathbf{x}; \mathbf{Z}_j, D) Y_j\right)\right] \\
			 & \quad + \E_{D, D'}\left[\sum_{i = c+1}^{s_1}\sum_{j = 1}^{c} \kappa(\mathbf{x}; \mathbf{Z}_i^{\prime}, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_i^{\prime} Y_j\right]
			+ \E_{D, D'}\left[\left(\sum_{i = c+1}^{s_1}\kappa(\mathbf{x}; \mathbf{Z}_i^{\prime}, D^{\prime}) Y_i^{\prime}\right)
				\left(\sum_{j = c+1}^{s_2} \kappa(\mathbf{x}; \mathbf{Z}_j, D) Y_j\right)\right]
		\end{aligned}
	\end{equation}
	Again, we have four terms to analyze individually.
	\begin{equation}
		\begin{aligned}
			\E_{D, D'}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c} \kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_i Y_j\right]
			 & = \E_{D, D'}\left[\sum_{i = 1}^{c} Y_{i}^2 \kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_i, D)\right]                                                            \\
			%
			 & = \E_{D, D'}\left[Y_{1}^2 \, c \, \kappa(\mathbf{x}; \mathbf{Z}_1, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]                                                                     \\
			%
			 & = \E_{D, D'}\left[\left(\mu^2(\mathbf{X}_{1}) + \sigma^2_{\varepsilon}\right) \, c \, \kappa(\mathbf{x}; \mathbf{Z}_1, D_{[c]})\kappa(\mathbf{x}; \mathbf{Z}_1, D^{\prime}_{c+1:s_1})\right] \\
			%
			 & = \E_{D}\left[\left(\mu^2(\mathbf{X}_{1}) + \sigma^2_{\varepsilon}\right) \, c \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]                                                                 \\
			%
			 & = \frac{c}{s_1} \E_{D}\left[\left(\mu^2(\mathbf{X}_{1}) + \sigma^2_{\varepsilon}\right) \, s_1 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]                                                 \\
			%
			 & \lesssim (c/s_1)(\mu^2(\mathbf{x}) + \sigma^2_{\varepsilon}) + o(1)
		\end{aligned}
	\end{equation}
	Considering the second term, we find the following.
	\begin{equation}
		\begin{aligned}
			 & \E_{D, D'}\left[\left(\sum_{i = 1}^{c}\kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime}) Y_i\right)
			\left(\sum_{j = c+1}^{s_2} \kappa(\mathbf{x}; \mathbf{Z}_j, D) Y_j\right)\right]                                                                                          \\                                                                                                                                                                         \\
			%
			 & \quad = \E_{D, D'}\left[\sum_{i = 1}^{c}\sum_{j = c+1}^{s_2}Y_i Y_j \kappa(\mathbf{x}; \mathbf{Z}_i, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_j, D)\right]             \\
			%
			 & \quad = \E_{D, D'}\left[c(s_2 - c) \, Y_1 Y_{s_1} \kappa(\mathbf{x}; \mathbf{Z}_1, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right]                           \\
			%
			 & \quad = \frac{c (s_2 - c)}{s_1 s_2}\E_{D, D'}\left[Y_1 Y_{s_2} \, s_1 s_2 \,\kappa(\mathbf{x}; \mathbf{Z}_1, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right] \\
			%
			 & \quad \leq \frac{c (s_2 - c)}{s_1 s_2}
			\E_{D'}\left[|\mu(\mathbf{X}_1)| \, s_1  \,\kappa(\mathbf{x}; \mathbf{Z}_1, D^{\prime})\right]
			\E_{D}\left[|\mu(\mathbf{X}_{s_2})| \, s_2  \,\kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)\right]                                                                              \\
			%
			 & \quad \lesssim \frac{c(s_2 - c)}{s_1 s_2} \mu^2(\mathbf{x}) + o(1)
		\end{aligned}
	\end{equation}
	Similarly, by simplifying the third term, we find the following.
	\begin{equation}
		\begin{aligned}
			 & \E_{D, D'}\left[\sum_{i = c+1}^{s_1}\sum_{j = 1}^{c} \kappa(\mathbf{x}; \mathbf{Z}_i^{\prime}, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_j, D)Y_i^{\prime} Y_j\right]                                                \\
			%
			 & \quad = \E_{D, D'}\left[Y_{s_1}^{\prime} Y_{1} \, (s_1 - c)c \, \kappa(\mathbf{x}; \mathbf{Z}_{s_1}^{\prime}, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]                                                 \\
			% 
			 & \quad = \frac{(s_1 - c)c}{s_1 s_2}\E_{D, D'}\left[\mu(\mathbf{X}_{s_1}^{\prime})\mu(\mathbf{X}_1) \, s_1 s_2 \, \kappa(\mathbf{x}; \mathbf{Z}_{s_1}^{\prime}, D^{\prime})\kappa(\mathbf{x}; \mathbf{Z}_1, D)\right] \\
			%
			 & \quad \leq \frac{(s_1 - c)c}{s_1 s_2}
			\E_{D}\left[|\mu(\mathbf{X}_{s_1}^{\prime})| \, s_1 \, \kappa(\mathbf{x}; \mathbf{Z}_{s_1}^{\prime}, D^{\prime})\right]
			\E_{D}\left[|\mu(\mathbf{X}_1)| \, s_2 \, \kappa(\mathbf{x}; \mathbf{Z}_1, D)\right]                                                                                                                                   \\
			%
			 & \quad \lesssim \frac{(s_1 - c)c}{s_1 s_2} \mu^2(\mathbf{x}) + o(1)
		\end{aligned}
	\end{equation}
	Lastly, concerning the fourth term, observe the following.
	\begin{equation}
		\begin{aligned}
			 & \E_{D, D'}\left[\left(\sum_{i = c+1}^{s_1}\kappa(\mathbf{x}; \mathbf{Z}_i^{\prime}, D^{\prime}) Y_i^{\prime}\right)
			\left(\sum_{j = c+1}^{s_2} \kappa(\mathbf{x}; \mathbf{Z}_j, D) Y_j\right)\right]                                                                                                                                                         \\
			%
			 & \quad = \E_{D, D'}\left[\sum_{i = c+1}^{s_1}\sum_{j = c+1}^{s_2}\kappa(\mathbf{x}; \mathbf{Z}_i^{\prime}, D^{\prime}) \kappa(\mathbf{x}; \mathbf{Z}_j, D)  Y_i^{\prime}Y_j\right]                                                     \\
			%
			 & \quad = \E_{D, D'}\left[\mu(\mathbf{X}_{s_1}^{\prime}) \mu(\mathbf{X}_{s_2}) \, (s_1 - c)(s_2 -c) \,\kappa(\mathbf{x}; \mathbf{Z}_{s_1}^{\prime}, D^{\prime}) \kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)  \right]                        \\                                                                                                                                                                       \\
			%
			 & \quad = \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}\E_{D, D'}\left[\mu(\mathbf{X}_{s_1}^{\prime}) \mu(\mathbf{X}_{s_2}) \, s_1 s_2 \,\kappa(\mathbf{x}; \mathbf{Z}_{s_1}^{\prime}, D^{\prime}) \kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)  \right] \\                                                                                                                                                                       \\
			%
			 & \quad \leq \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}
			\E_{D'}\left[|\mu(\mathbf{X}_{s_1}^{\prime})| \, s_1 \,\kappa(\mathbf{x}; \mathbf{Z}_{s_1}^{\prime}, D^{\prime})   \right]
			\E_{D}\left[ |\mu(\mathbf{X}_{s_2})| \, s_2 \, \kappa(\mathbf{x}; \mathbf{Z}_{s_2}, D)  \right]                                                                                                                                          \\
			%
			 & \quad \lesssim \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}\mu^2(\mathbf{x}) + o(1)
		\end{aligned}
	\end{equation}
\end{proof}

\newpage
\begin{lem}[Kernel Variance of the TDNN Kernel]\label{lem:Var_TDNN_k}
	For the kernel of the TDNN estimator with subsampling scales $s_1$ and $s_2$, it holds that
	\begin{equation}
		\zeta_{s_1, s_2}^{s_2}\left(\mathbf{x}\right)
		\lesssim \mu^2(\mathbf{x}) + \sigma_{\varepsilon} + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < c_1 \leq s_1 / s_2 \leq c_2 < 1.
	\end{equation}
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:Var_TDNN_k}]
	Consider first the following decomposition.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{s_2}\left(\mathbf{x}\right)
			 & = \Var\left(h_{s_1, s_2}\left(\mathbf{x}; \mathbf{Z}_1, \ldots, \mathbf{Z}_{s_2}\right)\right)
			= \Var_{D}\left(h_{s_1, s_2}\left(\mathbf{x}; D\right)\right)                                     \\
			%
			 & \leq \E_{D}\left[h_{s_1, s_2}^{2}\left(\mathbf{x}; D\right)\right]
			= \E_{D}\left[
				\left(w_{1}^{*}\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right) + w_{2}^{*} h_{s_2}\left(\mathbf{x}; D\right)\right)^2
			\right]                                                                                           \\
			%
			 & = \left(w_{1}^{*}\right)^2\E_{D}\left[\tilde{\mu}_{s_1}^{2}\left(\mathbf{x}; D\right)\right]
			+ 2 w_{1}^{*}w_{2}^{*} \E_{D}\left[\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right) h_{s_2}\left(\mathbf{x}; D\right)\right]
			+ \left(w_{2}^{*}\right)^2\Omega_{s_2}
		\end{aligned}
	\end{equation}

	Then, observe the following.
	\begin{equation}
		\begin{aligned}
			\E_{D}\left[\tilde{\mu}_{s_1}^{2}\left(\mathbf{x}; D\right)\right]
			 & = \E_D\left[\left(\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(\mathbf{x}; D_{\ell}\right)\right)^2\right]
			= \binom{s_2}{s_1}^{-2} \E_{D}\left[\sum_{\iota, \iota' \in L_{s_2, s_1}}h_{s_1}\left(\mathbf{x}; D_{\iota}\right)h_{s_1}\left(\mathbf{x}; D_{\iota'}\right)\right] \\
			%
			 & = \binom{s_2}{s_1}^{-2} \sum_{c = 0}^{s_1} \binom{s_2}{s_1}\binom{s_1}{c}\binom{s_2 - s_1}{s_1 - c} \Omega_{s_1}^{c}
			= \binom{s_2}{s_1}^{-1} \sum_{c = 0}^{s_1} \binom{s_1}{c}\binom{s_2 - s_1}{s_1 - c} \Omega_{s_1}^{c}                                                                \\
			%
			 & \lesssim \Omega_{s_1}
			\lesssim \mu(\mathbf{x})^2 + \sigma_{\varepsilon}^2 + o(1)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}

	Recall that by Lemma \ref{lem:omega_s}, we have the following.
	\begin{equation}
		\begin{aligned}
			\Omega_{s_2}
			\lesssim \mu(\mathbf{x})^2 + \sigma_{\varepsilon}^2 + o(1)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}

	Lastly, consider the following.
	\begin{equation}
		\begin{aligned}
			\E_{D}\left[\tilde{\mu}_{s_1}\left(\mathbf{x}; D\right) h_{s_2}\left(\mathbf{x}; D\right)\right]
			 & = \E_D\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(\mathbf{x}; D_{\ell}\right)h_{s_2}\left(\mathbf{x}; D\right)\right] \\
			%
			 & = \E_D\left[h_{s_1}\left(\mathbf{x}; D_{[s_1]}\right)h_{s_2}\left(\mathbf{x}; D\right)\right]
			= \Upsilon_{s_1, s_2}\left(\mathbf{x}\right)
		\end{aligned}
	\end{equation}

	Thus, we find the following.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_2, s_2}\left(\mathbf{x}\right)
			 & \lesssim \left(w_{1}^{*}\right)^2 \Omega_{s_1}
			+ 2 w_{1}^{*}w_{2}^{*} \Upsilon_{s_1, s_2}\left(\mathbf{x}\right)
			+ \left(w_{1}^{*}\right)^2 \Omega_{s_2}                                                                       \\
			%
			 & \lesssim \left(w_{1}^{*} + w_{2}^{*}\right)^2 \left(\mu^2(\mathbf{x}) + \sigma_{\varepsilon}\right) + o(1)
			= \mu^2(\mathbf{x}) + \sigma_{\varepsilon} + o(1).
		\end{aligned}
	\end{equation}
\end{proof}

\newpage

\begin{lem}[Lemma 10 - \citet{demirkaya_optimal_2024}]\label{lem:dem10}
	For the kernel of the TDNN estimator with subsampling scales $s_1$ and $s_2$ satisfying
	\begin{equation}
		0 < c_1 \leq s_1 / s_2 \leq c_2 < 1
		\quad \text{and} \quad
		s_2 = o(n),
	\end{equation}
	it holds that
	\begin{equation}
		\zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right)
		\sim s_2^{-1}.
	\end{equation}
\end{lem}

% \subsection{Jackknife Lemmas}
% \hrule

% \begin{lem}[\citet{lee_u-statistics_2019} - Chapter 5, Theorem 1]\label{thm:lee_ch5_1}\mbox{}\\*
% 	Let $U_n$ be a U-statistic of degree $s$ based on a kernel $\psi_s$.
% 	Then, the Jackknife Variance estimator takes the following form.
% 	\begin{equation}
% 		\hat{\omega}^{2}_{JK}\left(\mathbf{x}; \mathbf{D}_n\right)
% 		= \frac{n-1}{n^2}\binom{n-1}{s}^{-2}\sum_{c = 0}^{s}\left(cn - s^2\right)S_c
% 	\end{equation}
% 	where the quantities $S_c$ are defined by
% 	\begin{equation}
% 		S_c
% 		= \sum_{\substack{\iota, \iota' \in L_{n,s}, \\ \, |\iota \cap \iota'| = c}}
% 		\psi_{s}(D_{\iota})\psi_{s}(D_{\iota'}).
% 	\end{equation}
% \end{lem}

% \hrule

% \begin{lem}[Jackknife Variance Estimator as U-Statistic]\label{lem:JK_as_U_Stat}\mbox{}\\*
% 	The Jackknife Variance Estimator as described in Lemma \ref{thm:lee_ch5_1} can be expressed as a U-statistic of order $2s$.
% 	\begin{equation}
% 		\hat{\omega}^{2}_{JK}\left(\mathbf{x}; \mathbf{D}_n\right)
% 		= \binom{n}{2s}^{-1} \sum_{\ell \in L_{n, 2 s}}J_{s}\left(\mathbf{x}; n, D_\ell\right)
% 	\end{equation}
% 	with
% 	\begin{equation}
% 		J_{s}\left(\mathbf{x}; n, D\right)
% 		:= A(n, s)
% 		\sum_{c = 0}^{s} \frac{cn - s^2}{B(n, s, c)}
% 		\sum_{\varkappa \in L_{2 s, c}}
% 		\sum_{\substack{\varsigma, \varpi \in L_{s - c}([2 s] \backslash \varkappa),\\
% 				\varsigma \cap \varpi = \emptyset}}
% 		j_{s}\left(\mathbf{x}; \varkappa, \varsigma, \varpi, D\right)
% 	\end{equation}
% 	where
% 	\begin{equation}
% 		j_{s}\left(\mathbf{x}; \varkappa, \varsigma, \varpi, D\right) =
% 		h_{s}(\mathbf{x}; D_{\varkappa \cup \varsigma}) \cdot
% 		h_{s}(\mathbf{x};D_{\varkappa \cup \varpi})
% 	\end{equation}
% 	and
% 	\begin{equation}
% 		A(n, s) := \frac{n-1}{n^2} \binom{n}{2s} \binom{n-1}{s}^{-2}
% 		\quad \text{and} \quad
% 		B(n, s, c) := \binom{n-2s+c}{2s - c}{\color{red} \text{???}}
% 	\end{equation}
% \end{lem}

% \begin{proof}[Proof of Lemma \ref{lem:JK_as_U_Stat}]
% 	Taking Lemma \ref{thm:lee_ch5_1} as given, we find that the Jackknife variance estimator for the TDNN estimator can be expressed in the following form.
% 	\begin{equation}
% 		\hat{\omega}^{2}_{JK}
% 		= \frac{n-1}{n^2}\binom{n-1}{s}^{-2}
% 		\left(\sum_{c = 0}^{s}\left(cn - s^2\right)
% 		\sum_{\substack{\ell, \iota \in L_{n,s}, \\ \, |\ell \cap \iota| = c}} h_{s}(\mathbf{x}; D_{\ell})h_{s}(\mathbf{x};D_{\iota})\right)
% 	\end{equation}
% 	Taking now the kernel as described in Lemma \ref{lem:JK_as_U_Stat}, we can make the following observation.
% 	\begin{equation}
% 		\begin{aligned}
% 			\hat{\omega}^{2}_{JK}
% 			%
% 			 & = \frac{n-1}{n^2}\binom{n-1}{s}^{-2}
% 			\left(\sum_{c = 0}^{s}\left(cn - s^2\right)
% 			\sum_{\substack{\iota, \iota' \in L_{n,s},                                                              \\ \, |\iota' \cap \iota| = c}} h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})\right) \\
% 			%
% 			 & = \binom{n}{2 s}^{-1} A(n, s)
% 			\left(
% 			\sum_{\iota', \iota \in L_{n,s}}
% 			\left(|\iota' \cap \iota|n - s^2\right) h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})\right) \\
% 			%
% 			 & = \binom{n}{2 s}^{-1} A(n, s)
% 			\left(
% 			\sum_{\iota', \iota \in L_{n,s}}
% 			\frac{
% 			\sum_{\ell \in L_{n, 2s}}
% 			\1\left(\iota, \iota' \subseteq \ell\right)
% 			\left(|\iota' \cap \iota|n - s^2\right) h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})}
% 			{\sum_{\ell \in L_{n, 2s_2}}\1\left(\iota, \iota' \subseteq \ell\right)}\right)                         \\
% 			%
% 			 & = \binom{n}{2 s}^{-1} A(n, s)
% 			\left(
% 			\sum_{\ell \in L_{n, 2s}}
% 			\sum_{\iota', \iota \in L_{n,s}}
% 			\frac{\1\left(\iota, \iota' \subseteq \ell\right)}{B(n,s,|\iota \cap \iota'|)}
% 			\left(|\iota' \cap \iota|n - s^2\right) h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})
% 			\right)                                                                                                 \\
% 			%
% 			 & = \binom{n}{2 s}^{-1} A(n, s)
% 			\left(
% 			\sum_{\ell \in L_{n, 2s}}
% 			\sum_{\iota', \iota \in L_{s}(\ell)}
% 			\left(|\iota' \cap \iota|n - s^2\right)
% 			\frac{h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})}
% 			{B(n,s,|\iota \cap \iota'|)}
% 			\right)                                                                                                 \\
% 			%
% 			 & = \binom{n}{2 s}^{-1} A(n, s)
% 			\left(
% 			\sum_{c = 0}^{s}\frac{cn - s^2}{B(n,s,c)}
% 			\sum_{\ell \in L_{n, 2s}}
% 			\sum_{\substack{\iota', \iota \in L_{s}(\ell),                                                          \\ |\iota \cap \iota'| = c}}
% 			h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})
% 			\right)                                                                                                 \\
% 			%
% 			 & = \binom{n}{2 s}^{-1}
% 			\sum_{\ell \in L_{n, 2s}} \left(A(n, s)
% 			\sum_{c = 0}^{s}\frac{cn - s^2}{B(n,s,c)}
% 			\sum_{\substack{\iota', \iota \in L_{s}(\ell),                                                          \\ |\iota \cap \iota'| = c}}
% 			h_{s}(\mathbf{x}; D_{\iota})h_{s}(\mathbf{x};D_{\iota'})
% 			\right)                                                                                                 \\
% 			%
% 			 & = \binom{n}{2 s}^{-1}
% 			\sum_{\ell \in L_{n, 2s}}\left(
% 			A(n, s)
% 			\sum_{c = 0}^{s} \frac{cn - s^2}{B(n, s, c)}
% 			\sum_{\varkappa \in L_{2 s, c}}
% 			\sum_{\substack{\varsigma, \varpi \in L_{s - c}([2 s] \backslash \varkappa),                            \\
% 					\varsigma \cap \varpi = \emptyset}}
% 			j_{s}\left(\mathbf{x}; \varkappa, \varsigma, \varpi, D_{\ell}\right)\right)                             \\
% 			%
% 			 & = \binom{n}{2 s}^{-1}
% 			\sum_{\ell \in L_{n, 2s}}
% 			J_{s}\left(\mathbf{x}; n, D_{\ell}\right)
% 		\end{aligned}
% 	\end{equation}
% 	Thus showing that the Jackknife variance estimator is a U-statistic of order $2s$.
% \end{proof}

\newpage
\subsection{Variance Estimator Consistency Theorems}
\hrule

\begin{lem}[Asymptotic Dominance of H\'ajek Projection]\label{lem:Hajek_Dominance}\mbox{}\\*
	Let $U_{s}\left(\mathbf{D}_{[n]}\right)$ be a non-randomized complete generalized U-statistic with kernel $h_s$.
	Let the kernel variance terms $\zeta_{s}^{s}$ and $\zeta_{s}^{1}$ be defined in analogy to Section \ref{sec:TDNN}.
	Assume that the following condition holds.
	\begin{equation}
		\frac{s}{n}\left(\frac{\zeta_{s}^{s}}{s \zeta_{s}^{1}} - 1\right) \rightarrow 0
	\end{equation}
	Then, asymptotically, the H\'ajek projection term dominates the variance of the U-statistic in the following sense.
	\begin{equation}
		\frac{n}{s^2}\frac{\Var\left(\mathrm{U}_{s}\left(\mathbf{D}_{[n]}\right)\right)}{\zeta_{s}^{1}}
		\rightarrow 1.
	\end{equation}
\end{lem}

\begin{proof}
	\begin{equation}
		\begin{aligned}
			1 \leq \frac{n}{s^2}\frac{\Var\left(\mathrm{U}_{s}\left(\mathbf{D}_{[n]}\right)\right)}{\zeta_{s}^{1}}
			 & = \left(\frac{s^2}{n} \zeta_{s}^{1}\right)^{-1} \sum_{j=1}^{s}\binom{s}{j}^2\binom{n}{j}^{-1} V_{s}^{j}     \\
			%
			 & \leq 1 + \left(\frac{s^2}{n} \zeta_{s}^{1}\right)^{-1} \frac{s^2}{n^2} \sum_{j=2}^{s}\binom{s}{j} V_{s}^{j} \\
			%
			 & \leq 1 + \frac{s}{n}\left(\frac{\zeta_{s}^{s}}{s\zeta_{s}^{1}} - 1\right)
			\rightarrow 1.
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

\begin{lem}[H\'ajek Dominance for TDNN Estimator]\label{lem:TDNN_Hajek_Dominance}\mbox{}\\*
	Let $0 < c_1 \leq s_1/s_2 \leq c_2 < 1$ and $s_2 = o(n)$, then under Assumptions \ref{asm:DGP}, \ref{asm:subexp_tails} and \ref{asm:smoothness}, then the TDNN estimator fulfills the asymptotic H\'ajek dominance condition shown in Lemma \ref{lem:Hajek_Dominance}.
\end{lem}

\begin{proof}
	Recall the results from Lemmas \ref{lem:Var_TDNN_k} and \ref{lem:dem10}.
	\begin{equation*}
		\zeta_{s_1, s_2}^{s_2}\left(\mathbf{x}\right) \lesssim \mu^2(\mathbf{x}) + \sigma_{\varepsilon} + o(1)
		\quad \text{and} \quad
		\zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right) \sim s_2^{-1}
	\end{equation*}
	Using these results, we can find the following.
	\begin{equation}
		\begin{aligned}
			\frac{s_2}{n}\left(\frac{
				\zeta_{s_1, s_2}^{s_2}\left(\mathbf{x}\right)}{s_2 \zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right)} - 1\right)
			 & \sim \frac{s_2}{n}\left(\mu^2(\mathbf{x}) + \sigma_{\varepsilon} + o(1) - 1\right)
			\sim \frac{s_2}{n} \rightarrow 0
		\end{aligned}
	\end{equation}
\end{proof}
\hrule 

\begin{proof}[Proof of Theorem \ref{thm:PI_JK_Cons}]\mbox{}\\*
	The desired result immediately follows from an application of Theorem 6 from \citet{peng_bias_2021}.
\end{proof}

\newpage
\begin{proof}[Proof of Theorem \ref{thm:JK_Cons}]\mbox{}\\*
	Recall the definition of the Jackknife Variance estimator.
	\begin{equation}
		\hat{\omega}_{JK}^2\left(\mathbf{x}; \mathbf{D}_n\right)
		= \frac{n-1}{n} \sum_{i = 1}^{n}\left(\hat{\mu}_{s_1, s_2}\left(\mathbf{x}; \mathbf{D}_{n, -i}\right) - \hat{\mu}_{s_1, s_2}\left(\mathbf{x}; \mathbf{D}_{n}\right)\right)^2
	\end{equation}

	Using the Hoeffding-decomposition of the original U-statistic, we can reformulate this expression in the following way.
	\begin{equation}
		\begin{aligned}
			\hat{\omega}_{JK}^2\left(\mathbf{x}; \mathbf{D}_n\right)
			 & = \frac{n-1}{n} \sum_{i = 1}^{n}\left(
			\sum_{j = 1}^{s_2}\binom{s_2}{j} H_{s_1, s_2}^{j}\left(\mathbf{D}_{n, -i}\right)
			- \sum_{j = 1}^{s_2}\binom{s_2}{j}H_{s_1, s_2}^{j}
			\right)^2                                                                                                    \\
			%
			 & = \frac{n-1}{n} \sum_{j = 1}^{n}\left(
			\sum_{j = 1}^{s_2}\binom{s_2}{j}\left(H_{s_1, s_2}^{j}
				- H_{s_1, s_2}^{j}\left(\mathbf{D}_{n, -i}\right)\right)
			\right)^2                                                                                                    \\
			%
			 & = \frac{n-1}{n} \sum_{j = 1}^{n}\left(\sum_{j = 1}^{s_2}\binom{s_2}{j}
			\left( \binom{n}{j}^{-1}\sum_{\iota \in L_{n,j}} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})
			- \binom{n - 1}{j}^{-1}\sum_{\ell \in L_{j}\left([n]\backslash \{i\}\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\ell})\right)
			\right)^2                                                                                                    \\
			%
			 & = \frac{n-1}{n} \sum_{j = 1}^{n}\left[
				\frac{s_2}{n} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})
			+ \sum_{j \neq i} \left(\frac{s_2}{n} - \frac{s_2}{n - 1}\right) h^{(1)}_{s_1, s_2}(\mathbf{Z}_{j}) \right.  \\
			 & \quad \quad + \left.\sum_{j = 2}^{s_2}\binom{s_2}{j}
				\left( \binom{n}{j}^{-1}\sum_{\iota \in L_{n,j}} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})
				- \binom{n - 1}{j}^{-1}\sum_{\ell \in L_{j}\left([n]\backslash \{i\}\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\ell})\right)
			\right]^2                                                                                                    \\
			%
			 & = \frac{n-1}{n} \frac{s^2}{n^2}\sum_{j = 1}^{n}\left[
				h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})
			- \frac{1}{n-1} \sum_{j \neq i} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{j}) \right.                                   \\
			 & \quad \quad + \left. \frac{n}{s}\sum_{j = 2}^{s_2}\binom{s_2}{j}\left(
				\binom{n}{j}^{-1}\sum_{\iota \in L_{j-1}\left([n] \backslash \{i\}\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota \cup \{i\}})
				+ \left[\binom{n}{j}^{-1} - \binom{n - 1}{j}^{-1}\right] \sum_{\ell \in L_{j}\left([n] \backslash \{i\}\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\ell})
				\right)
			\right]                                                                                                      \\
			%
			 & =: \frac{n-1}{n}\frac{s^2}{n^2} \sum_{j = 1}^{n}\left[h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i}) + T_{i}\right]^2
		\end{aligned}
	\end{equation}
	Observe that due to the independence of the observations and the uncorrelatedness of Hoeffding projections of differing orders, $h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})$ and $T_{i}$ are uncorrelated and both have mean zero.
	Now, continuing to follow the line of argument in \citet{peng_bias_2021}, observe the following.
	\begin{equation}
		\begin{aligned}
			\E\left[\left(h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})\right)^{2}\right]
			 & = V_{s_1, s_2}^{1}
			= \zeta_{s_1, s_2}^{1}
		\end{aligned}
	\end{equation}

	Furthermore, as a consequence of the independence of the observations and the uncorrelatedness of Hoeffding projections of differing order, we find that
	\begin{equation}
		\begin{aligned}
			\E\left[T_{i}^{2}\right]
			 & = \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{n^2}{s_2^2}\sum_{j = 2}^{s_2}\binom{s_2}{j}^2\left\{
			\binom{n}{j}^{-2}\binom{n-1}{j-1}V_{s_1, s_2}^{j}
			+ \left[\binom{n}{j}^{-1} - \binom{n - 1}{j}^{-1}\right]^2 \binom{n-1}{j}V_{s_1, s_2}^{j}
			\right\}                                                                                                                         \\
			%
			 & = \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{n^2}{s_2^2}\sum_{j = 2}^{s_2}\binom{s_2}{j}^2\left\{
			\binom{n}{j}^{-2}\frac{j}{n - j}\binom{n-1}{j}V_{s_1, s_2}^{j}
			+ \binom{n}{j}^{-2}\left[1 - \binom{n}{j}\binom{n - 1}{j}^{-1}\right]^2 \binom{n-1}{j}V_{s_1, s_2}^{j}
			\right\}                                                                                                                         \\
			%
			 & = \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{n^2}{s_2^2}\sum_{j = 2}^{s_2}\binom{s_2}{j}^2\binom{n}{j}^{-2}
			\left(\frac{j}{n - j} + \left(1 - \frac{n}{n - j}\right)^2\right) \binom{n-1}{j}V_{s_1, s_2}^{j}                                 \\
			%
			 & = \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{n^2}{s_2^2}\sum_{j = 2}^{s_2}\binom{s_2}{j}
			\binom{n}{j}^{-2}\binom{n-1}{j} \cdot \left(\frac{j}{n - j} + \frac{j^2}{(n - j)^2}\right)
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                      \\
			%
			 & = \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{n^2}{s_2^2}\sum_{j = 2}^{s_2}\binom{s_2}{j}\binom{n}{j}^{-1}
			\frac{n - j}{n} \cdot \frac{j}{n}\left(\frac{n}{n - j} + \frac{j n}{(n - j)^2}\right)
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                      \\
			%
			 & =  \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \sum_{j = 2}^{s_2}\frac{j}{s_2}
			\binom{s_2 - 1}{j - 1}\binom{n-1}{j-1}^{-1}
			\frac{n - j}{n} \left(\frac{n}{n - j} + \frac{j}{n}\right)
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                      \\
			%
			 & \leq \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \sum_{j = 2}^{s_2}\frac{j}{s_2} \left(e \frac{s_2 - 1}{n - 1}\right)^{j-1}
			\frac{n - j}{n}\left(\frac{n}{n - j} + \frac{j}{n}\right)
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                      \\
			%
			 & \lesssim \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ 2 \sum_{j = 2}^{s_2}\frac{j}{s_2} \left(e \frac{s_2 - 1}{n - 1}\right)^{j-1}
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                      \\
			%
			 & \leq \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ 2e \sum_{j = 2}^{s_2}\frac{1}{s_2} \frac{s_2 - 1}{n - 1}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]
			+ 2  \sum_{j = 2}^{s_2}\frac{j - 1}{s_2} \left(e \frac{s_2 - 1}{n - 1}\right)^{j-1}  \left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right] \\
			%
			 & \leq \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{2e}{n - 1} \sum_{j = 2}^{s_2}\frac{s_2 - 1}{s_2}
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]
			+ 2 \sum_{j = 2}^{s_2}\frac{j-1}{s_2} \left(e \frac{s_2 - 1}{n - 1}\right)^{j-1}\zeta_{s_1, s_2}^{s_2}                           \\
			%
			 & = \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{2e}{n} \sum_{j = 2}^{s_2}\frac{n (s_2 - 1)}{(n - 1)s_2}
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]
			+ 2\zeta_{s_1, s_2}^{s_2} \sum_{j = 2}^{s_2}\frac{j-1}{s_2} \left(e \frac{s_2 - 1}{n - 1}\right)^{j-1}                           \\
			%
			 & \leq \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{2e}{n} \sum_{j = 2}^{s_2}\binom{s_2}{j}V_{s_1, s_2}^{j}
			+ \frac{2\zeta_{s_1, s_2}^{s_2}}{s_2} \sum_{j = 1}^{\infty}j \left(e \frac{s_2 - 1}{n - 1}\right)^{j}                            \\
			%
			 & \leq \frac{1}{n-1} V_{s_1, s_2}^{1}
			+ \frac{2e}{n} \sum_{j = 2}^{s_2}\binom{s_2}{j}V_{s_1, s_2}^{j}
			+ \frac{2 \zeta_{s_1, s_2}^{s_2}}{s_2}\sum_{j = 1}^{\infty}j \left(e \frac{s_2}{n}\right)^{j}                                    \\
			%
			 & = \frac{1}{n-1} \zeta_{s_1, s_2}^{1}
			+ \frac{2e}{n} \left(\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}\right)
			+ \frac{2 e n}{\left(n - e s_2\right)^2} \zeta_{s_1, s_2}^{s_2}                                                                  \\
			%
			 & = \left(\frac{1}{n-1} + \frac{2 e s_2 n}{\left(n - e s_2\right)^2}\right) \zeta_{s_1, s_2}^{1}
			+ 2e\left(\frac{1}{n} + \frac{n}{\left(n - e s_2\right)^2}\right) \left(\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}\right)
		\end{aligned}
	\end{equation}
	Recall the results of Lemmas \ref{lem:Var_TDNN_k} and \ref{lem:dem10}.
	\begin{equation}
		\zeta_{s_1, s_2}^{s_2}\left(\mathbf{x}\right) \lesssim \mu^2(\mathbf{x}) + \sigma_{\varepsilon} + o(1)
		\quad \text{and} \quad
		\zeta_{s_1, s_2}^{1}\left(\mathbf{x}\right) \sim s_2^{-1}
	\end{equation}
	This immediately implies that $\frac{s_2}{n}\left(\frac{\zeta_{s_1, s_2}^{s_2}}{s_2 \zeta_{s_1, s_2}^{1}} - 1\right) \rightarrow 0$.
	Using this result and the previous asymptotic upper bound, we can find the following.
	\begin{equation}
		\begin{aligned}
			\frac{\E\left[T_{i}^{2}\right]}{V_{s_1, s_2}^{1}}
			 & \leq \frac{\left(\frac{1}{n-1} + \frac{2 e s_2 n}{\left(n - e s_2\right)^2}\right) \zeta_{s_1, s_2}^{1}
			+ 2e\left(\frac{1}{n} + \frac{n}{\left(n - e s_2\right)^2}\right) \left(\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}\right)}{\zeta_{s_1, s_2}^{1}} \\
			%
			 & = \frac{1}{n-1} + \frac{2 e s_2 n}{\left(n - e s_2\right)^2}
			+ 2e\left(\frac{1}{n} + \frac{n}{\left(n - e s_2\right)^2}\right) \left(\frac{\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}}{\zeta_{s_1, s_2}^{1}}\right)
			\rightarrow 0
		\end{aligned}
	\end{equation}
	Therefore, we can conclude that $h_{s}^{(1)}\left(\mathbf{Z}_{i}\right)$ dominates $T_i^2$ in the expression of interest.
	Using Lemma \ref{lem:peng1}, we can thus conclude the following.
	\begin{equation}
		\begin{aligned}
			\frac{\frac{n}{s_2^2}\hat{\omega}^{2}_{JK}\left(\mathbf{x}; \mathbf{D}_n\right)}{V_{s_1, s_2}^{1}\left(\mathbf{x}\right)}
			 & \rightarrow_{p} \frac{n - 1}{n}\frac{1}{n}\sum_{i = 1}^{n}\frac{\left(h^{(1)}_{s_1, s_2}(\mathbf{x}; \mathbf{Z}_i)\right)^{2}}{V_{s_1, s_2}^{1}\left(\mathbf{x}\right)} \\\
			%
			 & \rightarrow_{p} \frac{n - 1}{n}\frac{\E\left[\left(h^{(1)}_{s_1, s_2}(\mathbf{x}; \mathbf{Z}_i)\right)^{2}\right]}{V_{s_1, s_2}^{1}\left(\mathbf{x}\right)}
			\rightarrow 1
		\end{aligned}
	\end{equation}
	The desired rate-consistency then immediately follows from an application of Lemma \ref{lem:Hajek_Dominance}.
\end{proof}

\newpage
\begin{proof}[Proof of Theorem \ref{thm:JKD_Cons}]\mbox{}\\*
	Consider first the case absent additional randomization in the form of $\omega$ and recall the definition of the delete-d Jackknife Variance estimator.
	\begin{equation}
		\hat{\omega}_{JKD}^2\left(\mathbf{x}; d, \mathbf{D}_n\right)
		= \frac{n-d}{d}\binom{n}{d}^{-1} \sum_{\ell \in L_{n,d}}
		\left(\hat{\mu}_{s_1, s_2}\left(\mathbf{x}; \mathbf{D}_{n, -\ell}\right)
		- \hat{\mu}_{s_1, s_2}\left(\mathbf{x}; \mathbf{D}_{n}\right)
		\right)^2
	\end{equation}
	Now, as in the proof for the conventional Jackknife variance estimator, we make use of the Hoeffding-decomposition in the following way.
	\begin{equation}
		\begin{aligned}
			\hat{\omega}_{JKD}^2\left(\mathbf{x}; d, \mathbf{D}_n\right)
			 & = \frac{n-d}{d}\binom{n}{d}^{-1} \sum_{\ell \in L_{n, d}}
			\left(\sum_{j = 1}^{s_2}\binom{s_2}{j} \left(H_{P_{t}}^{j} - H_{P_{t}}^{j}\left(\mathbf{D}_{n, -\ell}\right)\right)\right)^2 \\
			%
			 & = \frac{n-d}{d}\binom{n}{d}^{-1} \sum_{\ell \in L_{n, d}}
			\left(\sum_{j = 1}^{s_2}\binom{s_2}{j}
			\left( \binom{n}{j}^{-1}\sum_{\iota \in L_{n,j}} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})
			- \binom{n-d}{j}^{-1}\sum_{\iota \in L_{j}\left([n]\backslash \ell\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})\right)
			\right)^2                                                                                                                    \\
			%
			 & = \frac{n-d}{d}\binom{n}{d}^{-1} \sum_{\ell \in L_{n, d}}\left[
				\frac{s_2}{n}\sum_{i \in \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})
				+ \sum_{i \in [n] \backslash \ell} \left(\frac{s_2}{n} - \frac{s_2}{n - d}\right) h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})
			\right.                                                                                                                      \\
			 & \quad \quad + \left.\sum_{j = 2}^{s_2}\binom{s_2}{j}
				\left( \binom{n}{j}^{-1}\sum_{\iota \in L_{n,j}} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})
				- \binom{n-d}{j}^{-1}\sum_{\iota \in L_{j}\left([n]\backslash \ell\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})\right)
			\right]^2                                                                                                                    \\
			%         
			 & = \frac{n-d}{d}\binom{n}{d}^{-1}\left(\frac{s_2}{n}\right)^2 \sum_{\ell \in L_{n, d}}\left[
				\sum_{i \in \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})
				- \frac{d}{n - d}\sum_{i \in [n] \backslash \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})
			\right.                                                                                                                      \\
			 & \quad \quad + \left. \frac{n}{s_2}\sum_{j = 2}^{s_2}\binom{s_2}{j}
				\left( \binom{n}{j}^{-1}\sum_{\iota \in L_{n,j}} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})
				- \binom{n-d}{j}^{-1}\sum_{\iota \in L_{j}\left([n]\backslash \ell\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})\right)
			\right]^2                                                                                                                    \\
			%                                                                                                       \\
			 & =: (n-d)\binom{n}{d}^{-1}\left(\frac{s_2}{n}\right)^2 \sum_{\ell \in L_{n, d}}\left[
				\frac{1}{\sqrt{d}}\sum_{i \in \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i}) + T_{\ell}\right]^2
		\end{aligned}
	\end{equation}

	We want to proceed in an analogous way to the proof of the pure Jackknife result.
	Thus, we want to show that $\sum_{i \in \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})$ dominates $T_{\ell}$ in the sense of Lemma \ref{lem:peng1}.
	Luckily, since Lemma \ref{lem:peng1} does not depend on any particular independence assumptions of summands etc. this is a relatively straightforward adaptation of the strategy shown in the proof of Theorem \ref{thm:JK_Cons}.
	Thus, consider the following for an arbitrary fixed index-subset $\ell$ with cardinality $d$.
	\begin{equation}
		\begin{aligned}
			\E\left[\left(\frac{1}{\sqrt{d}}\sum_{i \in \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})\right)^2\right]
			 & = \frac{1}{d}\E\left[\sum_{i \in \ell}\sum_{j \in \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})h^{(1)}_{s_1, s_2}(\mathbf{Z}_{j})\right]
			= \frac{1}{d}\sum_{i \in \ell}\sum_{j \in \ell} \E\left[h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})h^{(1)}_{s_1, s_2}(\mathbf{Z}_{j})\right]    \\
			%
			 & = \frac{\left| \ell \right|}{d} \cdot \E\left[\left(h^{(1)}_{s_1, s_2}(Z_{1})\right)^2\right]
			= \zeta_{P_{t}, 1}
		\end{aligned}
	\end{equation}

	\newpage
	For the error term we introduce a case distinction.
	Case one corresponds to parameter choices where $s_2 \geq d$ and thus takes the following form.
	\begin{equation}
		\begin{aligned}
			T_{\ell}
			 & = \frac{\sqrt{d}}{n - d}\sum_{i \in [n] \backslash \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})             \\
			 & \quad \quad + \frac{n}{s_2\sqrt{d}} \left\{\sum_{j = 2}^{d}\binom{s_2}{j}\left(\binom{n}{j}^{-1}\left(
			\sum_{a = 1}^{j}\sum_{\substack{\varkappa \in L_{a}\left(\ell\right)                                       \\ \varrho \in L_{j - a}\left([n]\backslash \ell\right)}} h^{(j)}_{s_1, s_2}(D_{\varkappa \cup \varrho})\right)
			+ \left(\binom{n}{j}^{-1} - \binom{n-d}{j}^{-1}\right)
			\sum_{\iota \in L_{j}\left([n]\backslash \ell\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})\right)\right. \\
			 & \quad \quad +\left.  \sum_{j = d + 1}^{s_2}\binom{s_2}{j}
			\left( \binom{n}{j}^{-1}\left(
			\sum_{a = 1}^{d}\sum_{\substack{\varkappa \in L_{a}\left(\ell\right)                                       \\ \varrho \in L_{j - a}\left([n]\backslash \ell\right)}} h^{(j)}_{s_1, s_2}(D_{\varkappa \cup \varrho})\right)
			+ \left(\binom{n}{j}^{-1} - \binom{n-d}{j}^{-1}\right)\sum_{\iota \in L_{j}\left([n]\backslash \ell\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})\right)
			\right\}
		\end{aligned}
	\end{equation}
	Case two covers setups of the form $s_2 < d$ and thus takes the following form.
	\begin{equation}
		\begin{aligned}
			T_{\ell}
			 & = \frac{\sqrt{d}}{n - d}\sum_{i \in [n] \backslash \ell} h^{(1)}_{s_1, s_2}(\mathbf{Z}_{i})         \\
			 & \quad \quad + \frac{n}{s_2 \sqrt{d}}  \sum_{j = 2}^{s_2}\binom{s_2}{j}\left(\binom{n}{j}^{-1}\left(
			\sum_{a = 1}^{j}\sum_{\substack{\varkappa \in L_{a}\left(\ell\right)                                   \\ \varrho \in L_{j - a}\left([n]\backslash \ell\right)}} h^{(j)}_{s_1, s_2}(D_{\varkappa \cup \varrho})\right)
			+ \left(\binom{n}{j}^{-1} - \binom{n-d}{j}^{-1}\right)
			\sum_{\iota \in L_{j}\left([n]\backslash \ell\right)} h^{(j)}_{s_1, s_2}(\mathbf{D}_{\iota})\right)    \\
		\end{aligned}
	\end{equation}

	Having separated these two cases, we continue by investigating the expectation of their respective squares.
	Beginning with case one, we find the following.
	\begin{equation}
		\begin{aligned}
			\E\left[\left(T_{\ell}\right)^2\right]
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1}                                                                                                          \\
			 & \quad \quad + \frac{n^2}{s_2^2 d}\sum_{j = 2}^{d}\binom{s_2}{j}^2
			\left(\binom{n}{j}^{-2}\sum_{a = 1}^{j}\left[\binom{d}{a}\binom{n - d}{j - a}\right]
			+ \left[\binom{n}{j}^{-1} - \binom{n-d}{j}^{-1}\right]^{2} \binom{n - d}{j}\right)V_{s_1, s_2}^{j}                                             \\
			 & \quad \quad + \frac{n^2}{s_2^2 d}\sum_{j = d + 1}^{s_2}\binom{s_2}{j}^2
			\left(\binom{n}{j}^{-2}\sum_{a = 1}^{d}\left[\binom{d}{a}\binom{n - d}{j - a}\right]
			+ \left[\binom{n}{j}^{-1} - \binom{n-d}{j}^{-1}\right]^{2}\binom{n - d}{j}\right)V_{s_1, s_2}^{j}                                              \\
			%
			 & \overset{(\star)}{=} \frac{d}{n - d} V_{s_1, s_2}^{1}                                                                                       \\
			 & \quad \quad + \frac{n^2}{s_2^2 d}\sum_{j = 2}^{d}\binom{s_2}{j}^2\binom{n}{j}^{-2}
			\left(\binom{n}{j} - \binom{n - d}{j}
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2}\binom{n - d}{j} \right)V_{s_1, s_2}^{j}                                                 \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = d + 1}^{s_2}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}\frac{\binom{n - d}{j}}{\binom{n}{j}}
			\left(\sum_{a = 1}^{d}\frac{\binom{d}{a}\binom{n - d}{j - a}}{\binom{n - d}{j}}
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2}\right)\binom{s_2}{j}V_{s_1, s_2}^{j}
		\end{aligned}
	\end{equation}
	The equality marked by $(\star)$ holds by the Chu-Vandermonde identity - specifically with respect to the equivalent expression for the sum in the second term.

	Continuing the analysis, we find the following.
	\begin{equation}
		\begin{aligned}
			\E\left[\left(T_{\ell}\right)^2\right]
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1}                                                                                                                        \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = 2}^{d}\frac{\binom{s_2 - 1}{j - 1}\binom{n - d}{j}}{\binom{n - 1}{j - 1}\binom{n}{j}}
			\left(\binom{n}{j}\binom{n - d}{j}^{-1} - 1
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2} \right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                    \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = d + 1}^{s_2}\frac{\binom{s_2 - 1}{j - 1}\binom{n - d}{j}}{\binom{n - 1}{j - 1}\binom{n}{j}}
			\left(\frac{\binom{n}{j}}{\binom{n - d}{j}}\sum_{a = 1}^{d}\frac{\binom{d}{a}\binom{n - d}{j - a}}{\binom{n}{j}}
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2}\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                     \\
			%
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1}                                                                                                                        \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = 2}^{d}\frac{\binom{s_2 - 1}{j - 1}\binom{n - d}{j}}{\binom{n - 1}{j - 1}\binom{n}{j}}
			\left(\binom{n}{j}^{2}\binom{n-d}{j}^{-2} - \binom{n}{j}\binom{n-d}{j}^{-1}\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = d + 1}^{s_2}\frac{\binom{s_2 - 1}{j - 1}\binom{n - d}{j}}{\binom{n - 1}{j - 1}\binom{n}{j}}
			\left(\frac{\binom{n}{j}}{\binom{n - d}{j}\binom{n}{d}}\sum_{a = 1}^{d}\binom{j}{a}\binom{n - j}{d - a}
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2}\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                     \\
			%
			 & \overset{(\star\star)}{=} \frac{d}{n - d} V_{s_1, s_2}^{1}                                                                                                \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = 2}^{d}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}
			\left(\binom{n}{j}\binom{n-d}{j}^{-1} - 1\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                  \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = d + 1}^{s_2}\frac{\binom{s_2 - 1}{j - 1}\binom{n - d}{j}}{\binom{n - 1}{j - 1}\binom{n}{j}}
			\left(\frac{\binom{n}{j}}{\binom{n - d}{j}}\left[1 - \binom{n - j}{d}\binom{n}{d}^{-1}\right]
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2}\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                     \\
			%
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2}\sum_{j = 2}^{d}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}
			\left(\binom{n}{j}\binom{n-d}{j}^{-1} - 1\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                  \\
			 & \quad \quad + \frac{n}{s_2 d}\sum_{j = d + 1}^{s_2}\frac{\binom{s_2 - 1}{j - 1}\binom{n - d}{j}}{\binom{n - 1}{j - 1}\binom{n}{j}}
			\left(\frac{\binom{n}{j}}{\binom{n - d}{j}} - 1
			+ \left[1 - \binom{n}{j}\binom{n-d}{j}^{-1}\right]^{2}\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                     \\
			%
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2 d}\sum_{j = 2}^{s_2}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}
			\left(\binom{n}{j}\binom{n-d}{j}^{-1} - 1\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                  \\
			%
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2 d}\sum_{j = 2}^{s_2}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}
			\left(\prod_{i = 0}^{d - 1}\left(1 + \frac{j}{n - i - j}\right) - 1\right)\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                        \\
			%
			 & \overset{(\star\star\star)}{\leq} \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2 d}\sum_{j = 2}^{s_2}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}
			\frac{\sum_{i = 0}^{d - 1}\frac{j}{n - i - j}}{1 - \sum_{i = 0}^{d - 1}\frac{j}{n - i - j}}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                       \\
			%
			 & \leq \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2 d}\sum_{j = 2}^{s_2}\frac{\binom{s_2 - 1}{j - 1}}{\binom{n - 1}{j - 1}}
			\frac{j (n - j)}{(n - d - j + 1)(n - d - 2 j)}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                    \\
			%
			 & \leq \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2 d}\sum_{j = 2}^{s_2}\left(\frac{e (s_2 - 1)}{n - 1}\right)^{j-1}
			\frac{j (n - 2)}{(n - d - s_2 + 1)(n - d - 2s_t)}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]
		\end{aligned}
	\end{equation}
	The equality marked by $(\star\star)$ holds by the Chu-Vandermonde identity applied to the third summand, whereas the inequality marked by the equality marked by $(\star\star\star)$ follows from a Weierstrass-Product type inequality.
	Furthermore, this derivation shows that we do not really need to distinguish between the two described cases for the error term.

	Proceeding this way allows us to continue our analysis similar to the proof for the simple leave-one-out Jackknife.
	\begin{equation}
		\begin{aligned}
			\E\left[\left(T_{\ell}\right)^2\right]
			 & \lesssim \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{n}{s_2 d}\sum_{j = 2}^{s_2}\left(\frac{e (s_2 - 1)}{n - 1}\right)^{j-1}
			\frac{j (n - 2)}{(n - d - s_2 + 1)(n - d - 2s_t)}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                   \\
			%
			 & = \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{ 2 e \cdot n (n - 2)}{(n - 1)(n - d - s_2 + 1)(n - d - 2s_t)d}
			\sum_{j = 2}^{s_2}\frac{j}{s_2} \left(\frac{e (s_2 - 1)}{n - 1}\right)^{j-1}
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                                    \\
			%
			 & \lesssim \frac{d}{n - d} V_{s_1, s_2}^{1} + \frac{4e}{(n - d - s_2)s_2 d}
			\sum_{j = 2}^{s_2}j  \left(\frac{e (s_2 - 1)}{n - 1}\right)^{j-1}
			\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]                                                                                                    \\
			%
			 & \leq  \frac{d}{n - d} V_{s_1, s_2}^{1}
			+ \frac{4e}{(n - d - s_2)s_2 d} \sum_{j = 2}^{s_2}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]
			+ \frac{4e}{(n - d - s_2)s_2 d} \sum_{j = 2}^{s_2}(j-1)\left(\frac{e (s_2 - 1)}{n - 1}\right)^{j-1}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right] \\
			%
			 & \leq  \frac{d}{n - d} V_{s_1, s_2}^{1}
			+ \frac{4e}{(n - d - s_2)s_2 d} \sum_{j = 2}^{s_2}\left[\binom{s_2}{j}V_{s_1, s_2}^{j}\right]
			+ \frac{4e \cdot \zeta_{s_1, s_2}^{s_2}}{(n - d - s_2)s_2 d} \sum_{j = 1}^{\infty}j\left(\frac{e (s_2 - 1)}{n - 1}\right)^{j}                  \\
			%
			 & = \frac{d}{n - d} \zeta_{s_1, s_2}^{1}
			+ \frac{4e}{(n - d - s_2)s_2 d} \left(\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}\right)
			+ \frac{4e \cdot \zeta_{s_1, s_2}^{s_2}}{(n - d - s_2)s_2 d} \cdot \frac{e (s_2 - 1)(n - 1)}{\left(n - 1 - e (s_2 - 1)\right)^2}               \\
			%
			 & = \left(\frac{d}{n - d} + \frac{e (s_2 - 1)(n - 1)}{\left(n - 1 - e (s_2 - 1)\right)^2}\right) \zeta_{s_1, s_2}^{1}
			+ \frac{4e}{(n - d - s_2)s_2 d}\left(1 + \frac{e (s_2 - 1)(n - 1)}{\left(n - 1 - e (s_2 - 1)\right)^2}\right) \left(\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}\right)
		\end{aligned}
	\end{equation}

	We continue as in the default Jackknife case.
	\begin{equation}
		\begin{aligned}
			\frac{\E\left[T_{\ell}^2\right]}{V_{s_1, s_2}^{1}}
			 & \leq \frac{d}{n - d} + \frac{e (s_2 - 1)(n - 1)}{\left(n - 1 - e (s_2 - 1)\right)^2}
			+ \frac{4e}{(n - d - s_2)s_2 d}\left(1 + \frac{e (s_2 - 1)(n - 1)}{\left(n - 1 - e (s_2 - 1)\right)^2}\right) \frac{\zeta_{s_1, s_2}^{s_2} - s_2 \zeta_{s_1, s_2}^{1}}{\zeta_{s_1, s_2}^{1}} \\
			%
			 & \rightarrow 0.
		\end{aligned}
	\end{equation}
	Now, following the exact same logic as in the proof for the consistency of the Jackknife variance estimator, we obtain consistency of the delete-d Jackknife variance estimator.
\end{proof}

\newpage
\subsection{Pointwise Inference Results}

\begin{proof}[Proof of Theorem \ref{thm:pw_inf_TDNN}]\mbox{}\\*

\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:pw_inf_TDNN_HTE}]\mbox{}\\*

\end{proof}

\newpage
\section{Proofs for Results in Section \ref{sec:unif_inf}}
\hrule

\end{document}