% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global/global}
    \entry{arcones_bootstrap_1992}{article}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=24fb77746561a28919ed0b99daaa2a6c}{%
           family={Arcones},
           familyi={A\bibinitperiod},
           given={Miguel\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a080fabb620157837ced0c5c596403cf}{%
           family={Gine},
           familyi={G\bibinitperiod},
           given={Evarist},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e43310a39992c34206cdcfe629d28fa0}
      \strng{fullhash}{e43310a39992c34206cdcfe629d28fa0}
      \strng{fullhashraw}{e43310a39992c34206cdcfe629d28fa0}
      \strng{bibnamehash}{e43310a39992c34206cdcfe629d28fa0}
      \strng{authorbibnamehash}{e43310a39992c34206cdcfe629d28fa0}
      \strng{authornamehash}{e43310a39992c34206cdcfe629d28fa0}
      \strng{authorfullhash}{e43310a39992c34206cdcfe629d28fa0}
      \strng{authorfullhashraw}{e43310a39992c34206cdcfe629d28fa0}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0090-5364}
      \field{journaltitle}{The Annals of Statistics}
      \field{month}{6}
      \field{number}{2}
      \field{title}{On the {Bootstrap} of \${U}\$ and \${V}\$ {Statistics}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{20}
      \field{year}{1992}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1214/aos/1176348650
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/annals-of-statistics/volume-20/issue-2/On-the-Bootstrap-of-U-and-V-Statistics/10.1214/aos/1176348650.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/annals-of-statistics/volume-20/issue-2/On-the-Bootstrap-of-U-and-V-Statistics/10.1214/aos/1176348650.full
      \endverb
    \endentry
    \entry{arvesen_jackknifing_1969}{article}{}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=a56dda22b4e07fb27fdaba6723c3a3b7}{%
           family={Arvesen},
           familyi={A\bibinitperiod},
           given={James\bibnamedelima N.},
           giveni={J\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{fullhash}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{fullhashraw}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{bibnamehash}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{authorbibnamehash}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{authornamehash}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{authorfullhash}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \strng{authorfullhashraw}{a56dda22b4e07fb27fdaba6723c3a3b7}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0003-4851}
      \field{journaltitle}{The Annals of Mathematical Statistics}
      \field{month}{12}
      \field{number}{6}
      \field{title}{Jackknifing \${U}\$-{Statistics}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{40}
      \field{year}{1969}
      \field{urldateera}{ce}
      \field{pages}{2076\bibrangedash 2100}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1214/aoms/1177697287
      \endverb
      \verb{urlraw}
      \verb http://projecteuclid.org/euclid.aoms/1177697287
      \endverb
      \verb{url}
      \verb http://projecteuclid.org/euclid.aoms/1177697287
      \endverb
    \endentry
    \entry{biau_rate_2010}{article}{}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=d905131276d4431aea7b505a2b27db2f}{%
           family={Biau},
           familyi={B\bibinitperiod},
           given={Gérard},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=079cb28e463866db49fa98a4be5f776f}{%
           family={Cérou},
           familyi={C\bibinitperiod},
           given={Frédéric},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9ffcdb5f42c08e56cba38060b5ba2a8c}{%
           family={Guyader},
           familyi={G\bibinitperiod},
           given={Arnaud},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{fullhash}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{fullhashraw}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{bibnamehash}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{authorbibnamehash}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{authornamehash}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{authorfullhash}{c1a0f51abc5187efcc33fe3ac33640d9}
      \strng{authorfullhashraw}{c1a0f51abc5187efcc33fe3ac33640d9}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{22}
      \field{title}{On the {Rate} of {Convergence} of the {Bagged} {Nearest} {Neighbor} {Estimate}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{11}
      \field{year}{2010}
      \field{urldateera}{ce}
      \field{pages}{687\bibrangedash 712}
      \range{pages}{26}
      \verb{urlraw}
      \verb http://jmlr.org/papers/v11/biau10a.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v11/biau10a.html
      \endverb
    \endentry
    \entry{biau_layered_2010}{article}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=d905131276d4431aea7b505a2b27db2f}{%
           family={Biau},
           familyi={B\bibinitperiod},
           given={Gérard},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=33c7859e1f8c74639d1c015cbebd73e4}{%
           family={Devroye},
           familyi={D\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{fullhash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{fullhashraw}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{bibnamehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authorbibnamehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authornamehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authorfullhash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authorfullhashraw}{f5c2ac281cba7ff931a0404cb0c743e5}
      \field{extraname}{1}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Let X1,…,Xn be identically distributed random vectors in Rd, independently drawn according to some probability density. An observation Xi is said to be a layered nearest neighbour (LNN) of a point x if the hyperrectangle defined by x and Xi contains no other data points. We first establish consistency results on Ln(x), the number of LNN of x. Then, given a sample (X,Y),(X1,Y1),…,(Xn,Yn) of independent identically distributed random vectors from Rd×R, one may estimate the regression function r(x)=E[Y{|}X=x] by the LNN estimate rn(x), defined as an average over the Yi’s corresponding to those Xi which are LNN of x. Under mild conditions on r, we establish the consistency of E{|}rn(x)−r(x){|}p towards 0 as n→∞, for almost all x and all p≥1, and discuss the links between rn and the random forest estimates of Breiman (2001) [8]. We finally show the universal consistency of the bagged (bootstrap-aggregated) nearest neighbour method for regression and classification.}
      \field{issn}{0047-259X}
      \field{journaltitle}{Journal of Multivariate Analysis}
      \field{month}{11}
      \field{number}{10}
      \field{title}{On the layered nearest neighbour estimate, the bagged nearest neighbour estimate and the random forest method in regression and classification}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{101}
      \field{year}{2010}
      \field{urldateera}{ce}
      \field{pages}{2499\bibrangedash 2518}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1016/j.jmva.2010.06.019
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0047259X10001387
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0047259X10001387
      \endverb
      \keyw{Bagging,Layered nearest neighbours,One nearest neighbour estimate,Random forests,Regression estimation}
    \endentry
    \entry{biau_lectures_2015}{book}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=d905131276d4431aea7b505a2b27db2f}{%
           family={Biau},
           familyi={B\bibinitperiod},
           given={Gérard},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=33c7859e1f8c74639d1c015cbebd73e4}{%
           family={Devroye},
           familyi={D\bibinitperiod},
           given={Luc},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{fullhash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{fullhashraw}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{bibnamehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authorbibnamehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authornamehash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authorfullhash}{f5c2ac281cba7ff931a0404cb0c743e5}
      \strng{authorfullhashraw}{f5c2ac281cba7ff931a0404cb0c743e5}
      \field{extraname}{2}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-3-319-25386-2 978-3-319-25388-6}
      \field{series}{Springer {Series} in the {Data} {Sciences}}
      \field{title}{Lectures on the {Nearest} {Neighbor} {Method}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2015}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-319-25388-6
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/978-3-319-25388-6
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-319-25388-6
      \endverb
      \keyw{Density Estimation,Nearest Neighbor Method,Order Statistics,Regression Estimation,Stone's Theorem}
    \endentry
    \entry{breiman_random_2001}{article}{}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=132b7100417675d55d5d4d8b244f7a34}{%
           family={Breiman},
           familyi={B\bibinitperiod},
           given={Leo},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{fullhash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{fullhashraw}{132b7100417675d55d5d4d8b244f7a34}
      \strng{bibnamehash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{authorbibnamehash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{authornamehash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{authorfullhash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{authorfullhashraw}{132b7100417675d55d5d4d8b244f7a34}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}
      \field{issn}{1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{month}{10}
      \field{number}{1}
      \field{title}{Random {Forests}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{45}
      \field{year}{2001}
      \field{urldateera}{ce}
      \field{pages}{5\bibrangedash 32}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1023/A:1010933404324
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1010933404324
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1010933404324
      \endverb
      \keyw{Artificial Intelligence,classification,ensemble,regression}
    \endentry
    \entry{breiman_classification_2017}{book}{}{}
      \name{author}{4}{ul=1}{%
        {{un=0,uniquepart=base,hash=132b7100417675d55d5d4d8b244f7a34}{%
           family={Breiman},
           familyi={B\bibinitperiod},
           given={Leo},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=971cf69058b64a94326dea0f2a8bbcce}{%
           family={Friedman},
           familyi={F\bibinitperiod},
           given={Jerome},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=42eef899f6ca85b313c801e8bf869f5d}{%
           family={Olshen},
           familyi={O\bibinitperiod},
           given={R.\bibnamedelimi A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ed356c1829419cf046f711730f90941a}{%
           family={Stone},
           familyi={S\bibinitperiod},
           given={Charles\bibnamedelima J.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {New York}%
      }
      \list{publisher}{2}{%
        {Chapman}%
        {Hall/CRC}%
      }
      \strng{namehash}{3a2470c8fd470cc40d886536285a6eb3}
      \strng{fullhash}{37f267790381e9c48a0f1fdd5ea9651e}
      \strng{fullhashraw}{37f267790381e9c48a0f1fdd5ea9651e}
      \strng{bibnamehash}{3a2470c8fd470cc40d886536285a6eb3}
      \strng{authorbibnamehash}{3a2470c8fd470cc40d886536285a6eb3}
      \strng{authornamehash}{3a2470c8fd470cc40d886536285a6eb3}
      \strng{authorfullhash}{37f267790381e9c48a0f1fdd5ea9651e}
      \strng{authorfullhashraw}{37f267790381e9c48a0f1fdd5ea9651e}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.}
      \field{isbn}{978-1-315-13947-0}
      \field{month}{10}
      \field{title}{Classification and {Regression} {Trees}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.1201/9781315139470
      \endverb
    \endentry
    \entry{chen_randomized_2019}{article}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=6c6851240f65824ce08e14c46bc0d94f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Xiaohui},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=14b456424727553f557ab47ed30cc84f}{%
           family={Kato},
           familyi={K\bibinitperiod},
           given={Kengo},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{fullhash}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{fullhashraw}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{bibnamehash}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{authorbibnamehash}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{authornamehash}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{authorfullhash}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \strng{authorfullhashraw}{5bda6782dbf0f16d4b438c0a5b3db34a}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper studies inference for the mean vector of a high-dimensional \$U\$-statistic. In the era of big data, the dimension \$d\$ of the \$U\$-statistic and the sample size \$n\$ of the observations tend to be both large, and the computation of the \$U\$-statistic is prohibitively demanding. Data-dependent inferential procedures such as the empirical bootstrap for \$U\$-statistics is even more computationally expensive. To overcome such a computational bottleneck, incomplete \$U\$-statistics obtained by sampling fewer terms of the \$U\$-statistic are attractive alternatives. In this paper, we introduce randomized incomplete \$U\$-statistics with sparse weights whose computational cost can be made independent of the order of the \$U\$-statistic. We derive nonasymptotic Gaussian approximation error bounds for the randomized incomplete \$U\$-statistics in high dimensions, namely in cases where the dimension \$d\$ is possibly much larger than the sample size \$n\$, for both nondegenerate and degenerate kernels. In addition, we propose generic bootstrap methods for the incomplete \$U\$-statistics that are computationally much less demanding than existing bootstrap methods, and establish finite sample validity of the proposed bootstrap methods. Our methods are illustrated on the application to nonparametric testing for the pairwise independence of a high-dimensional random vector under weaker assumptions than those appearing in the literature.}
      \field{issn}{0090-5364, 2168-8966}
      \field{journaltitle}{The Annals of Statistics}
      \field{month}{12}
      \field{note}{Publisher: Institute of Mathematical Statistics}
      \field{number}{6}
      \field{title}{Randomized incomplete \${U}\$-statistics in high dimensions}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{47}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{3127\bibrangedash 3156}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1214/18-AOS1773
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-6/Randomized-incomplete-U-statistics-in-high-dimensions/10.1214/18-AOS1773.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-6/Randomized-incomplete-U-statistics-in-high-dimensions/10.1214/18-AOS1773.full
      \endverb
      \keyw{62E17,62F40,62H15,Bernoulli sampling,Divide and conquer,Gaussian approximation,bootstrap,incomplete \$U\$-statistics,randomized inference,sampling with replacement}
    \endentry
    \entry{chernozhukov_simple_2022}{article}{}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=497b6554670ef9f4dfd8d456efd3b41d}{%
           family={Chernozhukov},
           familyi={C\bibinitperiod},
           given={V},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=706b6cbc836d7f68c8a59223b16c843c}{%
           family={Newey},
           familyi={N\bibinitperiod},
           given={W\bibnamedelima K},
           giveni={W\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=535b46081523fc5c5dcfaba2a621d930}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={R},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{fullhash}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{fullhashraw}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{bibnamehash}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{authorbibnamehash}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{authornamehash}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{authorfullhash}{7a552ffd7001b2d5a51c32fe47000208}
      \strng{authorfullhashraw}{7a552ffd7001b2d5a51c32fe47000208}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Debiased machine learning is a meta-algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e., scalar summaries, of machine learning algorithms. For example, an analyst may seek the confidence interval for a treatment effect estimated with a neural network. We present a non-asymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation and semiparametric efficiency by finite-sample arguments. The rate of convergence is {\textbackslash}n{\textasciicircum}\{-1/2\}{\textbackslash} for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill-posed inverse problems.}
      \field{issn}{1464-3510}
      \field{journaltitle}{Biometrika}
      \field{month}{6}
      \field{note}{\_eprint: https://academic.oup.com/biomet/article-pdf/110/1/257/49160070/asac033.pdf}
      \field{number}{1}
      \field{title}{A simple and general debiased machine learning theorem with finite-sample guarantees}
      \field{volume}{110}
      \field{year}{2022}
      \field{pages}{257\bibrangedash 264}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1093/biomet/asac033
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1093/biomet/asac033
      \endverb
      \verb{url}
      \verb https://doi.org/10.1093/biomet/asac033
      \endverb
    \endentry
    \entry{chernozhukov_doubledebiased_2018}{article}{}{}
      \name{author}{7}{}{%
        {{un=2,uniquepart=given,hash=c75cbe2cefd1e532f636a50892605d06}{%
           family={Chernozhukov},
           familyi={C\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=2}}%
        {{un=0,uniquepart=base,hash=ef4b66d2ca2dde47ec1b0afa98306775}{%
           family={Chetverikov},
           familyi={C\bibinitperiod},
           given={Denis},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3c2cc443c71254678269de10d61d61c}{%
           family={Demirer},
           familyi={D\bibinitperiod},
           given={Mert},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7e6b0ea4eee38822cdb0a4361e98752c}{%
           family={Duflo},
           familyi={D\bibinitperiod},
           given={Esther},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b38b53cdfa3ce5b62046f22663d5cc6f}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9c98526d9f1775b95096664c9cf0c633}{%
           family={Newey},
           familyi={N\bibinitperiod},
           given={Whitney},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bf615cc72310078c444f0c653a1fc5e7}{%
           family={Robins},
           familyi={R\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e8ad45c8076487daa445bfbeb4e02a04}
      \strng{fullhash}{7892f5fdea2b46a0fb4e6c031c27c781}
      \strng{fullhashraw}{7892f5fdea2b46a0fb4e6c031c27c781}
      \strng{bibnamehash}{e8ad45c8076487daa445bfbeb4e02a04}
      \strng{authorbibnamehash}{e8ad45c8076487daa445bfbeb4e02a04}
      \strng{authornamehash}{e8ad45c8076487daa445bfbeb4e02a04}
      \strng{authorfullhash}{7892f5fdea2b46a0fb4e6c031c27c781}
      \strng{authorfullhashraw}{7892f5fdea2b46a0fb4e6c031c27c781}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}
      \field{issn}{1368-4221}
      \field{journaltitle}{The Econometrics Journal}
      \field{month}{2}
      \field{number}{1}
      \field{title}{Double/debiased machine learning for treatment and structural parameters}
      \field{urlday}{24}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{volume}{21}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{C1\bibrangedash C68}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1111/ectj.12097
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1111/ectj.12097
      \endverb
      \verb{url}
      \verb https://doi.org/10.1111/ectj.12097
      \endverb
    \endentry
    \entry{demirkaya_optimal_2024}{article}{}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=2b48b0983710378ce0bc9010163b46af}{%
           family={Demirkaya},
           familyi={D\bibinitperiod},
           given={Emre},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=84e992947ee6777f06640104c0dbbf0e}{%
           family={Fan},
           familyi={F\bibinitperiod},
           given={Yingying},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5ba25d7c8db3eff6411ff04c4e94062}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Lan},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0f66fa2ea251428c6943aaa92cd79812}{%
           family={Lv},
           familyi={L\bibinitperiod},
           given={Jinchi},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d8033b252f35c23bc730b3c2683fd0e}{%
           family={Vossler},
           familyi={V\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d02bba7e4893c3e13e5c1fd3739858c}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Jingbo},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{84bffa369faa31b1c5d7097ab6220104}
      \strng{fullhash}{2d477f98c2cfe21eed7652736b492dc8}
      \strng{fullhashraw}{2d477f98c2cfe21eed7652736b492dc8}
      \strng{bibnamehash}{84bffa369faa31b1c5d7097ab6220104}
      \strng{authorbibnamehash}{84bffa369faa31b1c5d7097ab6220104}
      \strng{authornamehash}{84bffa369faa31b1c5d7097ab6220104}
      \strng{authorfullhash}{2d477f98c2cfe21eed7652736b492dc8}
      \strng{authorfullhashraw}{2d477f98c2cfe21eed7652736b492dc8}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The weighted nearest neighbors (WNN) estimator has been popularly used as a flexible and easy-to-implement nonparametric tool for mean regression estimation. The bagging technique is an elegant way to form WNN estimators with weights automatically generated to the nearest neighbors (Steele 2009; Biau, Cérou, and Guyader 2010); we name the resulting estimator as the distributional nearest neighbors (DNN) for easy reference. Yet, there is a lack of distributional results for such estimator, limiting its application to statistical inference. Moreover, when the mean regression function has higher-order smoothness, DNN does not achieve the optimal nonparametric convergence rate, mainly because of the bias issue. In this work, we provide an in-depth technical analysis of the DNN, based on which we suggest a bias reduction approach for the DNN estimator by linearly combining two DNN estimators with different subsampling scales, resulting in the novel two-scale DNN (TDNN) estimator. The two-scale DNN estimator has an equivalent representation of WNN with weights admitting explicit forms and some being negative. We prove that, thanks to the use of negative weights, the two-scale DNN estimator enjoys the optimal nonparametric rate of convergence in estimating the regression function under the fourth-order smoothness condition. We further go beyond estimation and establish that the DNN and two-scale DNN are both asymptotically normal as the subsampling scales and sample size diverge to infinity. For the practical implementation, we also provide variance estimators and a distribution estimator using the jackknife and bootstrap techniques for the two-scale DNN. These estimators can be exploited for constructing valid confidence intervals for nonparametric inference of the regression function. The theoretical results and appealing finite-sample performance of the suggested two-scale DNN method are illustrated with several simulation examples and a real data application.}
      \field{issn}{0162-1459}
      \field{journaltitle}{Journal of the American Statistical Association}
      \field{month}{1}
      \field{note}{Publisher: ASA Website \_eprint: https://doi.org/10.1080/01621459.2022.2115375}
      \field{number}{545}
      \field{title}{Optimal {Nonparametric} {Inference} with {Two}-{Scale} {Distributional} {Nearest} {Neighbors}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{119}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{297\bibrangedash 307}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1080/01621459.2022.2115375
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1080/01621459.2022.2115375
      \endverb
      \verb{url}
      \verb https://doi.org/10.1080/01621459.2022.2115375
      \endverb
      \keyw{Bagging,Bootstrap and jackknife,Nonparametric estimation and inference,Two-scale distributional nearest neighbors,Weighted nearest neighbors,k-nearest neighbors}
    \endentry
    \entry{hoeffding_class_1948}{article}{}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=26d3c01664dc4b0c60afbdf4153437e2}{%
           family={Hoeffding},
           familyi={H\bibinitperiod},
           given={Wassily},
           giveni={W\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{fullhash}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{fullhashraw}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{bibnamehash}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{authorbibnamehash}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{authornamehash}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{authorfullhash}{26d3c01664dc4b0c60afbdf4153437e2}
      \strng{authorfullhashraw}{26d3c01664dc4b0c60afbdf4153437e2}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Let \$X\_1, {\textbackslash}cdot, X\_n\$ be \$n\$ independent random vectors, \$X\_{\textbackslash}nu = (X{\textasciicircum}\{(1)\}\_{\textbackslash}nu, {\textbackslash}cdots, X{\textasciicircum}\{(r)\}\_{\textbackslash}nu),\$ and \${\textbackslash}Phi(x\_1, {\textbackslash}cdots, x\_m)\$ a function of \$m({\textbackslash}leq n)\$ vectors \$x\_{\textbackslash}nu = (x{\textasciicircum}\{(1)\}\_{\textbackslash}nu , {\textbackslash}cdots, x{\textasciicircum}\{(r)\}\_{\textbackslash}nu)\$. A statistic of the form \$U = {\textbackslash}sum"{\textbackslash}Phi(X\_\{{\textbackslash}alpha 1\}, {\textbackslash}cdots, X\_\{{\textbackslash}alpha\_m\})/n(n - 1) {\textbackslash}cdots (n - m + 1),\$ where the sum \${\textbackslash}sum"\$ is extended over all permutations \$({\textbackslash}alpha\_1, {\textbackslash}cdots, {\textbackslash}alpha\_m)\$ of \$m\$ different integers, \$1 {\textbackslash}leq {\textbackslash}alpha\_i {\textbackslash}leq n\$, is called a \$U\$-statistic. If \$X\_1, {\textbackslash}cdots, X\_n\$ have the same (cumulative) distribution function (d.f.) \$F(x), U\$ is an unbiased estimate of the population characteristic \${\textbackslash}theta(F) = {\textbackslash}int {\textbackslash}cdots {\textbackslash}int{\textbackslash}Phi(x\_1, {\textbackslash}cdots, x\_m) dF(x\_1) {\textbackslash}cdots dF(x\_m). {\textbackslash}theta(F)\$ is called a regular functional of the d.f. \$F(x)\$. Certain optimal properties of \$U\$-statistics as unbiased estimates of regular functionals have been established by Halmos [9] (cf. Section 4). The variance of a \$U\$-statistic as a function of the sample size \$n\$ and of certain population characteristics is studied in Section 5. It is shown that if \$X\_1, {\textbackslash}cdots, X\_n\$ have the same distribution and \${\textbackslash}Phi(x\_1, {\textbackslash}cdots, x\_m)\$ is independent of \$n\$, the d.f. of \${\textbackslash}sqrt n(U - {\textbackslash}theta)\$ tends to a normal d.f. as \$n {\textbackslash}rightarrow {\textbackslash}infty\$ under the sole condition of the existence of \$E{\textbackslash}Phi{\textasciicircum}2(X\_1, {\textbackslash}cdots, X\_m)\$. Similar results hold for the joint distribution of several \$U\$-statistics (Theorems 7.1 and 7.2), for statistics \$U'\$ which, in a certain sense, are asymptotically equivalent to \$U\$ (Theorems 7.3 and 7.4), for certain functions of statistics \$U\$ or \$U'\$ (Theorem 7.5) and, under certain additional assumptions, for the case of the \$X\_{\textbackslash}nu\$'s having different distributions (Theorems 8.1 and 8.2). Results of a similar character, though under different assumptions, are contained in a recent paper by von Mises [18] (cf. Section 7). Examples of statistics of the form \$U\$ or \$U'\$ are the moments, Fisher's \$k\$-statistics, Gini's mean difference, and several rank correlation statistics such as Spearman's rank correlation and the difference sign correlation (cf. Section 9). Asymptotic power functions for the non-parametric tests of independence based on these rank statistics are obtained. They show that these tests are not unbiased in the limit (Section 9f). The asymptotic distribution of the coefficient of partial difference sign correlation which has been suggested by Kendall also is obtained (Section 9h).}
      \field{issn}{0003-4851, 2168-8990}
      \field{journaltitle}{The Annals of Mathematical Statistics}
      \field{month}{9}
      \field{note}{Publisher: Institute of Mathematical Statistics}
      \field{number}{3}
      \field{title}{A {Class} of {Statistics} with {Asymptotically} {Normal} {Distribution}}
      \field{urlday}{3}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{volume}{19}
      \field{year}{1948}
      \field{urldateera}{ce}
      \field{pages}{293\bibrangedash 325}
      \range{pages}{33}
      \verb{doi}
      \verb 10.1214/aoms/1177730196
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-19/issue-3/A-Class-of-Statistics-with-Asymptotically-Normal-Distribution/10.1214/aoms/1177730196.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-19/issue-3/A-Class-of-Statistics-with-Asymptotically-Normal-Distribution/10.1214/aoms/1177730196.full
      \endverb
    \endentry
    \entry{lee_u-statistics_2019}{book}{}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=2bcf6cad251d975b02d256f56b7281d6}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={A\bibnamedelima J.},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \list{publisher}{1}{%
        {Routledge}%
      }
      \strng{namehash}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{fullhash}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{fullhashraw}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{bibnamehash}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{authorbibnamehash}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{authornamehash}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{authorfullhash}{2bcf6cad251d975b02d256f56b7281d6}
      \strng{authorfullhashraw}{2bcf6cad251d975b02d256f56b7281d6}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{edition}{0}
      \field{isbn}{978-0-203-73452-0}
      \field{month}{3}
      \field{title}{U-{Statistics}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1201/9780203734520
      \endverb
      \verb{urlraw}
      \verb https://www.taylorfrancis.com/books/9781351405867
      \endverb
      \verb{url}
      \verb https://www.taylorfrancis.com/books/9781351405867
      \endverb
    \endentry
    \entry{lin_random_2006}{article}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=83462145d1960e68e830e4b0dd4314ef}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Yi},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4f21b7c0154e88b51e06880a7636b54f}{%
           family={Jeon},
           familyi={J\bibinitperiod},
           given={Yongho},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{fullhash}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{fullhashraw}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{bibnamehash}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{authorbibnamehash}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{authornamehash}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{authorfullhash}{5578a9ab9ab72aecfd31a46cf845d294}
      \strng{authorfullhashraw}{5578a9ab9ab72aecfd31a46cf845d294}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0162-1459, 1537-274X}
      \field{journaltitle}{Journal of the American Statistical Association}
      \field{month}{6}
      \field{number}{474}
      \field{title}{Random {Forests} and {Adaptive} {Nearest} {Neighbors}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{101}
      \field{year}{2006}
      \field{urldateera}{ce}
      \field{pages}{578\bibrangedash 590}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1198/016214505000001230
      \endverb
      \verb{urlraw}
      \verb https://www.tandfonline.com/doi/full/10.1198/016214505000001230
      \endverb
      \verb{url}
      \verb https://www.tandfonline.com/doi/full/10.1198/016214505000001230
      \endverb
    \endentry
    \entry{peng_rates_2022}{article}{}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=0ffbf7da53582dbc0a920a400eddc057}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d44622ed0304b58ed1f4a7462c02f8fa}{%
           family={Coleman},
           familyi={C\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3570b0ad408f1718fc65049b825ec7b}{%
           family={Mentch},
           familyi={M\bibinitperiod},
           given={Lucas},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{142449543f9679940960a838d9b77eae}
      \strng{fullhash}{142449543f9679940960a838d9b77eae}
      \strng{fullhashraw}{142449543f9679940960a838d9b77eae}
      \strng{bibnamehash}{142449543f9679940960a838d9b77eae}
      \strng{authorbibnamehash}{142449543f9679940960a838d9b77eae}
      \strng{authornamehash}{142449543f9679940960a838d9b77eae}
      \strng{authorfullhash}{142449543f9679940960a838d9b77eae}
      \strng{authorfullhashraw}{142449543f9679940960a838d9b77eae}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1935-7524}
      \field{journaltitle}{Electronic Journal of Statistics}
      \field{month}{1}
      \field{number}{1}
      \field{title}{Rates of convergence for random forests via generalized {U}-statistics}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{16}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1214/21-EJS1958
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-16/issue-1/Rates-of-convergence-for-random-forests-via-generalized-U-statistics/10.1214/21-EJS1958.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-16/issue-1/Rates-of-convergence-for-random-forests-via-generalized-U-statistics/10.1214/21-EJS1958.full
      \endverb
    \endentry
    \entry{peng_bias_2021}{misc}{}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=0ffbf7da53582dbc0a920a400eddc057}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3570b0ad408f1718fc65049b825ec7b}{%
           family={Mentch},
           familyi={M\bibinitperiod},
           given={Lucas},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e28015179b1409753c01e4f32825becd}{%
           family={Stefanski},
           familyi={S\bibinitperiod},
           given={Leonard},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{fullhash}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{fullhashraw}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{bibnamehash}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{authorbibnamehash}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{authornamehash}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{authorfullhash}{dd598c44c68b01a6d858f3abd17526d5}
      \strng{authorfullhashraw}{dd598c44c68b01a6d858f3abd17526d5}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Though introduced nearly 50 years ago, the infinitesimal jackknife (IJ) remains a popular modern tool for quantifying predictive uncertainty in complex estimation settings. In particular, when supervised learning ensembles are constructed via bootstrap samples, recent work demonstrated that the IJ estimate of variance is particularly convenient and useful. However, despite the algebraic simplicity of its final form, its derivation is rather complex. As a result, studies clarifying the intuition behind the estimator or rigorously investigating its properties have been severely lacking. This work aims to take a step forward on both fronts. We demonstrate that surprisingly, the exact form of the IJ estimator can be obtained via a straightforward linear regression of the individual bootstrap estimates on their respective weights or via the classical jackknife. The latter realization is particularly useful as it allows us to formally investigate the bias of the IJ variance estimator and better characterize the settings in which its use is appropriate. Finally, we extend these results to the case of U-statistics where base models are constructed via subsampling rather than bootstrapping and provide a consistent estimate of the resulting variance.}
      \field{note}{Version Number: 1}
      \field{title}{Bias, {Consistency}, and {Alternative} {Perspectives} of the {Infinitesimal} {Jackknife}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/ARXIV.2106.05918
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/2106.05918
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/2106.05918
      \endverb
      \keyw{FOS: Mathematics,Statistics Theory (math.ST)}
    \endentry
    \entry{ritzwoller_simultaneous_2024}{misc}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=4d6a912596e242b0cc13cd623ebb5e3f}{%
           family={Ritzwoller},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima M.},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=342033b26b8f8c243380f2343a53e1fa}{%
           family={Syrgkanis},
           familyi={S\bibinitperiod},
           given={Vasilis},
           giveni={V\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{fullhash}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{fullhashraw}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{bibnamehash}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{authorbibnamehash}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{authornamehash}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{authorfullhash}{04822a1dff3956e0aa8be2d94fdd90e3}
      \strng{authorfullhashraw}{04822a1dff3956e0aa8be2d94fdd90e3}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We construct simultaneous confidence intervals for solutions to conditional moment equations. The intervals are built around a class of nonparametric regression algorithms based on subsampled kernels. This class encompasses various forms of subsampled random forest regression, including Generalized Random Forests (Athey et al., 2019). Although simultaneous validity is often desirable in practice -- for example, for fine-grained characterization of treatment effect heterogeneity -- only confidence intervals that confer pointwise guarantees were previously available. Our work closes this gap. As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional U-statistics.}
      \field{month}{9}
      \field{note}{arXiv:2405.07860}
      \field{title}{Simultaneous {Inference} for {Local} {Structural} {Parameters} with {Random} {Forests}}
      \field{urlday}{9}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{year}{2024}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2405.07860
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2405.07860
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2405.07860
      \endverb
      \keyw{Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory}
    \endentry
    \entry{semenova_debiased_2021}{article}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=2c89c4ee946e74fb529d0d7257d1353d}{%
           family={Semenova},
           familyi={S\bibinitperiod},
           given={Vira},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=2,uniquepart=given,hash=c75cbe2cefd1e532f636a50892605d06}{%
           family={Chernozhukov},
           familyi={C\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=2}}%
      }
      \strng{namehash}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{fullhash}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{fullhashraw}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{bibnamehash}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{authorbibnamehash}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{authornamehash}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{authorfullhash}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \strng{authorfullhashraw}{e7c1fa2bb015bd60e1bf1449f3d82346}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper provides estimation and inference methods for the best linear predictor (approximation) of a structural function, such as conditional average structural and treatment effects, and structural derivatives, based on modern machine learning tools. We represent this structural function as a conditional expectation of an unbiased signal that depends on a nuisance parameter, which we estimate by modern machine learning techniques. We first adjust the signal to make it insensitive (Neyman-orthogonal) with respect to the first-stage regularisation bias. We then project the signal onto a set of basis functions, which grow with sample size, to get the best linear predictor of the structural function. We derive a complete set of results for estimation and simultaneous inference on all parameters of the best linear predictor, conducting inference by Gaussian bootstrap. When the structural function is smooth and the basis is sufficiently rich, our estimation and inference results automatically target this function. When basis functions are group indicators, the best linear predictor reduces to the group average treatment/structural effect, and our inference automatically targets these parameters. We demonstrate our method by estimating uniform confidence bands for the average price elasticity of gasoline demand conditional on income.}
      \field{issn}{1368-4221}
      \field{journaltitle}{The Econometrics Journal}
      \field{month}{5}
      \field{number}{2}
      \field{title}{Debiased machine learning of conditional average treatment effects and other causal functions}
      \field{urlday}{1}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{24}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{264\bibrangedash 289}
      \range{pages}{26}
      \verb{doi}
      \verb 10.1093/ectj/utaa027
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1093/ectj/utaa027
      \endverb
      \verb{url}
      \verb https://doi.org/10.1093/ectj/utaa027
      \endverb
    \endentry
    \entry{song_approximating_2019}{article}{}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=2c4a63a1f2932bedc5ee688c91746ac0}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Yanglei},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6c6851240f65824ce08e14c46bc0d94f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Xiaohui},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=14b456424727553f557ab47ed30cc84f}{%
           family={Kato},
           familyi={K\bibinitperiod},
           given={Kengo},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{add813a5457694f2d5f657b32f65d049}
      \strng{fullhash}{add813a5457694f2d5f657b32f65d049}
      \strng{fullhashraw}{add813a5457694f2d5f657b32f65d049}
      \strng{bibnamehash}{add813a5457694f2d5f657b32f65d049}
      \strng{authorbibnamehash}{add813a5457694f2d5f657b32f65d049}
      \strng{authornamehash}{add813a5457694f2d5f657b32f65d049}
      \strng{authorfullhash}{add813a5457694f2d5f657b32f65d049}
      \strng{authorfullhashraw}{add813a5457694f2d5f657b32f65d049}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We study the problem of distributional approximations to high-dimensional non-degenerate \$U\$-statistics with random kernels of diverging orders. Infinite-order \$U\$-statistics (IOUS) are a useful tool for constructing simultaneous prediction intervals that quantify the uncertainty of ensemble methods such as subbagging and random forests. A major obstacle in using the IOUS is their computational intractability when the sample size and/or order are large. In this article, we derive non-asymptotic Gaussian approximation error bounds for an incomplete version of the IOUS with a random kernel. We also study data-driven inferential methods for the incomplete IOUS via bootstraps and develop their statistical and computational guarantees.}
      \field{issn}{1935-7524, 1935-7524}
      \field{journaltitle}{Electronic Journal of Statistics}
      \field{month}{1}
      \field{note}{Publisher: Institute of Mathematical Statistics and Bernoulli Society}
      \field{number}{2}
      \field{shorttitle}{Approximating high-dimensional infinite-order \${U}\$-statistics}
      \field{title}{Approximating high-dimensional infinite-order \${U}\$-statistics: {Statistical} and computational guarantees}
      \field{urlday}{5}
      \field{urlmonth}{10}
      \field{urlyear}{2024}
      \field{volume}{13}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{4794\bibrangedash 4848}
      \range{pages}{55}
      \verb{doi}
      \verb 10.1214/19-EJS1643
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-13/issue-2/Approximating-high-dimensional-infinite-order-U-statistics--Statistical-and/10.1214/19-EJS1643.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-13/issue-2/Approximating-high-dimensional-infinite-order-U-statistics--Statistical-and/10.1214/19-EJS1643.full
      \endverb
      \keyw{Gaussian approximation,Infinite-order \$U\$-statistics,bootstrap,incomplete \$U\$ statistics,random forests,uncertainty quantification}
    \endentry
    \entry{steele_exact_2009}{article}{}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=6baf55f8936b01e7e62c1a8f966b2daf}{%
           family={Steele},
           familyi={S\bibinitperiod},
           given={Brian\bibnamedelima M.},
           giveni={B\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{fullhash}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{fullhashraw}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{bibnamehash}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{authorbibnamehash}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{authornamehash}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{authorfullhash}{6baf55f8936b01e7e62c1a8f966b2daf}
      \strng{authorfullhashraw}{6baf55f8936b01e7e62c1a8f966b2daf}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0885-6125, 1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{month}{3}
      \field{number}{3}
      \field{title}{Exact bootstrap k-nearest neighbor learners}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{74}
      \field{year}{2009}
      \field{urldateera}{ce}
      \field{pages}{235\bibrangedash 255}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1007/s10994-008-5096-0
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/s10994-008-5096-0
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/s10994-008-5096-0
      \endverb
    \endentry
    \entry{wager_estimation_2018}{article}{}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=e22c2c3bb1bd12e43f46c07ea788b581}{%
           family={Wager},
           familyi={W\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cab7b4f39472e6c01dc0fc30a32c6d89}{%
           family={Athey},
           familyi={A\bibinitperiod},
           given={Susan},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{fullhash}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{fullhashraw}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{bibnamehash}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{authorbibnamehash}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{authornamehash}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{authorfullhash}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \strng{authorfullhashraw}{d4c64d08a4af7035a4061fd6e82a6d2d}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0162-1459, 1537-274X}
      \field{journaltitle}{Journal of the American Statistical Association}
      \field{month}{7}
      \field{number}{523}
      \field{title}{Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{113}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{1228\bibrangedash 1242}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1080/01621459.2017.1319839
      \endverb
      \verb{urlraw}
      \verb https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839
      \endverb
      \verb{url}
      \verb https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1319839
      \endverb
    \endentry
  \enddatalist
  \missing{ritzwoller_uniform_2024}
  \missing{wager2014confidence}
\endrefsection
\endinput

