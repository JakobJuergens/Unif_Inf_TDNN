\section{Useful Results}
\hrule

\begin{lem}[\citet{demirkaya_optimal_2024} - Lemma 12]\label{lem:dem12}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_s\}$ an i.i.d.\ sample drawn from $P$.
	The indicator functions $\kappa\left(x; Z_{i}, D\right)$ satisfy the following properties.
	\begin{enumerate}
		\item For any $i \neq j$, we have $\kappa\left(x; Z_{i}, D\right) \kappa\left(x; Z_{j}, D\right)=0$ with probability one;
		\item $\sum_{i=1}^{s} \kappa\left(x; Z_{i}, D\right)=1$;
		\item $\forall i \in [s]: \quad \E_{1:s}\left[\kappa\left(x; Z_{i}, D\right)\right]=s^{-1}$
		\item $\E_{2: s}\left[\kappa\left(x; Z_1, D\right)\right]=\left\{1-\varphi\left(B\left(x,\left\|X_1-x\right\|\right)\right)\right\}^{s-1}$
	\end{enumerate}
	Here $\E_{i: s}$ denotes the expectation with respect to $\left\{Z_{i}, Z_{i+1}, \dotsc, Z_s\right\}$.
	Furthermore, $\varphi$ denotes the probability measure on $\mathbb{R}^{d}$ induced by the random vector $X$.
\end{lem}

\hrule

\begin{lem}[\citet{demirkaya_optimal_2024} - Lemma 13]\label{lem:dem13}\mbox{}\\*
	For any $L^1$ function $f$ that is continuous at $x$, it holds that
	\begin{equation}
		\lim _{s \rightarrow \infty} \E_{1}\left[f\left(X_1\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
		= f(x).
	\end{equation}
\end{lem}

\hrule

\begin{lem}\label{lem:limit_res}\mbox{}\\*
	As a consequence of Lemma~\ref{lem:dem13}, we find the following limit results in the nonparametric regression setup.
	\begin{equation}
		\begin{aligned}
			\lim_{s \rightarrow \infty} \E_{1}\left[Y_1 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \mu\left(x\right)
		\end{aligned}
	\end{equation}
	\begin{equation}
		\begin{aligned}
			\lim_{s \rightarrow \infty} \E_{1}\left[Y_1^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \mu^2\left(x\right) + \sigma_{\varepsilon}^{2}(x)
			\leq \mu^2\left(x\right) + \overline{\sigma}_{\varepsilon}^{2}
		\end{aligned}
	\end{equation}
	Similarly, in the CATE estimation setup, we can make the following observations.
	\begin{equation}
		\begin{aligned}
			\lim_{s \rightarrow \infty} \E_{1}\left[m\left(Z_{1}; \mu, \pi\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \mu_1\left(x\right) - \mu_0\left(x\right)
		\end{aligned}
	\end{equation}          
	\begin{equation}
		\begin{aligned}
			\lim_{s \rightarrow \infty} \E_{1}\left[m^2\left(Z_{1}; \mu, \pi\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \left(\mu_1\left(x\right) - \mu_0\left(x\right)\right)^2 + \frac{\sigma_{\varepsilon}^2(x)}{\pi\left(x\right)\left(1 - \pi\left(x\right)\right)}\\
			%
			& \leq \left(\mu_1\left(x\right) - \mu_0\left(x\right)\right)^2 + \frac{\overline{\sigma}_{\varepsilon}^2}{\mathfrak{p}\left(1 - \mathfrak{p}\right)}
		\end{aligned}
	\end{equation}
\end{lem}

\hrule

\begin{proof}[Proof of Lemma~\ref{lem:limit_res}]
	Starting with the first limit, we find the following.
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[Y_1 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \E_{1}\left[\left(\mu\left(X_1\right) + \varepsilon_1\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& = \E_{1}\left[\left(\mu\left(X_1\right) + \E\left[\varepsilon_1 \, \middle| \, X_1\right]\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& = \E_{1}\left[\mu\left(X_1\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			\overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \mu\left(x\right)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
	Similarly, when considering the second limit, we can make the following observation.
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[Y_1^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \E_{1}\left[\left(\mu\left(X_1\right) + \varepsilon_1\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& = \E_{1}\left[\left(\mu^2\left(X_1\right) + 2\mu\left(X_1\right)\varepsilon_1 + \varepsilon_1^2\right)s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& = \E_{1}\left[\left(\mu^2\left(X_1\right) + 2\mu\left(X_1\right) \E\left[\varepsilon_1 \, \middle| \, X_1\right] + \E\left[\varepsilon_1^2 \, \middle| \, X_1\right]\right)
			s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			& = \E_{1}\left[\left(\mu^2\left(X_1\right) +\sigma_{\varepsilon}^{2}(X_1)\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			& \overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \mu^2\left(x\right) +\sigma_{\varepsilon}^{2}(x)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
	In the CATE estimation setting, we can proceed analogously.
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[m\left(Z_{1}; \mu, \pi\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			& = \E_{1}\left[\left(\mu_1\left(X_{1}\right) - \mu_0\left(X_{1}\right) + \beta\left(W_{1}, X_{1}\right)\varepsilon_{i}\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			& = \E_{1}\left[\left(\mu_1\left(X_{1}\right) - \mu_0\left(X_{1}\right) + \beta\left(W_{1}, X_{1}\right)\E\left[\varepsilon_{i} \, \middle| \, \right]\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			& = \E_{1}\left[\left(\mu_1\left(X_{1}\right) - \mu_0\left(X_{1}\right)\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			& \overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \mu_1\left(x\right) - \mu_0\left(x\right)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
	Similarly, we can find the following.
	\begin{equation}
		\begin{aligned}
			& \E_{1}\left[m^2\left(Z_{i}; \mu, \pi\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			= \E_{1}\left[\left(\mu_1\left(X_{1}\right) - \mu_0\left(X_{1}\right) + \beta\left(W_{1}, X_{1}\right)\varepsilon_{i}\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			& \quad = \E_{1}\left[\left(\mu_1\left(X_{i}\right) - \mu_0\left(X_{1}\right)\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			+ \underbrace{\E_{1}\left[\left(\mu_1\left(X_{i}\right) - \mu_0\left(X_{1}\right)\right)\beta\left(W_{1}, X_{1}\right)\E\left[\varepsilon_{1} \, \middle| \, X_1\right] s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]}_{=0} \\
			& \quad \quad + \E_{1}\left[\left(\beta\left(W_{1}, X_{1}\right)\varepsilon_{i}\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& \quad =  \E_{1}\left[\left(\mu_1\left(X_{1}\right) - \mu_0\left(X_{1}\right)\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			+ \E_{1}\left[\left(\frac{W_{1}}{\pi\left(X_1\right)} - \frac{1 - W_{1}}{1 - \pi\left(X_1\right)}\right)^2 \varepsilon_{1}^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& \quad = \underbrace{\E_{1}\left[\left(\mu_1\left(X_{1}\right) - \mu_0\left(X_{1}\right)\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]}_{\longrightarrow \left(\mu_1\left(x\right) - \mu_0\left(x\right)\right)^2 \quad \text{as} \quad s \rightarrow \infty}
			+ \underbrace{\E_{1}\left[\E\left[\left(\frac{W_{1}}{\pi\left(X_1\right)} - \frac{1 - W_{1}}{1 - \pi\left(X_1\right)}\right)^2 \varepsilon_{1}^2 \, \middle| \, X_1\right] s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]}_{(B)}
		\end{aligned}
	\end{equation}
	Continuing with the second term, marked by $(B)$, we find the following.
	\begin{equation}
		\begin{aligned}
			(B) 
			& = \E_{1}\left[\E\left[\left(\frac{W_{1}}{\pi\left(X_1\right)} - \frac{1 - W_{1}}{1 - \pi\left(X_1\right)}\right)^2 \varepsilon_{1}^2 \, \middle| \, X_1\right] s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& = \E_{1}\left[\frac{\sigma_{\varepsilon}^2(X_1) \cdot s \E_{2:s}\left[\kappa(x; Z_1, D)\right]}{\pi^2\left(X_1\right)\left(1 - \pi\left(X_1\right)\right)^2} \cdot 
			\E\left[\left(W_{1}\left(1 - \pi\left(X_1\right)\right) - \left(1 - W_{1}\right)\pi\left(X_1\right)\right)^2 \, \middle| \, X_1\right] \right]\\
		\end{aligned}
	\end{equation}
	Observe that $W_1(1-W_1) = 0$, $W_1^2 = W_1$, and $(1-W_1)^2 = 1 - W_1$, which allows us to use the following simplification.
	\begin{equation}
		\begin{aligned}
			(B) 
			& = \E_{1}\left[\frac{\sigma_{\varepsilon}^2(X_1) \cdot s \E_{2:s}\left[\kappa(x; Z_1, D)\right]}{\pi^2\left(X_1\right)\left(1 - \pi\left(X_1\right)\right)^2} \cdot 
			\E\left[W_{1}\left(1 - \pi\left(X_1\right)\right)^2 + \left(1 - W_{1}\right)\pi^2\left(X_1\right) \, \middle| \, X_1\right] \right]\\
			%
			& = \E_{1}\left[\frac{\sigma_{\varepsilon}^2(X_1) \cdot s \E_{2:s}\left[\kappa(x; Z_1, D)\right]}{\pi^2\left(X_1\right)\left(1 - \pi\left(X_1\right)\right)^2} \cdot 
			\left(\pi(X_1)\left(1 - \pi\left(X_1\right)\right)^2 + (1 - \pi(X_1))\pi^2\left(X_1\right) \right)\right]\\
			%
			& = \E_{1}\left[\frac{\sigma_{\varepsilon}^2(X_1) \cdot s \E_{2:s}\left[\kappa(x; Z_1, D)\right]}{\pi\left(X_1\right)\left(1 - \pi\left(X_1\right)\right)}\right]
			\overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \frac{\sigma_{\varepsilon}^2(x)}{\pi\left(x\right)\left(1 - \pi\left(x\right)\right)}
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
	Recombining the terms of interest, we find the desired limit bound.
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[m^2\left(Z_{i}; \mu, \pi\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			\overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \left(\mu_1\left(x\right) - \mu_0\left(x\right)\right)^2 + \frac{\sigma_{\varepsilon}^2(x)}{\pi\left(x\right)\left(1 - \pi\left(x\right)\right)}
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

\begin{lem}\label{lem:kern_ineq}\mbox{}\\*
	Fix sample size $n$, subsampling scale $s$, and $c$ such that $0 < c \leq s \leq n$.
	Let $D = \left\{Z_1, Z_2, \dotsc, Z_c, Z_{c+1}, \dotsc Z_s \right\}$ be an i.i.d.\ data set drawn from $P$ as described in Setup~\ref{asm:npr_dgp}.
	Let $D^{\prime} = \left\{Z_1, Z_2, \dotsc, Z_c, Z_{c+1}^{\prime}, \dotsc Z_s^{\prime} \right\}$ be a second data set that shares the first $c$ observations with $D$.
	The remaining $s - c$ observations of $D^{\prime}$, i.e.\ $\left\{Z_{c+1}^{\prime}, \dotsc Z_s^{\prime} \right\}$, are i.i.d.\ draws from $P$ that are independent of $D$.

	Then, the following inequality holds.
	\begin{equation}
		\begin{aligned}
			\E_{D, D^{\prime}}\left[Y_{1}Y_{c+1}^{\prime} \, c(s-c) \, \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]
			& \leq {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}

	Similarly, in the CATE estimation setting (Setup~\ref{asm:hte_dgp}), i.e.\ replacing observations drawn from $P$ by observations drawn from $Q$, the following inequality holds.
	\begin{equation}
		\begin{aligned}
			\E_{D, D^{\prime}}\left[m\left(Z_{1}; \mu, \pi\right) m\left(Z_{c+1}^{\prime}; \mu, \pi\right) \, c(s-c) \, \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]
			& \leq {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}
\end{lem}

\hrule

\begin{proof}[Proof of Lemma~\ref{lem:kern_ineq}]\mbox{}\\*
	Consider first the following argument.
	\begin{equation}
		\begin{aligned}
			& \E_{D, D^{\prime}}\left[Y_{1}Y_{c+1}^{\prime} \, c(s-c) \, \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]
			= {\color{red} LOREM IPSUM}
		\end{aligned}
	\end{equation}
	{\color{red} LOREM IPSUM}
\end{proof}

\hrule

\begin{lem}[\citet{peng_bias_2021} - Lemma 1]\label{lem:peng1}\mbox{}\\*
	Suppose that $\sum X_{i}^2 \xrightarrow{p} 1, \sum \E\left[X_{i}^2\right] \rightarrow 1$, and $\sum_{i=1}^n \E\left[Y_{i}^2\right] \rightarrow 0$, then
	\begin{equation}
		\sum\left[X_{i}+Y_{i}\right]^2 \xrightarrow{p} 1 \quad \text { and } \E\left[\sum\left(X_{i}+Y_{i}\right)^2\right] \rightarrow 1.
	\end{equation}
\end{lem}

\hrule

\begin{lem}[Honesty of the DNN/TDNN Estimators]\label{lem:honesty}\mbox{}\\*
	The DNN and TDNN estimator kernels $\kappa\left(\cdot, \cdot, D_{\ell}\right)$ are Honest in the sense of \citet{wager_estimation_2018}.
	\begin{equation*}
		\kappa\left(x, X_{i}, D_{\ell}\right) \indep Y_{i} \mid X_{i}, D_{\ell,-i},
	\end{equation*}
	where $\indep$ denotes conditional independence and $D_{\ell,-i} = \{Z_l \, | \, l \in \ell \backslash \{i\}\}$.
\end{lem}