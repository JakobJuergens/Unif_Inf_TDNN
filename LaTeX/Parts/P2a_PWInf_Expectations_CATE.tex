\subsection{CATE - Kernel (Conditional) Expectations}\label{sec:CATE_exp}
\hrule
Next, we address the CATE estimation setup, where we first consider the scenario where the nuisance parameters are assumed to be known a priori.
In a second step, we will show that asymptotically, the estimation of nuisance parameters as described in Definition \ref{def:CATE_DNN_DML}, does not alter the asymptotic analysis of the estimator.
For clarity, we point out that in contexts relating to the estimation of the conditional average treatment effect, the kernel or score function $h_s$ could hypothetically signify the first or second stage kernel.
As the first stage is effectively covered by the nonparametric regression setup, we will take $h_s$ in these contexts to mean the kernel weighted Neyman-orthogonal score associated with the CATE.
\vspace{0.5cm}
\hrule

\begin{boxD}
    \begin{lem}[CATE - DNN Kernel Expectation]\label{lem:CATE_DNN_k_exp}\mbox{}\\*
	Let $x$ denote a point of interest.
	Then
	\begin{equation}
		\begin{aligned}
			\E_D\left[h_s\left(x; D\right)\right]
			& = \E_{1}\left[m(Z_1, \eta_0)
			s\E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]\\
			%
			& \longrightarrow \theta_{0}(x) \quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{lem}
\end{boxD}

\begin{proof}[Proof of Lemma~\ref{lem:CATE_DNN_k_exp}]
	This result follows immediately from Lemma~\ref{lem:dem13} and the following observation.
	\begin{equation}
		\begin{aligned}
			\E_{1}\left[m\left(Z_{1}; \eta_{0}\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]
			 & = \E_{1}\left[\left(\mu_{0}^{1}\left(X_{1}\right) - \mu_{0}^{0}\left(X_{1}\right) + \beta\left(W_{1}, X_{1}\right)\varepsilon_{i}\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]                                \\
			%
			 & = \E_{1}\left[\left(\mu_{0}^{1}\left(X_{1}\right) - \mu_{0}^{0}\left(X_{1}\right) + \beta\left(W_{1}, X_{1}\right)\E\left[\varepsilon_1 \, \middle| \, X_1\right]\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			 & = \E_{1}\left[\left(\mu_{0}^{1}\left(X_{1}\right) - \mu_{0}^{0}\left(X_{1}\right)\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]                                                                                \\
			%
			 & \overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \mu_{0}^{1}\left(x\right) - \mu_{0}^{0}\left(x\right)
             = \theta_{0}(x)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{proof}

\begin{boxD}
    \begin{lem}[CATE - DNN Haj\'ek Kernel Expectation]\label{lem:CATE_chi_s_1}\mbox{}\\*
	Let $z_1 = (x_1, W_{1}, y_1)$ denote a specific realization of $Z$ and $x$ denote a point of interest.
	Then
	\begin{equation}
		\psi_{s}^{1}\left(x; z_1\right)
		= \beta\left(W_{1}, X_1\right)\varepsilon_{1} \cdot \E\left[\kappa\left(x; Z_1, D\right) \; \middle| \; X_1 = x_1\right]
		+ \E_{D}\left[\sum_{i = 2}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{0}^{1}\left(X_{i}\right) - \mu_{0}^{0}\left(X_{i}\right)\right)
		\, \Big| \, Z_1 = z_1 \right]
	\end{equation}
\end{lem}
\end{boxD}

\begin{proof}[Proof of Lemma~\ref{lem:CATE_chi_s_1}]
	\begin{equation}
		\begin{aligned}
			\psi_{s}^{1}\left(x; z_1\right)
			 & = \E_{D}\left[\chi_{s,0}\left(x; D\right) \, | \, Z_1 = z_1 \right] 
			  = \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) m(Z_i, \eta_0)
			 \, \Big| \, Z_1 = z_1 \right]\\
			 %
			 & = \left(\mu_{0}^{1}\left(X_1\right) - \mu_{0}^{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\varepsilon_{1}\right)\E\left[\kappa\left(x; Z_1, D\right) \; \middle| \; X_1 = x_1\right]\\
			 & \quad + \E_{D}\left[\sum_{i = 2}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{0}^{1}\left(X_{i}\right) - \mu_{0}^{0}\left(X_{i}\right)\right)
			 \, \Big| \, Z_1 = z_1 \right]\\
			 %
			 & = \beta\left(W_{1}, X_1\right)\varepsilon_{1} \cdot \E\left[\kappa\left(x; Z_1, D\right) \; \middle| \; X_1 = x_1\right]
			 + \E_{D}\left[\sum_{i = 2}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{0}^{1}\left(X_{i}\right) - \mu_{0}^{0}\left(X_{i}\right)\right)
			 \, \Big| \, Z_1 = z_1 \right]
		\end{aligned}
	\end{equation}
\end{proof}

% \begin{lem}[TDNN Haj\'ek Kernel Expectation]\label{lem:psi_s1s2_1}\mbox{}\\*
% 	Let $z_1 = (x_1, y_1)$ denote a specific realization of $Z$ and $x$ denote a point of interest.
% 	Let $D = \left\{Z_1, \dotsc, Z_{s_2} \right\}$ and $D^{\prime} = \left\{Z_1^{\prime}, \dotsc, Z_{s_1}^{\prime} \right\}$ denote two independent and i.i.d.\ samples drawn from $P$.
% 	Furthermore, let $X \sim P$ and $X \indep D,D^{\prime}$.
% 	Then
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(x; z_1\right)
% 			 & = w_{1}^{*} \left(
% 			\frac{s_2}{s_1}\left(\varepsilon_1 \E_{D^{\prime}}\left[\kappa\left(x; Z_1^{\prime}, D^{\prime}\right)\, \Big| \, X_1^{\prime} = x_1 \right]
% 				+ \E_{D^{\prime}}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}^{\prime}, D^{\prime}\right) \mu(X_{i}^{\prime})\, \Big| \, X_1 ^{\prime}= x_1 \right]\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(X)\right]\right)                                                                                          \\
% 			 & \quad + w_{2}^{*} \left(\varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
% 			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]\right) \\
% 		\end{aligned}
% 	\end{equation}
% \end{lem}

% \begin{proof}[Proof of Lemma~\ref{lem:psi_s1s2_1}]
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(x; z_1\right)
% 			 & = \E_{D}\left[w_{1}^{*} \tilde{\mu}_{s_1}\left(x; D\right)
% 				+ w_{2}^{*} h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]
% 			= \E_{D}\left[h_{s_1, s_2}\left(x; D\right) \, | \, Z_1 = z_1 \right]                                                             \\
% 			%
% 			 & = w_{1}^{*} \E_{D}\left[\tilde{\mu}_{s_1}\left(x; D\right)\, | \, Z_1 = z_1\right]
% 			+ w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                            \\
% 			%
% 			 & = w_{1}^{*} \E_{D}\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}}h_{s_1}\left(x; D_\ell\right)\, | \, Z_1 = z_1\right]
% 			+ w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                            \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(\E_{D}\left[
% 				\sum_{\ell \in L_{s_1 - 1}\left([s_2]\backslash \{1\}\right)}h_{s_1}\left(x; D_{\ell \cup 1}\right)
% 				\sum_{\ell \in L_{s_1}\left([s_2]\backslash \{1\}\right)}h_{s_1}\left(x; D_\ell\right)
% 				\, | \, Z_1 = z_1\right]
% 			\right)                                                                                                                                                      \\
% 			 & \quad + w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                   \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(
% 			\binom{s_2-1}{s_1-1}\E_{1:s_1}\left[h_{s_1}\left(x; D_{[s_1]}\right) \, | \, Z_1 = z_1 \right]
% 			+ \binom{s_2-1}{s_1}\E_{2:(s_1+1)}\left[h_{s_1}\left(x; D_{2:(s_1+1)}\right)\right]
% 			\right)                                                                                                                                                      \\
% 			 & \quad + w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                   \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(
% 			\binom{s_2-1}{s_1-1}\psi_{s_1}^{1}\left(x; \mathbf{z_1}\right)
% 			+ \binom{s_2-1}{s_1}\E_{2:(s_1+1)}\left[h_{s_1}\left(x; D_{2:(s_1+1)}\right)\right]
% 			\right) + w_{2}^{*} \psi_{s_2}^{1}\left(x; \mathbf{z_1}\right)
% 		\end{aligned}
% 	\end{equation}
% 	Using Lemmas~\ref{lem:DNN_k_exp} and \ref{lem:psi_s_1}, we can further simplify this term significantly.
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(x; z_1\right)
% 			 & = w_{1}^{*} \left(\frac{s_2}{s_1}\psi_{s_1}^{1}\left(x; \mathbf{z_1}\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(X)\right]\right)
% 			+ w_{2}^{*} \psi_{s_2}^{1}\left(x; \mathbf{z_1}\right)                                                                                       \\
% 			%
% 			 & = w_{1}^{*} \left(
% 			\frac{s_2}{s_1}\left(\varepsilon_1 \E_{D^{\prime}}\left[\kappa\left(x; Z_1, D^{\prime}\right)\, \Big| \, X_1 = x_1 \right]
% 				+ \E_{D^{\prime}}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D^{\prime}\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(X)\right]\right)                                                                                          \\
% 			 & \quad + w_{2}^{*} \left(\varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
% 			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]\right) \\
% 			%
% 			 & \quad = {\color{red} LOREM IPSUM}
% 		\end{aligned}
% 	\end{equation}
% \end{proof}
