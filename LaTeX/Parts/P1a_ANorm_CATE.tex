\subsection{CATE-Estimator - Asymptotic Normality}
\hrule
Recall the decomposition of the DNN-DML2 estimator introduced in equation \ref{eq:DNNDML2_Decomp}.
\begin{equation}
    \begin{aligned}
        \hat{\theta}\left(x; \mathbf{D}\right)
        & = \underbrace{\underbrace{\E_{D}\left[\hat{\theta}_{0}\left(x; \mathbf{D}\right)\right]}_{\text{Centering-Term}}
        + \underbrace{\frac{s}{n}\sum_{i = 1}^{n} \chi_{s,0}^{(1)}\left(x; Z_{i}\right)}_{\text{Oracle-H\'ajek-Projection}}
        + \underbrace{\sum_{j = 2}^{s} \binom{s}{j} \binom{n}{j}^{-1}\sum_{\ell \in L_{n,j}} \chi_{s,0}^{(j)}\left(x; \mathbf{D}_{\ell}\right)}_{\text{Oracle-H\'ajek-Residual}}}_{\text{Oracle-Hoeffding-Projection}}\\
        & \quad \quad + \underbrace{\binom{n}{s}^{-1}\sum_{\ell \in L_{n,s}}\left(\chi_{s}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) - \chi_{s,0}\left(x; \mathbf{D}_{\ell}\right)\right)}_{\text{First-Stage Approximation Error}}
    \end{aligned}
\end{equation}
We can furthermore decompose the error component in the following way.
\begin{equation}
    \begin{aligned}
        & \binom{n}{s}^{-1} \sum_{\ell \in L_{n,s}}\left(\chi_{s}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) - \chi_{s,0}\left(x; \mathbf{D}_{\ell}\right)\right)
        = \underbrace{\sum_{i = 1}^{n - s + 1} \frac{\binom{n-i}{s-1}}{\binom{n}{s}} \left(m(Z_{(i)}, \hat{\eta}_{k_{(i)}}) - m(Z_{(i)}, \eta_{0})\right)}_{\text{Weighted-NN Error Representation}}
    \end{aligned}
\end{equation}
Additionally, define the following empirical process notation, where $f$ is any $Q$-integrable function on $\mathcal{Z}$.
\begin{equation}
    \begin{aligned}
        \mathbb{G}_{m,k}\left[f(Z)\right]
        & = \sqrt{\frac{1}{m}} \sum_{i \in I_{k}} \Big(f(Z_i) - \E_{Z}\left[f(Z)\right]\Big)
    \end{aligned}
\end{equation}

\begin{boxD}
    \begin{lem}[Behavior of First-Stage Approximation Error]\label{lem:fs_approx_error}\mbox{}\\*
    Concerning the first-stage approximation error, the following holds.
        \begin{equation}
         \binom{n}{s}^{-1}\sum_{\ell \in L_{n,s}}\left(\chi_{s}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) - \chi_{s,0}\left(x; \mathbf{D}_{\ell}\right)\right)
         = O_{p}\left(s \cdot \left(n^{-1/2} r_{n}^{\prime} + \lambda_{n}^{\prime}\right)\right)
    \end{equation}
    Furthermore, if $s = o\left(\min\left\{1/r_{n}^{\prime}, \left(\sqrt{n} \cdot \lambda_{n}^{\prime}\right)^{-1}\right\}\right)$, this implies that 
    \begin{equation}
        \binom{n}{s}^{-1}\sum_{\ell \in L_{n,s}}\left(\chi_{s}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) - \chi_{s,0}\left(x; \mathbf{D}_{\ell}\right)\right)
        \longrightarrow_{p} 0.
    \end{equation}
\end{lem}
\end{boxD}

\begin{proof}[Proof of Lemma \ref{lem:fs_approx_error}]\mbox{}\\*
    Recall first the weighted nearest neighbor representation of the first-stage approximation error.
    \begin{equation}
        \begin{aligned}
            \mathbf{R}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right)
            := \binom{n}{s}^{-1}\sum_{\ell \in L_{n,s}}\left(\chi_{s}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) - \chi_{s,0}\left(x; \mathbf{D}_{\ell}\right)\right) 
            = \sum_{i = 1}^{n - s + 1} \frac{\binom{n-i}{s-1}}{\binom{n}{s}} \left(m(Z_{(i)}, \hat{\eta}_{k_{(i)}}) - m(Z_{(i)}, \eta_{0})\right)
        \end{aligned}
    \end{equation}
    Now, fix an arbitrary fold $k \in [K]$ and consider the corresponding terms of the sum shown above.
    \begin{equation}
        \begin{aligned}
            \left| \mathbf{R}_{k}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) \right|
            & = \left| \sum_{i \in I_{k}} \frac{\binom{n - \rk(x; X_{i}, \mathbf{D})}{s-1}}{\binom{n}{s}} \left(m(Z_{i}, \hat{\eta}_{k}) - m(Z_{i}, \eta_{0})\right) \right|
            \leq \sum_{i \in I_{k}} \frac{\binom{n - \rk(x; X_{i}, \mathbf{D})}{s-1}}{\binom{n}{s}} \left|m(Z_{i}, \hat{\eta}_{k}) - m(Z_{i}, \eta_{0}) \right|
        \end{aligned}
    \end{equation}
    Observe, that given $\mathbf{D}_{I_{k}^{C}}$ this is a sum of identically distributed but dependent random variables where the dependence arises from the interdependence of the weights assigned to each observation.
    We can provide an upper bound to this sum by removing the interdependence and assigning the highest possible weight to each observation.
    \begin{equation}\label{eq:equal_weight_ineq}
        \begin{aligned}
            \left| \mathbf{R}_{k}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) \right| 
            & \leq \sum_{i \in I_{k}} \frac{\binom{n - 1}{s-1}}{\binom{n}{s}} \left|m(Z_{i}, \hat{\eta}_{k}) - m(Z_{i}, \eta_{0}) \right|
            = \frac{s}{n} \sum_{i \in I_{k}} \left|m(Z_{i}, \hat{\eta}_{k}) - m(Z_{i}, \eta_{0}) \right| \\
            %
            & = \frac{s}{k} \left(\frac{1}{m} \sum_{i \in I_{k}} \left|m(Z_{i}, \hat{\eta}_{k}) - m(Z_{i}, \eta_{0}) \right|\right)
        \end{aligned}
    \end{equation}
    Clearly, this is a very lax upper bound as the weights decay at a considerable rate and a more nuanced approach to bounding this term has the potential to improve the conditions obtained as part of this proof.
    Now we can take the ideas from \citet{chernozhukov_doubledebiased_2018} and apply part of the proof strategy of Theorem 3.1 to this sum.
    Thus observe the following.
    \begin{equation}
        \begin{aligned}
            \left| \mathbf{R}_{k}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) \right| 
            & \leq \frac{s}{k} \cdot \frac{\mathcal{I}_{3,k} + \mathcal{I}_{4,k}}{\sqrt{m}}
        \end{aligned}
    \end{equation}
    Here, we use the following definitions.
    \begin{align}
        \mathcal{I}_{3,k} 
        & = \Big|\mathbb{G}_{m,k}\left[m(Z, \hat{\eta}_{k})\right] 
        - \mathbb{G}_{m,k}\left[m(Z, \eta_{0})\right]\Big|\\
        %
        \mathcal{I}_{4,k} 
        & := \sqrt{m} \cdot \left|
        \E_{Z}\left[m(Z, \hat{\eta}_{k}) \, \middle| \, \mathbf{D}_{I_{k}^{C}}\right]
        - \E_{Z}\left[m(Z, \eta_{0}) \right]\right|
    \end{align}
    Continuing along the lines of the original proof, we can then find the following conditional on the event $\mathcal{E}_{n}$.
    \begin{equation}
        \begin{aligned}
            \E_{D}\left[\mathcal{I}_{3,k}^{2} \, \middle| \, \mathbf{D}_{I_{k}^{C}}\right]
            & = \E_{D}\left[ \left(\mathbb{G}_{m,k}\left[m\left(Z, \hat{\eta}_{k}\right)\right] 
            - \mathbb{G}_{m,k}\left[m\left(Z, \eta_{0}\right)\right]\right)^{2} \, \middle| \, \mathbf{D}_{I_{k}^{C}}\right]
            = \E_{D}\left[\left|m\left(Z, \hat{\eta}_{k}\right) - m\left(Z, \eta_{0}\right)\right|^{2}
            \, \middle| \, \mathbf{D}_{I_{k}^{C}}\right]\\
            %
            & \overset{\mathcal{E}_{n}}{\leq} \sup_{\eta \in \mathcal{T}_{n}} 
             \E_{D}\left[\left|m\left(Z, \eta\right) - m\left(Z, \eta_{0}\right)\right|^{2}
            \, \middle| \, \mathbf{D}_{I_{k}^{C}}\right]\\
            %
            & \leq \sup_{\eta \in \mathcal{T}_{n}} 
            \E_{D}\left[\left|m\left(Z, \eta\right) - m\left(Z, \eta_{0}\right)\right|^{2}\right]
            \overset{\text{Asm \ref{asm:DDML_Rate_Cond}}}{=} (r_{n}^{\prime})^{2}
        \end{aligned}
    \end{equation}
    Using Lemma 6.1 from \citet{chernozhukov_doubledebiased_2018}, we can thus find the following.
    \begin{equation}
        \mathcal{I}_{3,k} = O_{p}(r_{n}^{\prime})
    \end{equation}
    Continuing with $\mathcal{I}_{4,k}$, introduce the following function.
    \begin{equation}
        \begin{aligned}
            f_{k}(r) 
            & := \E_{Z}\left[m\left(Z, \eta_{0} + r \left(\hat{\eta}_{k} - \eta_{0}\right)\right) \, \middle| \, \mathbf{D}_{I_{k}^{C}}\right] - \E_{Z}\left[m\left(Z, \eta_{0}\right) \right] 
            \quad \text{for} \quad r \in [0,1]
        \end{aligned}
    \end{equation}
    Then, using a Taylor expansion, we can rewrite as follows.
    \begin{equation}
        f_{k}(r) = f_{k}(0) + f_{k}^{\prime}(0) + \frac{1}{2} f_{k}^{\prime\prime}(\tilde{r})
        \quad \text{for some} \quad \tilde{r} \in (0,1)
    \end{equation}
    Observe now that $f_{k}(0) = 0$, and furthermore by Neyman-orthogonality $f_{k}^{\prime}(0) = 0$.
    Thus, $f_{k}(r) = \frac{1}{2} f_{k}^{\prime\prime}(\tilde{r})$ for some $\tilde{r} \in (0,1)$.
    On the even $\mathcal{E}_{n}$, we can furthermore observe the following.
    \begin{equation}
        \left| f_{k}^{\prime\prime}(\tilde{r}) \right| 
        \leq \sup_{r \in (0,1)} \left| f_{k}^{\prime\prime}(r) \right|
        \overset{\text{Asm \ref{asm:DDML_Rate_Cond}}}{\leq} \lambda_{n}^{\prime}
    \end{equation}
    Combining these findings, we obtain the following result.
    \begin{equation}
         \mathbf{R}_{k}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right)
         = O_{p}\left(s \cdot \left(n^{-1/2} r_{n}^{\prime} + \lambda_{n}^{\prime}\right)\right)
    \end{equation}
    Since $k$ is fixed, we thus obtain the desired result.
    \begin{equation}
        \sqrt{n} \cdot \mathbf{R}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right)
         = O_{p}\left(s \cdot \left(r_{n}^{\prime} + \sqrt{n} \cdot \lambda_{n}^{\prime}\right)\right)
    \end{equation}
    If the subsampling scale $s$ is of order $o\left(\min\left\{1/r_{n}^{\prime}, \left(\sqrt{n} \cdot \lambda_{n}^{\prime}\right)^{-1}\right\}\right)$ this term is thus asymptotically negligible.
\end{proof}

\hrule

\begin{proof}[Proof of Theorem \ref{thm:DNNDML2_anorm}]\mbox{}\\*
    The asymptotic normality follows from the asymptotic normality of the Oracle-Estimator (see Theorem \ref{thm:DNNDML2_anorm_0}) and Lemma \ref{lem:fs_approx_error} by an application of Slutsky's Lemma.
\end{proof}
