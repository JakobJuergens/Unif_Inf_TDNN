\section{Proofs for Results in Section \ref{sec:pw_inf}}
\hrule

\subsection{Closed Form Representations}
\hrule

\begin{proof}[Proof of Theorem \ref{thm:JK_closed_form}]\mbox{}\\*
	Recall the closed form representation of the DNN estimator as presented in Equation \ref{eq:DNN_closed_form}.
	\begin{equation}
		\tilde{\mu}_{s}(x; \mathbf{D}_n)
		= \binom{n}{s}^{-1} \sum_{i = 1}^{n - s + 1}\binom{n - i}{s - 1}Y_{(i)}
	\end{equation}
	Plugging into the Jackknife variance estimator for the DNN estimator now gives us the following where we assume that $n$ is sufficiently large for $n - s + 1$ to be larger than $s$.
	\begin{equation}
		\begin{aligned}
			\hat{\omega}_{\text{JK}}^{2}
			 & = \frac{n - 1}{n}\sum_{i = 1}^{n} \left(\tilde{\mu}_{s}(x; \mathbf{D}_{n, -i}) - \tilde{\mu}_{s}(x; \mathbf{D}_n)\right)^2 \\
			%
			 & = \frac{n - 1}{n}\left\{\sum_{i = 1}^{s} \left(
			\binom{n - 1}{s}^{-1}\left(\sum_{j = 1}^{i - 1} \binom{n - j - 1}{s - 1}Y_{(j)}
			+ \sum_{j = i + 1}^{n - s + 1}\binom{n - j}{s - 1}Y_{(j)}\right)
			- \binom{n}{s}^{-1} \sum_{j = 1}^{n - s + 1} \binom{n - j}{s - 1}Y_{(j)}
			\right)^2 \right.                                                                                                                               \\
			 & \left. \quad \quad +
			\sum_{i = s + 1}^{n} \left(
			\binom{n - 1}{s}^{-1}\sum_{j = 1}^{n - s + 1} \binom{n - j - 1}{s - 1}Y_{(j)}
			- \binom{n}{s}^{-1} \sum_{j = 1}^{n - s + 1} \binom{n - j}{s - 1}Y_{(j)}
			\right)^2\right\}                                                                                                                               \\
			%
			 & =
		\end{aligned}
	\end{equation}
	The closed form of the Jackknife variance estimator for the TDNN estimator follows from the same approach.

		{\color{red} LOREM IPSUM}
\end{proof}

\newpage
\subsection{NPR - Kernel (Conditional) Expectations}\label{subsec:KernelCondExp}
\hrule
As part of deriving consistency results for the variance estimators under consideration, we need to do a careful analysis of the Kernel of the DNN and TDNN estimators.
In this section of the appendix we will thus derive the expectations of the kernel and its corresponding H\'ajek projection.
First, we start with the nonparametric regression setup.
\vspace{0.5cm}
\hrule

\begin{lem}[NPR - DNN Kernel Expectation]\label{lem:DNN_k_exp}\mbox{}\\*
	Let $x$ denote a point of interest.
	Then
	\begin{equation}
		\E_D\left[h_s\left(x; D\right)\right]
		= \E_{1}\left[\mu\left(X_1\right) s\left(1 - \psi\left(B\left(x, \|X_1 - x\|\right)\right)\right)^{s-1}\right]\\
		\longrightarrow \mu(x) \quad \text{as} \quad s \rightarrow \infty
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:DNN_k_exp}]
	This result follows immediately from Lemma \ref{lem:limit_res}.
\end{proof}

\hrule

\begin{lem}[NPR - DNN Haj\'ek Kernel Expectation]\label{lem:psi_s_1}\mbox{}\\*
	Let $z_1 = (x_1, y_1)$ denote a specific realization of $Z$ and $x$ denote a point of interest.
	Then
	\begin{equation}
		\psi_{s}^{1}\left(x; z_1\right)
		= \varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
		+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:psi_s_1}]
	\begin{equation}
		\begin{aligned}
			\psi_{s}^{1}\left(x; z_1\right)
			 & = \E_{D}\left[h_{s}\left(x; D\right) \, | \, Z_1 = z_1 \right]
			= \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) Y_{i} \, \Big| \, Z_1 = z_1 \right]  \\
			%
			 & = \E_{D}\left[\left(\mu(x_1) + \varepsilon_1\right)\kappa\left(x; Z_1, D\right)
			+ \sum_{i = 2}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, Z_1 = z_1 \right] \\
			%
			 & = \varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]
		\end{aligned}
	\end{equation}
\end{proof}
