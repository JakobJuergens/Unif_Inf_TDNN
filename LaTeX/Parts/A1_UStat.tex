\section{The DNN Estimators as a Generalized U-Statistics}
\hrule

As most of the theoretical results in \citet{demirkaya_optimal_2024} rely on representations as a U-statistic, it is helpful to introduce additional concepts and notation at this stage.
In analogy to the main part of this paper, I will first start with the regression context and then go on to consider the CATE estimation scenario separately.
Recalling Equation~\ref{eq:U_stat}, the DNN and TDNN estimators can be expressed in the following U-statistic form and are thus a type of generalized complete U-statistic as introduced by \citet{peng_rates_2022}.
\begin{equation}
	\tilde{\mu}_{s}(x; \mathbf{D}_n)
	= \binom{n}{s}^{-1} \sum_{\ell \in L_{n,s}} h_{s}(x; D_{\ell})
	\quad \text{and} \quad
	\hat{\mu}_{s_1, s_2}(x; \mathbf{D}_n)
	= \binom{n}{s}^{-1} \sum_{\ell \in L_{n,s_2}} h_{s_1, s_2}(x; D_{\ell})
\end{equation}
It is worth pointing out that in contrast to the DNN estimator, the kernel for the TDNN estimator is of order $s_2 > s_1$.
The authors derive an explicit formula for the kernel that shows the connection between the DNN and TDNN estimators.
This connection will prove useful going forward.
\begin{boxD}
	\begin{lem}[Kernel of TDNN Estimator - Adapted from Lemma 8 of \citet{demirkaya_optimal_2024}]\label{lem:dem8}\mbox{}\\*
		The kernel of the TDNN estimator takes the following form.
		\begin{equation}
			\begin{aligned}
				h_{s_1, s_2}\left(x; D\right)
				 & = w_{1}^{*}\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(x; D_{\ell}\right)\right] + w_{2}^{*} h_{s_2}\left(x; D\right) \\
				 & = w_{1}^{*} \tilde{\mu}_{s_1}\left(x; D\right) + w_{2}^{*} h_{s_2}\left(x; D\right)                                                            \\
			\end{aligned}
		\end{equation}
	\end{lem}
\end{boxD}
Borrowing the notational conventions from \citet{lee_u-statistics_2019}, I additionally introduce the following notation.
\begin{equation}\label{eq:psi_s_c}
	\psi_{s}^{c}(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c})
	= \E_{D}\left[h_{s}\left(x; D\right) \, | \,  Z_1 = \mathbf{z}_{1}, \dotsc, Z_c = \mathbf{z}_{c}\right]
\end{equation}
\begin{equation}
	h_{s}^{(1)}\left(x; \mathbf{z}_{1}\right)
	= \psi_{s}^{1}(x; \mathbf{z}_{1}) - \mu(x)
\end{equation}
\begin{equation}
	h_{s}^{(c)}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}\right)
	= \psi_{s}^{c}(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}) - \sum_{j = 1}^{c-1}\left(\sum_{\ell \in L_{n,j}}h_{s}^{(j)}(x; \mathbf{z}_{\ell})\right) - \mu(x)
	\quad \text{for } c = 2, \dotsc, s
\end{equation}
In contrast to the notational inspiration, the subsampling size $s$ is made explicit.
Since we are dealing with an infinite-order U-statistic, $s$ will be diverging with $n$.
Completely analogous, define the corresponding objects for the TDNN estimator.
For the DNN estimator and any $1 \leq c \leq s$, define
\begin{equation}\label{eq:xi_s_c}
	\xi_{s}^{c}\left(x\right)
	= \Var_{1:c}\left(\psi_{s}^{c}(x; Z_{1}, \dotsc, Z_{c})\right)
\end{equation}
where $Z_{c+1}^{\prime}, \ldots, Z_n^{\prime}$ are i.i.d.\ from $P$ and independent of $Z_1, \ldots, Z_n$ and thus
$\xi_{s}^{s}\left(x\right) = \Var\left(h_s\left(x; Z_1, \ldots, Z_s\right)\right)$.
% Then, I have the following result from the original paper.
Similarly, for the TDNN estimator and any $1 \leq c \leq s_2$, let
\begin{equation}\label{eq:zeta_s1s2_c}
	\zeta_{s_1, s_2}^{c}\left(x\right)
	= \Var_{1:c}\left(\psi_{s_1, s_2}^{c}(x; Z_{1}, \dotsc, Z_{c})\right)
\end{equation}
with an analogous definition of $Z^{\prime}$.
As a byproduct (or main purpose depending on the perspective) these terms can be used to derive the Hoeffding decomposition of the TDNN estimator.
\begin{equation}\label{eq:H_projection}
	\begin{aligned}
		H_{s}^{c}\left(x; \mathbf{D}_n\right)
		= \binom{n}{c}^{-1} \sum_{\ell \in L_{n,c}} h^{(c)}_{s}(x; D_{\ell})
		\quad \text{and} \quad
		H_{s_1, s_2}^{c}\left(x; \mathbf{D}_n\right)
		= \binom{n}{c}^{-1} \sum_{\ell \in L_{n,c}} h^{(c)}_{s_1, s_2}(x; D_{\ell})
	\end{aligned}
\end{equation}
These projection terms can then be used to construct the following Hoeffding decompositions.
\begin{equation}\label{eq:H_Decomp}
	\tilde{\mu}_{s}\left(x; \mathbf{D}_n\right)
	= \mu(x) + \sum_{j = 1}^{s}\binom{s}{j}H_{s}^{j}\left(x; \mathbf{D}_n\right)\\
	\quad \text{and} \quad
	\hat{\mu}_{s_1, s_2}\left(x; \mathbf{D}_n\right)
	= \mu(x) + \sum_{j = 1}^{s_2}\binom{s_2}{j}H_{s_1, s_2}^{j}\left(x; \mathbf{D}_n\right)
\end{equation}

Standard results for U-statistics (see, for example, \citet{lee_u-statistics_2019}) now give us a number of useful results.
First, an immediate result on the expectations of the Hoeffding-projection kernels.
\begin{align}\label{eq:H_k_expectation}
	\forall c = 1,2,\dotsc, j-1: \quad & \E_{D}\left[h_{s_1, s_2}^{(j)}\left(x; D\right) \, | \, Z_1 = \mathbf{z}_1, \dotsc, Z_c = \mathbf{z}_c\right] = 0
	\quad \text{and} \quad
	\E_{D}\left[h_{s_1, s_2}^{(j)}\left(x; D\right)\right] = 0
\end{align}
Second, I obtain a useful variance decomposition in terms of the Hoeffding-projection variances.
\begin{align}\label{eq:Var_decomp}
	\Var_{D}\left(\hat{\mu}_{s_1, s_2}\left(x; D\right)\right)
	 & = \sum_{j = 1}^{s_2} \binom{s_2}{j}^2 \Var_{D}\left(H_{s_1, s_2}^{j}\left(x; D\right)\right) \\
	%
	\Var_{D}\left(H_{s_1, s_2}^{j}\left(x; D\right)\right)
	 & = \binom{n}{j}^{-1} \Var_{D}\left(h_{s_1, s_2}^{(j)}\left(x; D\right)\right)
	=: \binom{n}{j}^{-1} V_{s_1, s_2}^{j}\left(x\right)
\end{align}
Third, the following equivalent expression for the kernel variance.
\begin{equation}\label{eq:k_var}
	\zeta_{s_1, s_2}^{s_2}\left(x\right)
	= \Var_{D}\left(h_{s_1, s_2}\left(x; D\right)\right)
	= \sum_{j = 1}^{s_2} \binom{s_2}{j}V_{s_1, s_2}^{j}
\end{equation}

\subsection{CATE-Estimators as Generalized U-Statistics}
\hrule

Given estimates of the functional nuisance parameters, the proposed CATE estimators can be analyzed as generalized U-statistics in the same way as in the nonparametric regression context.
As most of the theoretical results on these estimators will similarly rely on Hoeffding projection arguments, I will introduce analogous notation in this more general scenario.
First, observe that the DNN-DML2 CATE estimator can be rewritten as follows to explicitly show its construction as a generalized U-statistic.
\begin{equation}
    \begin{aligned}
        \widehat{\operatorname{CATE}}\left(x; \hat{\mu}, \hat{\pi}\right) 
        & = \sum_{i = 1}^{n - s + 1} \frac{\binom{n-i}{s-1}}{\binom{n}{s}}
		\left[\hat{\mu}_{k(i),s}^{1}\left(X_{(i)}\right) - \hat{\mu}_{k(i),s}^{0}\left(X_{(i)}\right) + \hat{\beta}_{k(i),s}\left(W_{(i)}, X_{(i)}\right)\left(Y_{(i)} - \hat{\mu}^{W_{(i)}}_{k(i),s}\left(X_{(i)}\right)\right)\right] \\
        %
        & = \binom{n}{s}^{-1} \sum_{\ell \in L_{n,s}}
        \underbrace{\sum_{i = 1}^{n}\frac{\1\left(\rk(x; Z_{i}, D_{\ell}) = 1\right)}{s!} 
        \left[\hat{\mu}_{k_i,s}^{1}\left(X_{i}\right) - \hat{\mu}_{k_i,s}^{0}\left(X_{i}\right) + \hat{\beta}_{k_i,s}\left(W_{i}, X_{i}\right)\left(Y_{i} - \hat{\mu}^{W_{i}}_{k_i,s}\left(X_{i}\right)\right)\right]}_{\chi_{s}(x; \mathbf{D}_{\ell}, \hat{\mu}, \hat{\pi})} \\
        %
        & = \binom{n}{s}^{-1} \sum_{\ell \in L_{n,s}}\chi_{s}(x; \mathbf{D}_{\ell}, \hat{\mu}, \hat{\pi})
    \end{aligned}
\end{equation}
In the same fashion as in the previous case, I can now define the Hoeffding decomposition for the estimator.
Note that in this step, I continue to take the estimates of the functional nuisance parameters as exogenously given.
\begin{equation}
    \vartheta_{s}^{c}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}, \hat{\mu}, \hat{\pi}\right)
    & = \E_{D}\left[\chi_{s}(x; \mathbf{D}, \hat{\mu}, \hat{\pi}) \, \middle| \, Z_1 = \mathbf{z}_1, \dotsc, Z_c = \mathbf{z}_c, \hat{\mu}, \hat{\pi}\right]
\end{equation}
\begin{equation}
    \chi_{s}^{(1)}\left(x; \mathbf{z}_{1}, \hat{\mu}, \hat{\pi}\right)
	= \vartheta_{s}^{1}\left(x; \mathbf{z}_{1}, \hat{\mu}, \hat{\pi}\right)
    - \E_{D}\left[\chi_{s}(x; \mathbf{D}, \hat{\mu}, \hat{\pi}) \, \middle| \, \hat{\mu}, \hat{\pi}\right]
\end{equation}
As before, I define the higher-order projection terms, i.e. for $c = 2, \dotsc, s$, in the following way.
\begin{equation}
    \chi_{s}^{(c)}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}, \hat{\mu}, \hat{\pi}\right)
	& = \vartheta_{s}^{c}(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}, \hat{\mu}, \hat{\pi}) 
    - \sum_{j = 1}^{c-1}\left(\sum_{\ell \in L_{n,j}} \chi_{s}^{(j)}(x; \mathbf{z}_{\ell}, \hat{\mu}, \hat{\pi})\right) 
    - \E_{D}\left[\chi_{s}(x; \mathbf{D}, \hat{\mu}, \hat{\pi}) \, \middle| \, \hat{\mu}, \hat{\pi}\right]
\end{equation}
In anticipation of arguments involving empirical process theory, I define as follows.
\begin{equation}
    \chi_{s, 0}(x; \mathbf{D}_{[s]})
    = \chi_{s}(x; \mathbf{D}_{[s]}, \mu, \pi)
\end{equation}
\begin{equation}
    \vartheta_{s,0}^{c}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}\right)
    & = \vartheta_{s}^{c}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}, \mu, \pi\right)
\end{equation}
\begin{equation}
    \chi_{s,0}^{(1)}\left(x; \mathbf{z}_{1}\right)
	= \chi_{s}^{(1)}\left(x; \mathbf{z}_{1}, \mu, \pi\right)
    = \vartheta_{s,0}^{1}\left(x; \mathbf{z}_{1}\right) 
    - \E_{D}\left[\chi_{s,0}\left(x; \mathbf{D}\right)\right]
\end{equation}
\begin{equation}
    \begin{aligned}
        \chi_{s,0}^{(c)}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}\right)
	    & = \chi_{s}^{(c)}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}, \mu, \pi\right) \\
        %
        & = \vartheta_{s,0}^{c}(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}) 
        - \sum_{j = 1}^{c-1}\left(\sum_{\ell \in L_{n,j}} \chi_{s,0}^{(j)}(x; \mathbf{z}_{\ell})\right) 
        - \E_{D}\left[\chi_{s,0}^{(c)}\left(x; Z_{1}, \dotsc, Z_{c}\right)\right]
    \end{aligned}
\end{equation}
Furthermore, in an overloading of notation, I define as follows.
\begin{equation}
    \vartheta_{s}^{c}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}\right)
    & = \E_{D}\left[\chi_{s}(x; \mathbf{D}, \hat{\mu}, \hat{\pi}) \, \middle| \, Z_1 = \mathbf{z}_1, \dotsc, Z_c = \mathbf{z}_c\right]
\end{equation}
\begin{equation}
    \chi_{s}^{(1)}\left(x; \mathbf{z}_{1}\right)
	= \vartheta_{s}^{1}\left(x; \mathbf{z}_{1}\right)
    - \E_{D}\left[\chi_{s}(x; \mathbf{D}, \hat{\mu}, \hat{\pi})\right]
\end{equation}
\begin{equation}
    \chi_{s}^{(c)}\left(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}\right)
	& = \vartheta_{s}^{c}(x; \mathbf{z}_{1}, \dotsc, \mathbf{z}_{c}) 
    - \sum_{j = 1}^{c-1}\left(\sum_{\ell \in L_{n,j}} \chi_{s}^{(j)}(x; \mathbf{z}_{\ell})\right) 
    - \E_{D}\left[\chi_{s}(x; \mathbf{D}, \hat{\mu}, \hat{\pi})\right]
\end{equation}
To explicitly point out the difference between these definitions, note that we are now not taking the estimates of the functional nuisance parameters as exogenously given.
Instead, we also consider the corresponding expectations with respect to the first-stage estimates.
These definitions allow us to use the following decomposition where now I acknowledge the randomness in the first-stage estimation.
\begin{equation}\label{eq:DNNDML2_ResidDecomp}
    \begin{aligned}
        \chi_{s}^{(c)}\left(x; \mathbf{D}_{\ell}, \hat{\mu}, \hat{\pi}\right)
        & = \underbrace{\chi_{s}^{(c)}\left(x; \mathbf{D}_{\ell}, \hat{\mu}, \hat{\pi}\right) - \chi_{s}^{(c)}\left(x; \mathbf{D}_{\ell}\right)}_{A_{c}\left(x; \mathbf{D}_{\ell}\right)}
        + \underbrace{\chi_{s}^{(c)}\left(x; \mathbf{D}_{\ell}\right) - \chi_{s,0}^{(c)}\left(x; \mathbf{D}_{\ell}\right)}_{B_{c}\left(x; \mathbf{D}_{\ell}\right)} \\
        & + \underbrace{\chi_{s,0}^{(c)}\left(x; \mathbf{D}_{\ell}\right) - \E_{D}\left[\chi_{s,0}^{(c)}\left(x; \mathbf{D}_{\ell}\right)\right]}_{C_{c}\left(x; \mathbf{D}_{\ell}\right)}
        + \underbrace{\E_{D}\left[\chi_{s,0}^{(c)}\left(x; \mathbf{D}_{\ell}\right)\right]}_{D_{c}\left(x\right)}
    \end{aligned}
\end{equation}
Considering this decomposition will be of great usefulness when analyzing the asymptotic properties of the CATE estimators.

Plugging into the full expression for the CATE estimator, the following is obtained.
\begin{equation}\label{eq:DNNDML2_Decomp}
    \begin{aligned}
        \widehat{\operatorname{CATE}}\left(x\right)
        & = \E_{D}\left[\widehat{\operatorname{CATE}}\left(x\right)\right] 
        + \sum_{j = 1}^{s} \binom{s}{j} \binom{n}{j}^{-1}\sum_{\ell \in L_{n,j}} \left[A_{j}\left(x; \mathbf{D}_{\ell}\right) + B_{j}\left(x; \mathbf{D}_{\ell}\right) + C_{j}\left(x; \mathbf{D}_{\ell}\right) + D_{j}\left(x\right)\right]\\
        %
        & = \underbrace{\E_{D}\left[\widehat{\operatorname{CATE}}\left(x\right)\right]}_{\text{Centering-Term}}
        + \underbrace{\frac{s}{n} \sum_{i = 1}^{n} \left[A_{1}\left(x; Z_{i}\right) + B_{1}\left(x; Z_{i}\right) + C_{1}\left(x; Z_{i}\right) + D_{1}\left(x\right)\right]}_{\text{Pseudo-H\'ajek-Projection}}\\
        & \quad \quad + \underbrace{\sum_{j = 2}^{s} \binom{s}{j} \binom{n}{j}^{-1}\sum_{\ell \in L_{n,j}} \left[A_{j}\left(x; \mathbf{D}_{\ell}\right) + B_{j}\left(x; \mathbf{D}_{\ell}\right) + C_{j}\left(x; \mathbf{D}_{\ell}\right) + D_{j}\left(x\right)\right]}_{\text{Pseudo-H\'ajek-Residual}}
    \end{aligned}
\end{equation}
I call these terms pseudo-H\'ajek projection and pseudo-H\'ajek residual because of the additional randomness due to the first-stage estimation of the functional nuisance parameters.
An integral part of showing the asymptotic properties of the estimator is to show that a number of terms in this representation are asymptotically negligible as long as the first-stage estimator converges fast enough.