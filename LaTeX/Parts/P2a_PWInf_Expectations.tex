\section{Proofs for Results in Section \ref{sec:pw_inf}}
\hrule

\subsection{Closed Form Representations}
\hrule

\begin{proof}[Proof of Theorem \ref{thm:JK_closed_form}]\mbox{}\\*
	Recall the closed form representation of the DNN estimator as presented in Equation \ref{eq:DNN_closed_form}.
	\begin{equation}
		\tilde{\mu}_{s}(x; \mathbf{D}_n)
		= \binom{n}{s}^{-1} \sum_{i = 1}^{n - s + 1}\binom{n - i}{s - 1}Y_{(i)}
	\end{equation}
	Plugging into the Jackknife variance estimator for the DNN estimator now gives us the following where we assume that $n$ is sufficiently large for $n - s + 1$ to be larger than $s$.
	\begin{equation}
		\begin{aligned}
			\hat{\omega}_{\text{JK}}^{2}
			 & = \frac{n - 1}{n}\sum_{i = 1}^{n} \left(\tilde{\mu}_{s}(x; \mathbf{D}_{n, -i}) - \tilde{\mu}_{s}(x; \mathbf{D}_n)\right)^2 \\
			%
			 & = \frac{n - 1}{n}\left\{\sum_{i = 1}^{s} \left(
			\binom{n - 1}{s}^{-1}\left(\sum_{j = 1}^{i - 1} \binom{n - j - 1}{s - 1}Y_{(j)}
			+ \sum_{j = i + 1}^{n - s + 1}\binom{n - j}{s - 1}Y_{(j)}\right)
			- \binom{n}{s}^{-1} \sum_{j = 1}^{n - s + 1} \binom{n - j}{s - 1}Y_{(j)}
			\right)^2 \right.                                                                                                                               \\
			 & \left. \quad \quad +
			\sum_{i = s + 1}^{n} \left(
			\binom{n - 1}{s}^{-1}\sum_{j = 1}^{n - s + 1} \binom{n - j - 1}{s - 1}Y_{(j)}
			- \binom{n}{s}^{-1} \sum_{j = 1}^{n - s + 1} \binom{n - j}{s - 1}Y_{(j)}
			\right)^2\right\}                                                                                                                               \\
			%
			 & =
		\end{aligned}
	\end{equation}
	The closed form of the Jackknife variance estimator for the TDNN estimator follows from the same approach.

		{\color{red} LOREM IPSUM}
\end{proof}

\newpage
\subsection{NPR - Kernel (Conditional) Expectations}\label{subsec:KernelCondExp}
\hrule
As part of deriving consistency results for the variance estimators under consideration, we need to do a careful analysis of the Kernel of the DNN and TDNN estimators.
In this section of the appendix we will thus derive the expectations of the kernel and its corresponding H\'ajek projection.
First, we start with the nonparametric regression setup.
\vspace{0.5cm}
\hrule

\begin{lem}[NPR - DNN Kernel Expectation]\label{lem:DNN_k_exp}\mbox{}\\*
	Let $x$ denote a point of interest.
	Then
	\begin{equation}
		\E_D\left[h_s\left(x; D\right)\right]
		= \E_{1}\left[\mu\left(X_1\right) s\left(1 - \psi\left(B\left(x, \|X_1 - x\|\right)\right)\right)^{s-1}\right]\\
		\longrightarrow \mu(x) \quad \text{as} \quad s \rightarrow \infty
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:DNN_k_exp}]
	\begin{equation}
		\begin{aligned}
			\E_D\left[h_s\left(x; D\right)\right]
			 & = \E_D\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) Y_{i}\right]
			= s \E_{1}\left[Y_1 \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\, | \, X_1 \right]\right]                                                                \\
			%
			 & = \E_{1}\left[\left(\mu\left(X_1\right) + \varepsilon_1\right) s\left(1 - \psi\left(B\left(x, \|X_1 - x\|\right)\right)\right)^{s-1}\right] \\
			%
			 & = \E_{1}\left[\mu\left(X_1\right) s\left(1 - \psi\left(B\left(x, \|X_1 - x\|\right)\right)\right)^{s-1}\right]                              \\
			%
			 & \longrightarrow \mu(x) \quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

\begin{lem}[NPR - DNN Haj\'ek Kernel Expectation]\label{lem:psi_s_1}\mbox{}\\*
	Let $z_1 = (x_1, y_1)$ denote a specific realization of $Z$ and $x$ denote a point of interest.
	Then
	\begin{equation}
		\psi_{s}^{1}\left(x; z_1\right)
		= \varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
		+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:psi_s_1}]
	\begin{equation}
		\begin{aligned}
			\psi_{s}^{1}\left(x; z_1\right)
			 & = \E_{D}\left[h_{s}\left(x; D\right) \, | \, Z_1 = z_1 \right]
			= \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) Y_{i} \, \Big| \, Z_1 = z_1 \right]  \\
			%
			 & = \E_{D}\left[\left(\mu(x_1) + \varepsilon_1\right)\kappa\left(x; Z_1, D\right)
			+ \sum_{i = 2}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, Z_1 = z_1 \right] \\
			%
			 & = \varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]
		\end{aligned}
	\end{equation}
\end{proof}

\newpage
\subsection{CATE - Kernel (Conditional) Expectations}
\hrule
Next, we address the CATE estimation setup, where we first consider the scenario where the nuisance parameters are assumed to be known a priori.
In a second step, we will show that asymptotically, the estimation of nuisance parameters as described in Definition \ref{def:CATE_DNN_DML}, does not alter the asymptotic analysis of the estimator.
For clarity, we point out that in contexts relating to the estimation of the conditional average treatment effect, the kernel or score function $h_s$ could hypothetically signify the first or second stage kernel.
As the first stage is effectively covered by the nonparametric regression setup, we will take $h_s$ in these contexts to mean the kernel weighted Neyman-orthogonal score associated with the CATE.
\vspace{0.5cm}
\hrule

\begin{lem}[CATE - DNN Kernel Expectation]\label{lem:CATE_DNN_k_exp}\mbox{}\\*
	Let $x$ denote a point of interest.
	Then
	\begin{equation}
		\begin{aligned}
			\E_D\left[h_s\left(x; D\right)\right]
			& = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\left(Y_{1} - \mu_{W_{1}}\left(X_1\right)\right)\right)
			s\left(1 - \psi\left(B\left(x, \|X_1 - x\|\right)\right)\right)^{s-1}\right]\\
			%
			& \longrightarrow \operatorname{CATE}(x) \quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:CATE_DNN_k_exp}]
	\begin{equation}
		\begin{aligned}
			\E_D\left[h_s\left(x; D\right)\right]
			& = \E_D\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right) + \beta\left(W_{i}, X_{i}\right)\left(Y_{i} - \mu_{W_{i}}\left(X_{i}\right)\right)\right)\right]\\
			%
			& = s \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\left(Y_{1} - \mu_{W_{1}}\left(X_1\right)\right)\right)
			\cdot \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\, | \, X_1 \right]\right]\\
			%
			& = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\varepsilon_1\right)
			\cdot s\left(1 - \psi\left(B\left(x, \|X_1 - x\|\right)\right)\right)^{s-1}\right]\\
			%
			& \longrightarrow \mu_{1}\left(x\right) - \mu_{0}\left(x\right)
			= \operatorname{CATE}\left(x\right) 
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
\end{proof}

\hrule

\begin{lem}[CATE - DNN Haj\'ek Kernel Expectation]\label{lem:CATE_chi_s_1}\mbox{}\\*
	Let $z_1 = (x_1, W_{1}, y_1)$ denote a specific realization of $Z$ and $x$ denote a point of interest.
	Then
	\begin{equation}
		\psi_{s}^{1}\left(x; z_1\right)
		= \beta\left(W_{1}, X_1\right)\varepsilon_{1} \cdot \E\left[\kappa\left(x; Z_1, D\right) \; \middle| \; X_1 = x_1\right]
		+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right)\right)
		\, \Big| \, Z_1 = z_1 \right]
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:CATE_chi_s_1}]
	\begin{equation}
		\begin{aligned}
			\psi_{s}^{1}\left(x; z_1\right)
			 & = \E_{D}\left[h_{s}\left(x; D\right) \, | \, Z_1 = z_1 \right] \\
			 %
			 & = \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right) + \beta\left(W_{i}, X_{i}\right)\left(Y_{i} - \mu_{W_{i}}\left(X_{i}\right)\right)\right)
			 \, \Big| \, Z_1 = z_1 \right]\\
			 %
			 & = \left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\varepsilon_{1}\right)\E\left[\kappa\left(x; Z_1, D\right) \; \middle| \; X_1 = x_1\right]\\
			 & \quad + \E_{D}\left[\sum_{i = 2}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right)\right)
			 \, \Big| \, Z_1 = z_1 \right]\\
			 %
			 & = \beta\left(W_{1}, X_1\right)\varepsilon_{1} \cdot \E\left[\kappa\left(x; Z_1, D\right) \; \middle| \; X_1 = x_1\right]
			 + \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right)\right)
			 \, \Big| \, Z_1 = z_1 \right]
		\end{aligned}
	\end{equation}
\end{proof}


% \begin{lem}[TDNN Haj\'ek Kernel Expectation]\label{lem:psi_s1s2_1}\mbox{}\\*
% 	Let $z_1 = (x_1, y_1)$ denote a specific realization of $Z$ and $x$ denote a point of interest.
% 	Let $D = \left\{Z_1, \dotsc, Z_{s_2} \right\}$ and $D^{\prime} = \left\{Z_1^{\prime}, \dotsc, Z_{s_1}^{\prime} \right\}$ denote two independent and i.i.d. samples drawn from $P$.
% 	Furthermore, let $X \sim P$ and $X \indep D,D^{\prime}$.
% 	Then
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(x; z_1\right)
% 			 & = w_{1}^{*} \left(
% 			\frac{s_2}{s_1}\left(\varepsilon_1 \E_{D^{\prime}}\left[\kappa\left(x; Z_1^{\prime}, D^{\prime}\right)\, \Big| \, X_1^{\prime} = x_1 \right]
% 				+ \E_{D^{\prime}}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}^{\prime}, D^{\prime}\right) \mu(X_{i}^{\prime})\, \Big| \, X_1 ^{\prime}= x_1 \right]\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(X)\right]\right)                                                                                          \\
% 			 & \quad + w_{2}^{*} \left(\varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
% 			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]\right) \\
% 		\end{aligned}
% 	\end{equation}
% \end{lem}

% \begin{proof}[Proof of Lemma \ref{lem:psi_s1s2_1}]
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(x; z_1\right)
% 			 & = \E_{D}\left[w_{1}^{*} \tilde{\mu}_{s_1}\left(x; D\right)
% 				+ w_{2}^{*} h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]
% 			= \E_{D}\left[h_{s_1, s_2}\left(x; D\right) \, | \, Z_1 = z_1 \right]                                                             \\
% 			%
% 			 & = w_{1}^{*} \E_{D}\left[\tilde{\mu}_{s_1}\left(x; D\right)\, | \, Z_1 = z_1\right]
% 			+ w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                            \\
% 			%
% 			 & = w_{1}^{*} \E_{D}\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}}h_{s_1}\left(x; D_\ell\right)\, | \, Z_1 = z_1\right]
% 			+ w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                            \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(\E_{D}\left[
% 				\sum_{\ell \in L_{s_1 - 1}\left([s_2]\backslash \{1\}\right)}h_{s_1}\left(x; D_{\ell \cup 1}\right)
% 				\sum_{\ell \in L_{s_1}\left([s_2]\backslash \{1\}\right)}h_{s_1}\left(x; D_\ell\right)
% 				\, | \, Z_1 = z_1\right]
% 			\right)                                                                                                                                                      \\
% 			 & \quad + w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                   \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(
% 			\binom{s_2-1}{s_1-1}\E_{1:s_1}\left[h_{s_1}\left(x; D_{[s_1]}\right) \, | \, Z_1 = z_1 \right]
% 			+ \binom{s_2-1}{s_1}\E_{2:(s_1+1)}\left[h_{s_1}\left(x; D_{2:(s_1+1)}\right)\right]
% 			\right)                                                                                                                                                      \\
% 			 & \quad + w_{2}^{*} \E_D\left[h_{s_2}\left(x; D\right)\, | \, Z_1 = z_1\right]                                                   \\
% 			%
% 			 & = w_{1}^{*} \binom{s_2}{s_1}^{-1}\left(
% 			\binom{s_2-1}{s_1-1}\psi_{s_1}^{1}\left(x; \mathbf{z_1}\right)
% 			+ \binom{s_2-1}{s_1}\E_{2:(s_1+1)}\left[h_{s_1}\left(x; D_{2:(s_1+1)}\right)\right]
% 			\right) + w_{2}^{*} \psi_{s_2}^{1}\left(x; \mathbf{z_1}\right)
% 		\end{aligned}
% 	\end{equation}
% 	Using Lemmas \ref{lem:DNN_k_exp} and \ref{lem:psi_s_1}, we can further simplify this term significantly.
% 	\begin{equation}
% 		\begin{aligned}
% 			\psi_{s_1, s_2}^{1}\left(x; z_1\right)
% 			 & = w_{1}^{*} \left(\frac{s_2}{s_1}\psi_{s_1}^{1}\left(x; \mathbf{z_1}\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(X)\right]\right)
% 			+ w_{2}^{*} \psi_{s_2}^{1}\left(x; \mathbf{z_1}\right)                                                                                       \\
% 			%
% 			 & = w_{1}^{*} \left(
% 			\frac{s_2}{s_1}\left(\varepsilon_1 \E_{D^{\prime}}\left[\kappa\left(x; Z_1, D^{\prime}\right)\, \Big| \, X_1 = x_1 \right]
% 				+ \E_{D^{\prime}}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D^{\prime}\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]\right)
% 			+ \frac{s_2}{s_2 - s_1}\E\left[\mu(X)\right]\right)                                                                                          \\
% 			 & \quad + w_{2}^{*} \left(\varepsilon_1 \E_D\left[\kappa\left(x; Z_1, D\right)\, \Big| \, X_1 = x_1 \right]
% 			+ \E_{D}\left[\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \mu(X_{i})\, \Big| \, X_1 = x_1 \right]\right) \\
% 			%
% 			 & \quad = {\color{red} LOREM IPSUM}
% 		\end{aligned}
% 	\end{equation}
% \end{proof}

\newpage
