\subsection{NPR - Kernel Variances \& Covariances}
\hrule
Similar to the previous section of proofs, we will continue by analyzing the variances and covariances of the kernels under consideration.
These results will play an important role in the derivation of consistency properties for the variance estimators.
Similar to the previous part, we will first consider the nonparametric regression setup and then proceed to the conditional average treatment effect setup.
\vspace{0.5cm}
\hrule
\begin{lem}[Adapted from \citet{demirkaya_optimal_2024}]\label{lem:omega_s}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s}\}$ be a vector of i.i.d.\ random variables drawn from $P$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}\left(x\right)
		= \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}\left(x\right)
		= \E_{1}\left[\left(\mu\left(X_1\right)+ \varepsilon_1\right)^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]
		\lesssim \mu^2(x) + \overline{\sigma}_{\varepsilon}^2 + o(1)
		\quad \text{as} \quad s \rightarrow \infty.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma~\ref{lem:omega_s}]\mbox{}\\*
    This result follows immediately from Lemma~\ref{lem:dem13} and the following observation.
	\begin{equation}
		\begin{aligned}
			\Omega_{s}\left(x\right)
			 & = \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right]
			= \E_{D}\left[\left(\sum_{i = 1}^{s}\kappa\left(x; Z_{i}, D\right)Y_{i}\right)^2\right]
			= \E_{D}\left[\sum_{i = 1}^{s}\sum_{j = 1}^{s}\left(\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D\right)Y_{i}Y_{j}\right)\right] \\
			%
			 & = \E_{D}\left[s \kappa\left(x; Z_{1}, D\right)Y_{1}^2\right]
			= \E_{1}\left[Y_{1}^2 s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]       
            = \E_{1}\left[\left(\mu\left(X_1\right) + \varepsilon_1\right)^2 s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right] \\
			%
			 & = \E_{1}\left[\left(\mu^2\left(X_1\right) + 2\mu\left(X_1\right)\varepsilon_1 + \varepsilon_1^2\right)s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]                       \\
			%
			 & = \E_{1}\left[\left(\mu^2\left(X_1\right) + 2\mu\left(X_1\right) \E\left[\varepsilon_1 \, \middle| \, X_1\right] + \E\left[\varepsilon_1^2 \, \middle| \, X_1\right]\right)
			s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]                                                                                                                                \\
			%
			 & = \E_{1}\left[\left(\mu^2\left(X_1\right) +\sigma_{\varepsilon}^{2}(X_1)\right) s \E_{2:s}\left[\kappa(x; Z_1, D)\right]\right]              \overset{\text{(Lem~\ref{lem:dem13})}}{\longrightarrow} \mu^2\left(x\right) +\sigma_{\varepsilon}^{2}(x)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}
    Furthermore, we have the following inequality.
    \begin{equation}
        \mu^2(x) + \sigma_{\varepsilon}^2(x) \leq \mu^2\left(x\right) + \overline{\sigma}_{\varepsilon}^{2}
    \end{equation}
	Thus, we obtain the desired result.
\end{proof}

\hrule

\begin{lem}\label{lem:omega_sc}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s}\}$ be a vector of i.i.d.\ random variables drawn from $P$.
	Let $D^{\prime} = \{Z_1, \dotsc, Z_{c}, Z_{c+1}^{\prime}, \dotsc,  Z_{s}^{\prime}\}$ where $Z_{c+1}^{\prime}, \dotsc,  Z_{s}^{\prime}$ are i.i.d.\ draws from $P$ that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		= \E\left[h_{s}\left(x; Z_1, \ldots, Z_{c}, Z_{c+1}, \ldots, Z_{s}\right) \cdot
			h_{s}\left(x; Z_1, \ldots,Z_{c}, Z_{c+1}^{\prime}, \ldots, Z_{s}^{\prime}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		\lesssim \frac{s^2 + cs  - c^2}{s^2} \mu^2(x) + (c/s) \overline{\sigma}_{\varepsilon}^2 + o(1)
		\quad \text{for} \quad s \quad \text{sufficiently large}
	\end{equation}
	and thus
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		\lesssim \mu^2(x) + \overline{\sigma}_{\varepsilon}^2 + o(1)
		\quad \text{as} \quad s \rightarrow \infty.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma~\ref{lem:omega_sc}]
	\begin{equation}
		\begin{aligned}
			\Omega_{s}^{c}\left(x\right)
			 & = \E\left[h_{s}\left(x; Z_1, \ldots, Z_{c}, Z_{c+1}, \ldots, Z_{s}\right) \cdot
			h_{s}\left(x; Z_1, \ldots,Z_{c}, Z_{c+1}^{\prime}, \ldots, Z_{s}^{\prime}\right)\right]                                                                \\
			%
			 & = \E_{D, D^{\prime}}\left[
				\left(\sum_{i = 1}^{s}\kappa\left(x; Z_{i}, D\right)Y_{i}\right)
				\left(\sum_{j = 1}^{c}\kappa\left(x; Z_{j}, D^{\prime}\right)Y_{j}
				+ \sum_{j = c+1}^{s}\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right)Y_{j}^{\prime}\right)
			\right]                                                                                                                                                                                             \\
			%
			 & = \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D^{\prime}\right)Y_{i}Y_{j}\right]
			+  \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = c+1}^{s}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right)Y_{i}Y_{j}^{\prime}\right]   \\
			 & \quad + \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s}\sum_{j = 1}^{c}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D^{\prime}\right)Y_{i}Y_{j}\right]
			+  \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s}\sum_{j = c+1}^{s}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right)Y_{i}Y_{j}^{\prime}\right] \\
			%
			 & = \underbrace{\E_{D, D^{\prime}}\left[c \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{1}^{2}\right]}_{(A)}
			+ \underbrace{\E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{1}Y_{c+1}^{\prime}\right]}_{(B)}                           \\
			 & \quad + \underbrace{\E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{c+1}Y_{1}\right]}_{(C)}                                    \\
			 & \quad + \underbrace{\E_{D, D^{\prime}}\left[(s-c)^2 \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{c+1}Y_{c+1}^{\prime}\right]}_{(D)}
		\end{aligned}
	\end{equation}
	Starting from this decomposition, we will analyze the terms one by one.
	First, by Lemma~\ref{lem:limit_res}, we find the following.
	\begin{equation}
		\begin{aligned}
            (A) & = 
			\E_{D, D^{\prime}}\left[c\kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{1}^{2}\right]
			= (c/s) \E_{1}\left[Y_1^2 s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)\right]\right] \\
			%
			 & \leq (c/s) \E_{1}\left[Y_1^2 s\E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]                  
			\overset{\text{(Lem~\ref{lem:limit_res})}}{\lesssim} (c/s)\left(\mu^2(x) + \sigma_{\varepsilon}(x)\right) + o(1)\\
		\end{aligned}
	\end{equation}
	Similarly, we can find that:
	\begin{equation}
		\begin{aligned}
            (B)
			 & = \E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{1}Y_{c+1}^{\prime}\right]                                          
			\overset{\text{Lem \ref{lem:npr_kern_ineq1}}}{\lesssim} \frac{c(s-c)}{s^2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	Following analogous steps, we find the same result for the third term.
	\begin{equation}
		\begin{aligned}
			(C)
            = \E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{c+1}Y_{1}\right]
			\overset{\text{Lem \ref{lem:npr_kern_ineq1}}}{\lesssim} \frac{c(s-c)}{s^2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	The fourth term can be asymptotically bounded in the following way.
	\begin{equation}
		\begin{aligned}
			(D)
            & = \E_{D, D^{\prime}}\left[(s-c)^2 \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{c+1}Y_{c+1}^{\prime}\right]                                           \\
			%
			 & = \E_{D, D^{\prime}}\left[\mu(X_{c+1})\mu(X_{c+1}^{\prime})\, (s-c)^2 \, \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right] \\
			%
			 & \leq \E_{D}\left[|\mu(X_{c+1})|\, (s-c) \, \kappa\left(x; Z_{c+1}, D\right)\right]
			\E_{D^{\prime}}\left[|\mu(X_{c+1}^{\prime})|\, (s-c) \, \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                      \\
			%
			 & = \frac{(s-c)^2}{s^2} \E_{D}\left[|\mu(X_{c+1})|\, s \, \kappa\left(x; Z_{c+1}, D\right)\right]
			\E_{D^{\prime}}\left[|\mu(X_{c+1}^{\prime})|\, s \, \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                          \\
			%
			 & \quad = \frac{(s-c)^2}{s^2} \left(\E_{D}\left[|\mu(X_{c+1})|\, s \, \kappa\left(x; Z_{c+1}, D\right)\right]\right)^2                                                                        
			\lesssim \frac{(s-c)^2}{s^2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	The result of Lemma~\ref{lem:omega_sc} follows immediately by summing up the asymptotic bounds for the individual terms.
\end{proof}

\newpage
\begin{lem}\label{lem:upsilon_s}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s_2}\}$ be a vector of i.i.d.\ random variables drawn from $P$ for $s_2 > s_1$.
	Furthermore, let
	\begin{equation}
		\Upsilon_{s_1, s_2}\left(x\right)
		= \E\left[h_{s_1}\left(x; Z_1, \ldots,  Z_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots,Z_{s_1}, \ldots, Z_{s_2}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Upsilon_{s_1, s_2}\left(x\right)
		\lesssim \mu^{2}\left(x\right) + \overline{\sigma}^2_{\varepsilon} + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma~\ref{lem:upsilon_s}]
	\begin{equation}
		\begin{aligned}
			\Upsilon_{s_1, s_2}\left(x\right)
			 & = \E\left[h_{s_1}\left(x; Z_1, \ldots,  Z_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots,Z_{s_1}, \ldots, Z_{s_2}\right)\right]                                                                             \\
			%
			 & = \E_{D}\left[
				\left(\sum_{i = 1}^{s_1} \kappa(x; Z_{i}, D_{[s_1]})Y_{i}\right)
				\left(\sum_{j = 1}^{s_1}\kappa(x; Z_{j}, D)Y_j + \sum_{j = s_1 + 1}^{s_2}\kappa(x; Z_{j}, D)Y_j\right)
			\right]                                                                                                                                                                            \\
			%
			 & = \E_{D}\left[\sum_{i = 1}^{s_1} \kappa(x; Z_{i}, D) Y_{i}^2\right]
			+ \E_{D}\left[\sum_{i = 1}^{s_1}\sum_{j = s_1 + 1}^{s_2}\kappa(x; Z_{i}, D_{[s_1]})\kappa(x; Z_{j}, D) Y_{i} Y_j\right]                              \\
			%
			 & = \E_{D}\left[Y_1^2 \, s_1 \, \kappa(x; Z_1, D)\right]
			+ \E_{D}\left[Y_{1} Y_{s_2} \, s_1 (s_2 - s_1) \, \kappa(x; Z_1, D_{[s_1]})\kappa(x; Z_{s_2}, D)\right]                                        \\
			%
			 & = \E_{D}\left[\left(\mu^2(X_1) + \sigma^2_{\varepsilon}(X_1)\right) \, s_1 \, \kappa(x; Z_1, D)\right]
			+ \E_{D}\left[\mu(X_1) \mu(X_{s_2}) \, s_1 (s_2 - s_1) \, \kappa(x; Z_1, D_{[s_1]})\kappa(x; Z_{s_2}, D)\right]              \\
			%
			 & = \frac{s_1}{s_2}\E_{D}\left[\left(\mu^2(X_1) + \sigma^2_{\varepsilon}(X_1)\right) \, s_1 \, \kappa(x; Z_1, D)\right]
			+ \frac{s_2 - s_1}{s_2}\E_{D}\left[\mu(X_1) \mu(X_{s_2}) \, s_1 s_2 \, \kappa(x; Z_1, D_{[s_1]})\kappa(x; Z_{s_2}, D)\right] \\
			%
			 & \leq \frac{s_1}{s_2} \E_{D}\left[\left(\mu^2(X_1) + \sigma^2_{\varepsilon}(X_1)\right) \, s_2 \, \kappa(x; Z_1, D)\right]                   \\
			 & \quad \quad + \frac{s_2 - s_1}{s_2}\E_{D}\left[|\mu(X_1)| \, s_1 \, \kappa(x; Z_1, D_{[s_1]})\right]
			\E_{D}\left[|\mu(X_{s_2})| \, s_2 \, \kappa(x; Z_{s_2}, D)\right]                                                                                       \\
			%
			 & \lesssim \mu^{2}\left(x\right) + \sigma^2_{\varepsilon}(x) + o(1)
			\leq \mu^{2}\left(x\right) + \overline{\sigma}^2_{\varepsilon} + o(1).
		\end{aligned}
	\end{equation}
\end{proof}

\newpage
\begin{lem}\label{lem:upsilon_sc}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s_2}\}$ be a vector of i.i.d.\ random variables drawn from $P$ for $s_2 > s_1$.
	Let $D^{\prime} = \{Z_1, \dotsc, Z_{c}, Z_{c+1}^{\prime}, \dotsc,  Z_{s_1}^{\prime}\}$ where $Z_{c+1}^{\prime}, \dotsc,  Z_{s_1}^{\prime}$ are i.i.d.\ draws from $P$ that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Upsilon_{s_1, s_2}^{c}\left(x\right)
		= \E\left[h_{s_1}\left(x; Z_1, \ldots, Z_c, Z^{\prime}_{c+1}, \ldots,  Z^{\prime}_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots, Z_{s_2}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\begin{aligned}
			 & \Upsilon_{s_1, s_2}^{c}\left(x\right)
			\lesssim \frac{c s_2 - c^2 + s_1 s_2}{s_1 s_2}\mu^2(x) + (c/s_1) \overline{\sigma}^2_{\varepsilon} + o(1) \\
			%
			 & \text{for} \quad s_1, s_2 \quad \text{sufficiently large}
			\quad \text{with} \quad
			0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1
		\end{aligned}
	\end{equation}
	and thus
	\begin{equation}
		\Upsilon_{s_1, s_2}^{c}\left(x\right)
		\lesssim \mu^2(x) + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma~\ref{lem:upsilon_sc}]
	\begin{equation}
		\begin{aligned}
			\Upsilon_{s_1, s_2}^{c}\left(x\right)
			 & = \E\left[h_{s_1}\left(x; Z_1, \ldots, Z_c, Z^{\prime}_{c+1}, \ldots,  Z^{\prime}_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots, Z_{s_2}\right)\right]                                                                                                                       \\
			%                                                                     
			 & = \E_{D, D^{\prime}}\left[
				\left(\sum_{i = 1}^{c} \kappa(x; Z_{i}, D^{\prime})Y_{i} + \sum_{i = c+1}^{s_1} \kappa(x; Z_{i}^{\prime}, D^{\prime})Y_{i}^{\prime}\right)
				\left(\sum_{j = 1}^{c} \kappa(x; Z_{j}, D)Y_j + \sum_{j = c+1}^{s_2}\kappa(x; Z_{j}, D)Y_j \right)
			\right]                                                                                                                                                                                             \\
			%
			 & = \underbrace{\E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c} \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_{j}, D)Y_{i} Y_j\right]}_{(A)}
			+ \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c}\kappa(x; Z_{i}, D^{\prime}) Y_{i}\right) \left(\sum_{j = c+1}^{s_2} \kappa(x; Z_{j}, D) Y_j\right)\right]}_{(B)} \\
			 & \quad + \underbrace{\E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s_1}\sum_{j = 1}^{c} \kappa(x; Z_{i}^{\prime}, D^{\prime})\kappa(x; Z_{j}, D)Y_{i}^{\prime} Y_j\right]}_{(C)}
			+ \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = c+1}^{s_1}\kappa(x; Z_{i}^{\prime}, D^{\prime}) Y_{i}^{\prime}\right)
					\left(\sum_{j = c+1}^{s_2} \kappa(x; Z_{j}, D) Y_j\right)\right]}_{(D)}
		\end{aligned}
	\end{equation}
	Again, we have four terms to analyze individually.
	\begin{equation}
		\begin{aligned}
			(A)
			 & = \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c} \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_{j}, D)Y_{i} Y_j\right]\\
			 %
			& = \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c} Y_{i}^2 \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_{i}, D)\right]                                                                       \\
			%
			 & = \E_{D, D^{\prime}}\left[Y_{1}^2 \, c \, \kappa(x; Z_1, D^{\prime})\kappa(x; Z_1, D)\right]
			= \E_{D, D^{\prime}}\left[\left(\mu^2(X_{1}) + \sigma^2_{\varepsilon}(X_{1})\right) \, c \, \kappa(x; Z_1, D_{[c]})\kappa(x; Z_1, D^{\prime}_{c+1:s_1})\right] \\
			%
			 & = \E_{D}\left[\left(\mu^2(X_{1}) + \sigma^2_{\varepsilon}(X_{1})\right) \, c \, \kappa(x; Z_1, D)\right]
			= \frac{c}{s_1} \E_{D}\left[\left(\mu^2(X_{1}) + \sigma^2_{\varepsilon}(X_{1})\right) \, s_1 \, \kappa(x; Z_1, D)\right]                                               \\
			%
			 & \lesssim (c/s_1)(\mu^2(x) + \sigma^2_{\varepsilon}(x)) + o(1)
			\leq (c/s_1)(\mu^2(x) + \overline{\sigma}^2_{\varepsilon}) + o(1)
		\end{aligned}
	\end{equation}
	Considering the second term, we find the following.
	\begin{equation}
		\begin{aligned}
			(B)
			 & = \E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c}\kappa(x; Z_{i}, D^{\prime}) Y_{i}\right)\left(\sum_{j = c+1}^{s_2} \kappa(x; Z_{j}, D) Y_j\right)\right]
			= \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = c+1}^{s_2}Y_{i} Y_j \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_{j}, D)\right]                               \\
			%
			 & = \E_{D, D^{\prime}}\left[c(s_2 - c) \, Y_1 Y_{s_1} \kappa(x; Z_1, D^{\prime})\kappa(x; Z_{s_2}, D)\right]
			= \frac{c (s_2 - c)}{s_1 s_2}\E_{D, D^{\prime}}\left[Y_1 Y_{s_2} \, s_1 s_2 \,\kappa(x; Z_1, D^{\prime})\kappa(x; Z_{s_2}, D)\right]                   \\
			%
			 & \leq \frac{c (s_2 - c)}{s_1 s_2}
			\E_{D^{\prime}}\left[|\mu(X_1)| \, s_1  \,\kappa(x; Z_1, D^{\prime})\right]
			\E_{D}\left[|\mu(X_{s_2})| \, s_2  \,\kappa(x; Z_{s_2}, D)\right]                                                                                       \\
			%
			 & \lesssim \frac{c(s_2 - c)}{s_1 s_2} \mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	Similarly, by simplifying the third term, we find the following.
	\begin{equation}
		\begin{aligned}
			(C)
			 & = \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s_1}\sum_{j = 1}^{c} \kappa(x; Z_{i}^{\prime}, D^{\prime})\kappa(x; Z_{j}, D)Y_{i}^{\prime} Y_j\right]
			= \E_{D, D^{\prime}}\left[Y_{s_1}^{\prime} Y_{1} \, (s_1 - c)c \, \kappa(x; Z_{s_1}^{\prime}, D^{\prime})\kappa(x; Z_1, D)\right]                                                    \\
			% 
			 & = \frac{(s_1 - c)c}{s_1 s_2}\E_{D, D^{\prime}}\left[\mu(X_{s_1}^{\prime})\mu(X_1) \, s_1 s_2 \, \kappa(x; Z_{s_1}^{\prime}, D^{\prime})\kappa(x; Z_1, D)\right] \\
			%
			 & \leq \frac{(s_1 - c)c}{s_1 s_2}
			\E_{D}\left[|\mu(X_{s_1}^{\prime})| \, s_1 \, \kappa(x; Z_{s_1}^{\prime}, D^{\prime})\right]
			\E_{D}\left[|\mu(X_1)| \, s_2 \, \kappa(x; Z_1, D)\right]                                                                                                                             \\
			%
			 & \lesssim \frac{(s_1 - c)c}{s_1 s_2} \mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	Lastly, concerning the fourth term, observe the following.
	\begin{equation}
		\begin{aligned}
			(D)
			 & = \E_{D, D^{\prime}}\left[\left(\sum_{i = c+1}^{s_1}\kappa(x; Z_{i}^{\prime}, D^{\prime}) Y_{i}^{\prime}\right)
				\left(\sum_{j = c+1}^{s_2} \kappa(x; Z_{j}, D) Y_j\right)\right]
			= \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s_1}\sum_{j = c+1}^{s_2}\kappa(x; Z_{i}^{\prime}, D^{\prime}) \kappa(x; Z_{j}, D)  Y_{i}^{\prime}Y_j\right]                                                        \\
			%
			 & = \E_{D, D^{\prime}}\left[\mu(X_{s_1}^{\prime}) \mu(X_{s_2}) \, (s_1 - c)(s_2 -c) \,\kappa(x; Z_{s_1}^{\prime}, D^{\prime}) \kappa(x; Z_{s_2}, D)  \right]                        \\                                                                                                                                                                       \\
			%
			 & = \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}\E_{D, D^{\prime}}\left[\mu(X_{s_1}^{\prime}) \mu(X_{s_2}) \, s_1 s_2 \,\kappa(x; Z_{s_1}^{\prime}, D^{\prime}) \kappa(x; Z_{s_2}, D)  \right] \\                                                                                                                                                                       \\
			%
			 & \leq \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}
			\E_{D^{\prime}}\left[|\mu(X_{s_1}^{\prime})| \, s_1 \,\kappa(x; Z_{s_1}^{\prime}, D^{\prime})   \right]
			\E_{D}\left[ |\mu(X_{s_2})| \, s_2 \, \kappa(x; Z_{s_2}, D)  \right]                                                                                                                                    \\
			%
			 & \lesssim \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
\end{proof}

\newpage
\begin{lem}[Kernel Variance of the TDNN Kernel]\label{lem:Var_TDNN_k}
	For the kernel of the TDNN estimator with subsampling scales $s_1$ and $s_2$, it holds that
	\begin{equation}
		\zeta_{s_1, s_2}^{s_2}\left(x\right)
		\lesssim \mu^2(x) + \overline{\sigma}_{\varepsilon} + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma~\ref{lem:Var_TDNN_k}]
	Consider first the following decomposition.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{s_2}\left(x\right)
			 & = \Var\left(h_{s_1, s_2}\left(x; Z_1, \ldots, Z_{s_2}\right)\right)
			= \Var_{D}\left(h_{s_1, s_2}\left(x; D\right)\right)                                     \\
			%
			 & \leq \E_{D}\left[h_{s_1, s_2}^{2}\left(x; D\right)\right]
			= \E_{D}\left[
				\left(w_{1}^{*}\tilde{\mu}_{s_1}\left(x; D\right) + w_{2}^{*} h_{s_2}\left(x; D\right)\right)^2
			\right]                                                                                           \\
			%
			 & = \left(w_{1}^{*}\right)^2\E_{D}\left[\tilde{\mu}_{s_1}^{2}\left(x; D\right)\right]
			+ 2 w_{1}^{*}w_{2}^{*} \E_{D}\left[\tilde{\mu}_{s_1}\left(x; D\right) h_{s_2}\left(x; D\right)\right]
			+ \left(w_{2}^{*}\right)^2\Omega_{s_2}
		\end{aligned}
	\end{equation}

	Then, observe the following.
	\begin{equation}
		\begin{aligned}
			\E_{D}\left[\tilde{\mu}_{s_1}^{2}\left(x; D\right)\right]
			 & = \E_D\left[\left(\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(x; D_{\ell}\right)\right)^2\right]
			= \binom{s_2}{s_1}^{-2} \E_{D}\left[\sum_{\iota, \iota' \in L_{s_2, s_1}}h_{s_1}\left(x; D_{\iota}\right)h_{s_1}\left(x; D_{\iota'}\right)\right] \\
			%
			 & = \binom{s_2}{s_1}^{-2} \sum_{c = 0}^{s_1} \binom{s_2}{s_1}\binom{s_1}{c}\binom{s_2 - s_1}{s_1 - c} \Omega_{s_1}^{c}
			= \binom{s_2}{s_1}^{-1} \sum_{c = 0}^{s_1} \binom{s_1}{c}\binom{s_2 - s_1}{s_1 - c} \Omega_{s_1}^{c}                                                                \\
			%
			 & \lesssim \Omega_{s_1}
			\lesssim \mu(x)^2 + \sigma_{\varepsilon}^2 + o(1)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}

	Recall that by Lemma~\ref{lem:omega_s}, we have the following.
	\begin{equation}
		\begin{aligned}
			\Omega_{s_2}
			\lesssim \mu(x)^2 + \sigma_{\varepsilon}^2 + o(1)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}

	Lastly, consider the following.
	\begin{equation}
		\begin{aligned}
			\E_{D}\left[\tilde{\mu}_{s_1}\left(x; D\right) h_{s_2}\left(x; D\right)\right]
			 & = \E_D\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(x; D_{\ell}\right)h_{s_2}\left(x; D\right)\right] \\
			%
			 & = \E_D\left[h_{s_1}\left(x; D_{[s_1]}\right)h_{s_2}\left(x; D\right)\right]
			= \Upsilon_{s_1, s_2}\left(x\right)
		\end{aligned}
	\end{equation}

	Thus, we find the following.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_2, s_2}\left(x\right)
			 & \lesssim \left(w_{1}^{*}\right)^2 \Omega_{s_1}
			+ 2 w_{1}^{*}w_{2}^{*} \Upsilon_{s_1, s_2}\left(x\right)
			+ \left(w_{1}^{*}\right)^2 \Omega_{s_2}                                                                       \\
			%
			 & \lesssim \left(w_{1}^{*} + w_{2}^{*}\right)^2 \left(\mu^2(x) + \sigma_{\varepsilon}\right) + o(1)
			= \mu^2(x) + \sigma_{\varepsilon} + o(1).
		\end{aligned}
	\end{equation}
\end{proof}

\newpage

\begin{lem}[Lemma 10 - \citet{demirkaya_optimal_2024}]\label{lem:dem10}
	For the kernel of the TDNN estimator with subsampling scales $s_1$ and $s_2$ satisfying
	\begin{equation}
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1
		\quad \text{and} \quad
		s_2 = o(n),
	\end{equation}
	it holds that
	\begin{equation}
		\zeta_{s_1, s_2}^{1}\left(x\right)
		\sim s_2^{-1}.
	\end{equation}
\end{lem}