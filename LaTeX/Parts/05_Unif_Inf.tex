\section{Uniform Inference for the TDNN Estimator}\label{sec:unif_inf}
\hrule

Noteworthy properties of $\kappa$ are its permutational symmetry in $D_{\ell}$ and that $\kappa$ does not consider the response variable when assigning weights to the observations under consideration.
The latter immediately implies a property that has been called ``Honesty'' by \citet{wager_estimation_2018}.

\begin{boxD}
	\begin{dfn}[Symmetry and Honesty - Adapted from \citet{ritzwoller_simultaneous_2024}]\label{Symmetry_Honesty}\mbox{}
		\begin{enumerate}
			\item The kernel $\kappa\left(\cdot, \cdot, D_{\ell}\right)$ is Honest in the sense that
				  $$\kappa\left(x, X_{i}, D_{\ell}\right) \indep m\left(Z_{i} ; \mu\right) \mid X_{i}, D_{\ell,-i},$$
				  where $\indep$ denotes conditional independence.
			\item The kernel $\kappa\left(\cdot, \cdot, D_{\ell}\right)$ is positive and satisfies the restriction
				  $\sum_{i \in s} \kappa\left(\cdot, X_{i}, D_{\ell}\right)=1$ almost surely.
				  Moreover, the kernel $\kappa\left(\cdot, X_{i}, D_{\ell}\right)$ is invariant to permutations of the data $D_{\ell}.$
		\end{enumerate}
	\end{dfn}
\end{boxD}

\hrule
Absent from \citet{demirkaya_optimal_2024} is a way to construct uniformly valid confidence bands around the TDNN estimator.
Luckily, as a byproduct of considering the methods from \citet{ritzwoller_simultaneous_2024}, procedures for simultaneous inference can be developed relatively easily.

To consider this problem in detail we first introduce additional notation.
Instead of a single point of interest, previously denoted by $x$, we will consider a vector of $p$ points of interest denoted by $x^{(p)} \in \left(\operatorname{supp}\left(X\right)\right)^{p}$.
Consequently, the $j$-th entry of $x^{(p)}$ will be denoted by $x^{(p)}_{j}$.
In an abuse of notation, let functions (such as $\mu$ or the DNN/TDNN estimators) evaluated at $x^{(p)}$ denote the vector of corresponding function values evaluated at the point, respectively.
It should be pointed out that, due to the local definition of the kernel in the estimators, this does not translate to the evaluation of the same function at different points in the most immediate sense.
To summarize the kind of object we want to construct, we define a simultaneous confidence region for the TDNN estimator in the following way following closely the notation of \citet{ritzwoller_simultaneous_2024}.

\begin{boxD}
	\begin{dfn}[Uniform Confidence Regions]\mbox{}\\*
		A confidence region for the TDNN (or DNN) estimators that is uniformly valid at the rate $r_{n,d}$ is a family of random intervals
		\begin{equation}
			\hat{\mathcal{C}}\left(x^{(p)}\right)
			:= \left\{\hat{C}(x^{(p)}_{j})
			= \left[c_{L}(x^{(p)}_{j}), c_{U}(x^{(p)}_{j})\right]\, : \, j \in [p]\right\}
		\end{equation}
		based on the observed data, such that
	
		\begin{equation}
			\sup_{P \in \mathbf{P}} \left| P\left(\mu(x^{(d)}) \in \hat{\mathcal{C}}\left(x^{(d)}\right)\right) \right| \leq r_{n,d}
		\end{equation}
		for some sequence $r_{n,d}$, where $\mathbf{P}$ is some statistical family containing $P$.
	\end{dfn}
\end{boxD}

\subsection{Low-Level}
In our pursuit of constructing simultaneous confidence regions for the TDNN estimator, we return to the results from \citet{ritzwoller_simultaneous_2024} in their high-dimensional form.

\begin{boxD}
	\begin{thm}[\citet{ritzwoller_simultaneous_2024} - Theorem 4.1]\label{thm:rit4_1}\mbox{}\\*
		For any sequence of kernel orders $b=b_n$, where
		\begin{equation}
			\frac{1}{n} \frac{\nu_j^2}{\sigma_{b, j}^2} \rightarrow 0
			\quad \text{as} \quad
			n \rightarrow \infty,
		\end{equation}
		we have that
		\begin{equation}
			\sqrt{\frac{n}{\sigma_{b, j}^2 b^2}} \binom{n}{b}^{-1} \sum_{\mathbf{s} \in \mathbf{S}_{n, b}} u\left(x^{(p)}_{j} ; D_{\mathbf{s}}\right) \rightsquigarrow \mathcal{N}(0,1),
			\quad \text{as} \quad
			n \rightarrow \infty.
		\end{equation}
	\end{thm}
\end{boxD}

\begin{boxD}
	\begin{thm}[Adapted from \citet{ritzwoller_simultaneous_2024} - Theorem 4.2]\label{thm:rit4_2}\mbox{}\\*
		Define the terms
		\begin{equation}
			\bar{\psi}_{s_2}^2
			= \max_{j \in[p]}\left\{\nu_j^2- s_2 \sigma_{s_2, j}^2\right\}
			%
			\quad \text {and} \quad
			%
			\underline{\sigma}_{s_2}^2
			= \min_{j \in[p]} \sigma_{s_2, j}^2.
		\end{equation}
		If the kernel function $h_{s_1, s_2}\left(x ; D_{\ell}\right)$ satisfies the bound
		\begin{equation}
			\left\|h_{s_1, s_2}\left(x ; D_{\ell}\right)\right\|_{\psi_1} \leq \phi
		\end{equation}
		for each $j$ in $[d]$, then
		\begin{equation}
			\sqrt{\frac{n}{s_2^2 \underline{\sigma}_{s_2}^2}}
			\left\|\hat{\mu}_{s_1, s_2}(x^{(p)}; \mathbf{D}_n) - \mu(x^{(p)}) - \frac{s_2}{n} \sum_{i=1}^n h^{(1)}_{s_1, s_2}(x^{(p)}; \mathbf{z}_{i})\right\|_{\infty}
			%
			= \sqrt{\frac{n}{s_2^2 \underline{\sigma}_{s_2}^2}} \left\|\operatorname{HR}_{s_1, s_2}(x^{(p)}; \mathbf{D}_n)\right\|_{\infty}
			%
			\lesssim \xi_{n, s_2},
		\end{equation}
		where
		\begin{equation}
			\xi_{n, s_2}
			= \left(\frac{C s_2 \log(p n)}{n}\right)^{s_2 / 2}\left(\left(\frac{n \bar{\psi}_{s_2}^2}{{s_2}^2 \underline{\sigma}_{s_2}^2}\right)^{1 / 2}+\left(\frac{\phi^2 s_2 \log ^4(p n)}{\underline{\sigma}_{s_2}^2}\right)^{1 / 2}\right),
		\end{equation}
		with probability greater than $1-C / n$.
	\end{thm}
\end{boxD}

\subsection{High-Level}
Recent advances in the field of simultaneous inference for infinite-order U-statistics, specifically \citet{ritzwoller_simultaneous_2024}, and careful analysis of the Hoeffding projections of different orders will be the cornerstones in developing simultaneous inference methods.
The authors' approach to constructing simultaneous confidence regions is based on the half-sample bootstrap root.

\begin{boxD}
	\begin{dfn}[Half-Sample Bootstrap Root Approximation - \citet{ritzwoller_simultaneous_2024}]\mbox{}\\*
		The Half-Sample Bootstrap Root Approximation of the sampling distribution of the root
		\begin{equation}
			R\left(x^{(p)}; \mathbf{D}_n\right)
			:= \hat{\mu}\left(x^{(p)}; \mathbf{D}_n\right) - \mu(x^{(p)})
		\end{equation}
		is given by the conditional distribution of the half-sample bootstrap root
		\begin{equation}
			R^{*}\left(x^{(p)}; \mathbf{D}_n\right)
			:= \hat{\mu}\left(x^{(p)}; D_l\right) - \hat{\mu}\left(x^{(p)}; \mathbf{D}_n\right)
		\end{equation}
		where $l$ denotes a random element from $L_{n, n/2}$.
	\end{dfn}
\end{boxD}

Next, to standardize the relevant quantities, we introduce a corresponding studentized process.
\begin{equation}
	\hat{\lambda}_{j}^{2}\left(x^{(p)}; \mathbf{D}_n\right) = \Var\left(\sqrt{n} R^{*}(x^{(p)}_{j}; \mathbf{D}_n) \, | \, \mathbf{D}_n\right)
	\quad \text{and} \quad
	\hat{\Lambda}_n\left(x^{(p)}; \mathbf{D}_n\right) = \operatorname{diag}\left(\left\{\hat{\lambda}_{j}^{2}\left(x^{(p)}; \mathbf{D}_n\right)\right\}_{j = 1}^{p}\right)
\end{equation}
\begin{equation}
	\hat{S}^{*}\left(x^{(p)}; \mathbf{D}_n\right)
	:= \sqrt{n} \, \Big\| \left(\hat{\Lambda}_n\left(x^{(p)}; \mathbf{D}_n\right)\right)^{-1/2} R^{*}\left(x^{(p)}; \mathbf{D}_n\right)\Big\|_{2}
\end{equation}
Let $\operatorname{cv}\left(\alpha; \mathbf{D}_n\right)$ denote the $1-\alpha$ quantile of the distribution of $\hat{S}^{*}\left(x^{(p)}; \mathbf{D}_n\right)$.
As the authors point out specifically, and as indicated by the more explicit notation chosen in this presentation, this is a quantile of the conditional distribution given the data $\mathbf{D}_n$.
Given this construction, the simultaneous confidence region developed in \citet{ritzwoller_simultaneous_2024} adapted to the TDNN estimator takes the following form.

\begin{boxD}
	\begin{thm}[Uniform Confidence Region - \citet{ritzwoller_simultaneous_2024}]\mbox{}\\*
		Define the intervals
		\begin{equation}
			\hat{\mathcal{C}}\left(x^{(p)}_j; \mathbf{D}_n\right)
			:= \hat{\mu}\left(x^{(p)}_{j}; \mathbf{D}_n\right) \pm
			n^{-1/2} \hat{\lambda}_{j}\left(x^{(p)}; \mathbf{D}_n\right)\operatorname{cv}\left(\alpha; \mathbf{D}_n\right)
		\end{equation}
		The $\alpha$-level simultaneous confidence region for $\mu\left(x^{(p)}\right)$ is given by $\hat{\mathcal{C}}\left(x^{(p)}\right)$.
	\end{thm}
\end{boxD}

To justify the use of this simultaneous confidence region, it remains to be shown if and how the other conditions for the inner workings of this procedure apply to the TDNN estimator.
This is substantially simplified due to the absence of a nuisance parameter.
Thus, consider the following conditions from \cite{ritzwoller_uniform_2024} that are simplified to fit the problem at hand.

\begin{boxD}
	\begin{dfn}[Shrinkage and Incrementality - Adapted from \citet{ritzwoller_simultaneous_2024}]\mbox{}\\*
		We say that the kernel $\kappa\left(\cdot, \cdot, D_{\ell}\right)$ has a simultaneous shrinkage rate $\epsilon_b$ if
		\begin{equation}
			\sup_{P \in \mathbf{P}} \sup_{j \in[p]}
			\E\left[\max \left\{\left\|X_{i}-x^{(p)}_{j}\right\|_{2}: \kappa\left(x^{(p)}_{j}, X_{i}, D_{\ell}\right)>0\right\}\right]
			\leq \epsilon_b .
		\end{equation}
		We say that a kernel $\kappa\left(\cdot, \cdot, D_{\ell}\right)$ is uniformly incremental if
		\begin{equation}
			\inf_{P \in \mathbf{P}} \sup_{j \in[p]}
			\Var\left(\E\left[\sum_{i \in \ell} \kappa\left(x^{(p)}_{j}, X_{i}, D_{\ell}\right) m\left(Z_{i} ; \mu\right) \mid l \in \ell, Z_l = Z\right]\right)
			\gtrsim b^{-1}
		\end{equation}
		where $Z$ is an independent random variable with distribution $P$.
	\end{dfn}
\end{boxD}

Translating these properties to suit the TDNN regression problem, we obtain the following conditions that need to be verified.
First, to verify simultaneous shrinkage at a rate $\epsilon_b$, the following remains to be shown.
\begin{equation}
	\sup_{P \in \mathbf{P}} \sup_{j \in[p]}
	\E\left[\max \left\{\left\|X_{i}-x^{(p)}_{j}\right\|_{2}:
	\rk(x^{(p)}_{j}; X_{i}, D_{\ell}) = 1\right\}\right]
	\leq \epsilon_b
\end{equation}
Second, for simultaneous incrementality, we need to show the following.
\begin{equation}
	\begin{aligned}
		  & \inf_{P \in \mathbf{P}} \sup_{j \in[p]}
		\Var\left(\E\left[
				\sum_{i \in \ell} \1\left(\rk(x^{(p)}_{j}; X_{i}, D_{\ell}) = 1\right) \left(Y_{i} - \mu\left(X_{i}\right)\right) \mid l \in \ell, Z_l = Z\right]
		\right)                                     \\
		%
		= & \inf_{P \in \mathbf{P}} \sup_{j \in[p]}
		\Var\left(
		\sum_{i \in \ell}\E\left[\1\left(\rk(x^{(p)}_{j}; X_{i}, D_{\ell}) = 1\right) \varepsilon_i \mid l \in \ell, Z_l = Z\right]
		\right)                                     \\
		%
		= & \inf_{P \in \mathbf{P}} \sup_{j \in[p]}
		\Var\left(
		\sum_{i = 1}^{s}\E\left[\1\left(\rk(x^{(p)}_{j}; X_{i}, D_{1:s}) = 1\right) \varepsilon_i \mid l \in [s], Z_l = Z\right]
		\right)                                     \\
		%
		= & \inf_{P \in \mathbf{P}} \sup_{j \in[p]}
		s^2 \cdot \Var\left(
		\E\left[\1\left(\rk(x^{(p)}_{j}; X_1, D_{1:s}) = 1\right) \varepsilon_1 \mid l \in [s], Z_l = Z\right]
		\right)
		\gtrsim b^{-1}
	\end{aligned}
\end{equation}

To verify these assumptions, recent theory developed in \citet{peng_rates_2022} is of great help.
Specifically, the following Proposition and its proof are helpful in showing the desired simultaneous incrementality property.

	{\color{red} LOREM IPSUM}

\begin{boxD}
	\begin{asm}[Boundedness - Adapted from \citet{ritzwoller_simultaneous_2024}]\mbox{}\\*
		The absolute value of the function $m(\cdot; \mu)$ is bounded by the constant $(\theta+1) \phi$ almost surely.
		\begin{equation}
			|m(Z_{i} ; \mu)|
			= |Y_{i} - \mu(X_{i})|
			= |\varepsilon_i|
			\leq (\theta+1) \phi
			\quad \text{a.s.}
		\end{equation}
	\end{asm}
\end{boxD}
To follow the notational conventions, we will further define the two functions $m^{(1)}(Z_{i}; \mu) = - \mu(X_{i})$ and $m^{(2)}(Z_{i}) = Y_{i}$.
As the authors point out, the boundedness condition can easily be replaced by a condition on the subexponential norm.
This, being more in line with the assumptions of \citet{demirkaya_optimal_2024}, is a desirable substitution.
Thus, we will instead consider the following assumption and fill in parts of the proofs that hinge on boundedness for ease of exposition in the original paper.

\begin{boxD}
	\begin{asm}[Sub-Exponential Norm Bound]\mbox{}\\*

	\end{asm}	
\end{boxD}


{\color{red} LOREM IPSUM}

% 
% \begin{assumption}[Consistency]

% \end{assumption}

% {\color{red} LOREM IPSUM}

\begin{boxD}
	\begin{asm}[Moment Smoothness - Adapted from \citet{ritzwoller_simultaneous_2024}]\mbox{}\\*
		Define the moments
		\begin{equation}
			M^{(1)}(x ; \mu)
			= \E\left[m^{(1)}\left(Z_{i} ; \mu\right) \mid X_{i}= x\right]
			\quad \text{and} \quad
			M^{(2)}(x)
			= \E\left[m^{(2)}\left(Z_{i}\right) \mid X_{i} = x\right],
		\end{equation}
		associated with the functions $m^{(1)}(\cdot ; \mu)$ and $m^{(2)}(\cdot)$.
		Plugging in yields the following functions.
		\begin{equation}
			M^{(1)}(x ; \mu)
			= -\mu(x)
			\quad \text{and} \quad
			M^{(2)}(x)
			= \mu(x).
		\end{equation}
		Both moments are uniformly Lipschitz in their first component, in the sense that
		\begin{equation}
			\forall x, x^{\prime} \in \operatorname{supp}\left(X\right): \quad
			\sup _{P \in \mathbf{P}}
			\left|\mu(x)-\mu\left(x^{\prime}\right)\right|
			\lesssim\left\|x-x^{\prime}\right\|_{2}.
		\end{equation}
		and $M^{(1)}$ is bounded below in the following sense
		\begin{equation}
			\inf_{P \in \mathbf{P}} \inf_{j \in [p]} \Big|M^{(1)}\left(x^{(p)}_{j}\right) \Big|
			= \inf_{P \in \mathbf{P}} \inf_{j \in [p]} \Big|\mu\left(x^{(p)}_{j}\right) \Big| \geq c
		\end{equation}
		for some positive constant $c$.
	\end{asm}
\end{boxD}


The Lipschitz continuity part of this assumption translates directly into a Lipschitz continuity assumption on the unknown nonparametric regression function.
The boundedness assumption is
	{\color{red} LOREM IPSUM}

\subsection{Uniform Inference with the TDNN Estimator}
\begin{boxD}
	\begin{thm}[Uniform Inference in Nonparametric Regression]\label{thm:unif_inf_TDNN}\mbox{}\\*
		{\color{red} LOREM IPSUM}
	\end{thm}
\end{boxD}

\begin{boxD}
	\begin{thm}[Uniform Inference in Heterogeneous Treatment Effect Estimation]\label{thm:unif_inf_TDNN_HTE}\mbox{}\\*
		{\color{red} LOREM IPSUM}
	\end{thm}
\end{boxD}
