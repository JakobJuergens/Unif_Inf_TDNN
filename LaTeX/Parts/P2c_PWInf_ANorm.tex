\subsection{Asymptotic Normality Results}
\hrule

% Before going on to the proofs for the theorems showing asymptotic normality, we introduce some notation.
% First, we observe that the Neyman-orthogonal score function of interest $m$ is linear and can be decomposed as follows.
% \begin{equation}
%     \begin{aligned}
%         m\left(Z_{i}; \operatorname{CATE}(x), \eta\right)
% 		  & = \underbrace{\mu_{0}^{1}\left(X_{i}\right) - \mu_{0}^{0}\left(X_{i}\right) + \beta_{0}\left(W_{i}, X_{i}\right)\left(Y_{i} - \mu_{W_{i}}\left(X_{i}\right)\right)}_{\psi^{b}\left(Z_{i}; \eta\right)} 
%           - \operatorname{CATE}\left(x\right)\\
%           %
%           & = - \operatorname{CATE}\left(x\right) + \psi^{b}\left(Z_{i}; \eta\right)
%     \end{aligned}
% \end{equation}  

\subsection{NPR-Estimators - Asymptotic Normality}
\hrule

\subsection{CATE-Estimators - Asymptotic Normality}
\hrule
Recall the decomposition of the DNN-DML2 estimator introduced in equation \ref{eq:DNNDML2_Decomp}.
\begin{equation}
    \begin{aligned}
        \hat{\theta}\left(x; \mathbf{D}\right)
        & = \underbrace{\E_{D}\left[\hat{\theta}\left(x; \mathbf{D}\right)\right]}_{\text{Centering-Term}}
        + \underbrace{\frac{s}{n}\sum_{i = 1}^{n} \chi_{s,0}^{(1)}\left(x; Z_{i}\right)}_{\text{Oracle-H\'ajek-Projection}}
        + \underbrace{\frac{s}{k} \sum_{l = 1}^{k} \frac{1}{m} \sum_{i \in \mathcal{I}_{k}}\left(\underbrace{\chi_{s}^{(1)}\left(x; Z_{i}, \hat{\eta}_{k}\right) - \chi_{s,0}^{(1)}\left(x; Z_{i}\right)}_{R_{1,k}\left(x; Z_{i}\right)}\right)}_{\text{Oracle-H\'ajek-Projection Error}}\\
        & \quad \quad + \underbrace{\sum_{j = 2}^{s} \binom{s}{j} \binom{n}{j}^{-1}\sum_{\ell \in L_{n,j}} \chi_{s,0}^{(j)}\left(x; \mathbf{D}_{\ell}\right)}_{\text{Oracle-H\'ajek-Residual}}
         + \underbrace{\sum_{j = 2}^{s} \binom{s}{j} \binom{n}{j}^{-1}\sum_{\ell \in L_{n,j}} R_{j}\left(x; \mathbf{D}_{\ell}\right)}_{\text{Higher-Order Error Terms}}
    \end{aligned}
\end{equation}
where we have the following definition from Equation \ref{eq:DNNDML2_ResidDecomp}.
\begin{equation}
    \begin{aligned}
        \chi_{s}^{(c)}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right)
        & = \chi_{s,0}^{(c)}\left(x; \mathbf{D}_{\ell}\right) + \underbrace{\chi_{s}^{(c)}\left(x; \mathbf{D}_{\ell}, \hat{\eta}\right) - \chi_{s,0}^{(c)}\left(x; \mathbf{D}_{\ell}\right)}_{R_{c}\left(x; \mathbf{D}_{\ell}\right)}
    \end{aligned}
\end{equation}

\hrule

\begin{lem}[Behavior of Oracle-H\'ajek-Projection Error]\label{lem:ps_hajek_error}\mbox{}\\*
    
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:ps_hajek_error}]\mbox{}\\*
    Consider first the average Oracle-error within a given fold $k$ and observe the following.
    \begin{equation}
        \begin{aligned}
            \bar{R}_{1, k}\left(x\right)
            & = \frac{1}{m}\sum_{l \in I_{k}}R_{1, k}\left(x, Z_{l}\right)
            = \frac{1}{m}\sum_{l \in I_{k}}
            \left(\chi_{s}^{(1)}\left(x; Z_l, \hat{\eta}_{k}\right) - \chi_{s,0}^{(1)}\left(x; Z_l\right)\right) \\
            %
            & = \frac{1}{m}\sum_{l \in I_{k}}
            \left(\vartheta_{s}^{1}\left(x; Z_l, \hat{\eta}_{k}\right)
            - \E_{D}\left[\chi_{s}(x; \mathbf{D}_{[s]}, \hat{\eta}_{k})\right]
            - \vartheta_{s,0}^{1}\left(x; Z_l\right) 
            + \E_{D}\left[\chi_{s,0}\left(x; \mathbf{D}_{[s]}\right)\right]\right)\\
            %
            & = \frac{1}{m}\sum_{l \in I_{k}}
            \E_{D}\left[\chi_{s}(x; \mathbf{D}_{[s]}, \hat{\eta}_{k}) - \chi_{s,0}(x; \mathbf{D}_{[s]})\, \middle| \, Z_1 = Z_l \right]
            - \E_{D}\left[\chi_{s}(x; \mathbf{D}_{[s]}, \hat{\eta}_{k}) - \chi_{s,0}\left(x; \mathbf{D}_{[s]}\right)\right] \\
            %
            & = \frac{1}{m s!}\sum_{l \in I_{k}}
            \E_{D}\left[\sum_{i = 1}^{n}\kappa(x; Z_{i}, \mathbf{D}_{[s]})
            \left(m\left(Z_{i}, \hat{\eta}_{k}\right) - m\left(Z_{i}, \eta_{0}\right)\right) \, \middle| \, Z_1 = Z_l \right] \\
            & \quad - \E_{D}\left[\chi_{s}(x; \mathbf{D}_{[s]}, \hat{\eta}_{k}) - \chi_{s,0}\left(x; \mathbf{D}_{[s]}\right)\right] \\
            %
            & = \frac{1}{m s!}\sum_{l \in I_{k}}
            \Bigg\{
                \underbrace{\E_{D}\left[\kappa(x; Z_{1}, \mathbf{D}_{[s]})m\left(Z_{1}, \hat{\eta}_{k}\right)\, \middle| \, Z_1 = Z_l \right]}_{(A_{l})}
                - \underbrace{m\left(Z_{l}, \eta_{0}\right)\E_{D}\left[\kappa(x; Z_{1}, \mathbf{D}_{[s]}) \, \middle| \, Z_1 = Z_l \right]}_{(B_{l})}  \\
            & \quad \quad  + (s-1) \left( \underbrace{\E_{D}\left[\kappa(x; Z_{2}, \mathbf{D}_{[s]})m\left(Z_{2}, \hat{\eta}_{k}\right)\, \middle| \, Z_1 = Z_l \right]}_{(C_{l})}
            - \underbrace{\E_{D}\left[\kappa(x; Z_{2}, \mathbf{D}_{[s]})m\left(Z_{2}, \eta_{0}\right) \, \middle| \, Z_1 = Z_l \right]}_{(D_{l})} \right)
            \Bigg\} \\
            & \quad - \E_{D}\left[\chi_{s}(x; \mathbf{D}_{[s]}, \hat{\eta}_{k}) - \chi_{s,0}\left(x; \mathbf{D}_{[s]}\right)\right]
        \end{aligned}
    \end{equation}
    Here, in analogy to step 3 in the proof of Theorem 3.1 in \citet{chernozhukov_doubledebiased_2018}, we can now observe the following.
    \begin{equation}
        \begin{aligned}
            \left|\bar{R}_{1, k}\left(x\right)\right|
            & \leq \mathcal{I}_{3,k}^{(1)} + \mathcal{I}_{4,k}^{(1)}
        \end{aligned}
    \end{equation}
    where
    \begin{equation}
        \begin{aligned}
            \mathcal{I}_{1,k}^{(1)} 
            & = {\color{red} LOREM IPSUM}
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            \mathcal{I}_{2,k}^{(1)} 
            & = {\color{red} LOREM IPSUM}
        \end{aligned}
    \end{equation}
    Notice that conditional on $\mathbf{D}_{I_{k}^{C}}$ the first-stage estimate $\hat{\eta}_{k}$ is non-stochastic.
{\color{red} LOREM IPSUM}
\end{proof}

\hrule

\begin{lem}[Behavior of Higher-Order Error Terms]\label{lem:HO_error}\mbox{}\\*
    
\end{lem}

\begin{proof}[Proof of Lemma \ref{lem:HO_error}]\mbox{}\\*
    
\end{proof}

\hrule