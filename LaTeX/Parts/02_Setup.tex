\section{Setup}\label{sec:setup}
\hrule
Throughout this paper, I will consider two distinct setups focusing on separate but intertwined problems.
As a running example to give immediate economic meaning to the statistical problems, I consider the problem of evaluating a job training program given only observational data in the style of \citet{lalonde_evaluating_1986}.
\begin{boxE}
	\begin{exmp}[Average Hourly Earnings and IT Training]\label{exmp:run_exmp}\mbox{}\\*
        Imagine obtaining a large data set containing hourly earnings $Y$ of workers with age $X_1$ and $X_2$ years of education.
        A typical quantity of interest is the average hourly earnings given a specific combination of worker characteristics, for example the average earnings for a 20-year-old worker with 12 years of education (a recent high school graduate without further education).
	\end{exmp}
\end{boxE}
To simplify the presentation, the running example ignores many widely discussed economic problems in this environment and instead focuses on a highly stylized problem with a limited set of characteristics.
\begin{boxD}
	\begin{asm}[Nonparametric Regression DGP]\label{asm:npr_dgp}\mbox{}\\*
		The observed data consists of an i.i.d.\ sample taking the following form.
		\begin{equation}\label{DGP1}
			\mathbf{D}_n = \{Z_{i} = (X_{i}, Y_{i})\}_{i = 1}^{n}
			\quad \text{from the model} \quad
			Y = \mu(X) + \varepsilon,
		\end{equation}
		where $Y \in \mathcal{Y} \subset \mathbb{R}$ is the response, $X \in \mathcal{X} \subset \mathbb{R}^k$ is a feature vector of fixed dimension $k$ distributed according to a density function $f$ with associated probability measure $\varphi$ on $\mathcal{X}$, and $\mu(x)$ is the unknown mean regression function.
		$\varepsilon$ is the unobservable model error on which I impose the following conditions.
		\begin{equation}
			\E\left[\varepsilon \, \middle| \, X\right] = 0, \quad
			\Var\left(\varepsilon \, \middle| \, X = x\right) = \sigma_{\varepsilon}^2\left(x\right)
		\end{equation}
		Let the distribution induced by this model be denoted by $P$ and thus $Z_{i} = \left(X_{i}, Y_{i}\right) \overset{\text{iid}}{\sim} P$.
	\end{asm}
\end{boxD}
This first statistical setup is a pure nonparametric regression setup that closely mirrors the structure of \citet{demirkaya_optimal_2024} and will be useful to illustrate the inner workings of the estimator of interest.
We will also consider a second statistical setting with more immediate econometric relevance: estimation of and inference on heterogeneous treatment effects in the potential outcomes framework.
This statistical setup serves as a more immediately applicable version of the theoretical setup presented in \citet{ritzwoller_simultaneous_2024} and brings their results closer to practitioners in the field of economics.
\begin{boxE}
    \addtocounter{exmp}{-1}
    \begin{exmp}[Average Hourly Earnings and IT Training - continued]\mbox{}\\*
        Consider the introduction of a job training program that covers basic IT skills that potentially influence the average earnings of a worker with given characteristics.
        Think of providing said IT training to a 20-year-old high-school graduate without further education or a 56-year-old with a PhD. It seems unreasonable to expect the effect of basic IT training on hourly wages to be similar in these cases.
        Furthermore, it seems unlikely for these participants to choose to participate in the training with equal probability, adding an additional complicating factor.
    \end{exmp}    
\end{boxE}
\begin{boxD}
	\begin{asm}[Heterogeneous Treatment Effect DGP]\label{asm:CATE_dgp}\mbox{}\\*
		The observed data consists of an i.i.d. sample taking the following form.
		\begin{equation}\label{DGP2}
			\begin{aligned}
				\mathbf{D}_n & = \{Z_{i} = (X_{i}, W_{i}, Y_{i})\}_{i = 1}^{n}
				\quad \text{from the model} \quad
				Y = \1(W = 0)\mu_{0}^{0}(X) + \1(W = 1)\mu_{0}^{1}(X) + \varepsilon,	\\
				W_{i} & \sim \operatorname{Bern}\left(\pi_{0}\left(X_{i}\right)\right)
			\end{aligned}
		\end{equation}
		where $Y \in \mathcal{Y} \subset \mathbb{R}$ is the response and $W \in \{0,1\}$ is an observed treatment indicator.
		$X \in \mathcal{X} \subset \mathbb{R}^k$ is a vector of covariates of fixed dimension $k$ distributed according to a density function $f$ with an associated probability measure $\varphi$ on $\mathcal{X}$ and $\varepsilon$ is the unobservable model error on which I impose the following conditions.
		\begin{equation}
			\varepsilon \indep W \, | \, X, \quad
			\E\left[\varepsilon \, | \, X\right] = 0, \quad
			\Var\left(\varepsilon \, | \, X = x\right) = \sigma_{\varepsilon}^2\left(x\right)
		\end{equation}
		Furthermore, $\mu_{0}^{0}:\mathcal{X} \rightarrow \mathbb{R}$ and $\mu_{0}^{1}:\mathcal{X} \rightarrow \mathbb{R}$ are the two unknown potential outcome functions and $\pi_{0}:\mathcal{X} \rightarrow [0,1]$ is a function describing the probability of treatment uptake, effectively corresponding to the propensity score.
        \begin{itemize}
            \item We denote the vector of functional nuisance parameters by $\eta_{0} = \left(\mu_{0}^{0}, \mu_{0}^{1}, \pi_{0}\right)^{\prime}$.
            \item Let the distribution induced by this model be denoted by $Q$ and thus $Z_{i} = \left(X_{i}, W_{i}, Y_{i}\right) \overset{\text{iid}}{\sim} Q$ supported on $\mathcal{Z} \subset \mathcal{X}\times\{0,1\}\times\mathcal{Y}$.
        \end{itemize}
	\end{asm}
\end{boxD}
In this second setting, I will use the notation $\mathbf{D}^{(0)}$ and $\mathbf{D}^{(1)}$ to refer to the data subsets that contain only observations with $W = 0$ and $W = 1$, respectively.
Clearly, this model can be interpreted in the context of the potential outcomes framework in the usual way.
Throughout this paper, I will additionally rely on a number of assumptions that are more technical in nature.
\begin{boxD}
	\begin{asm}[Technical Assumptions]\label{asm:technical}\mbox{}\\*
		In both settings (Setup~\ref{asm:npr_dgp} and Setup~\ref{asm:CATE_dgp}) the following conditions hold as applicable to the settings, respectively.
		\begin{itemize}
			\item The feature space $\mathcal{X} = \operatorname{supp}(X)$ is a bounded, compact subset of $\mathbb{R}^k$
			\item The density $f(\cdot)$ is bounded away from 0 and $\infty$.
            \begin{equation}
                \forall x \in \mathcal{X}: \quad 0 < \underline{\mathfrak{f}} \leq f(x) \leq \overline{\mathfrak{f}} < \infty
            \end{equation}
			\item $f(\cdot)$, $\mu(\cdot)$, $\mu_{0}^{0}(\cdot)$, $\mu_{0}^{1}(\cdot)$, and $\pi_{0}(\cdot)$ are four times continuously differentiable with bounded second, third, and fourth-order partial derivatives. Specifically, in mathematical terms: 
            \begin{equation}
            \forall w\in \{0,1\} \quad \forall x \in \mathcal{X} \quad \forall (i,j,k,l) \in [d]^{4}:
            \end{equation}
            \vspace{-0.8cm}
            \begin{alignat*}{14}
                -\infty & \; < \; & \underline{\mathfrak{f}}^{\prime} 
                & \; \leq \; && \partial_{i,j} f(x), \; && \partial_{i,j,k} f(x), \; && \partial_{i,j,k,l} f(x) && \; \leq \; & \overline{\mathfrak{f}}^{\prime} & \; < \; & \infty \\
                %
                -\infty & \; < \; & \underline{\mathfrak{m}}^{\prime} 
                & \; \leq \; && \partial_{i,j} \mu(x), \; && \partial_{i,j,k} \mu(x), \; && \partial_{i,j,k,l} \mu(x)
                && \; \leq \; & \overline{\mathfrak{m}}^{\prime} & \; < \; & \infty \\
                %
                -\infty & \; < \; & \underline{\mathfrak{m}}^{\prime} 
                & \; \leq \; && \partial_{i,j} \mu_{0}^{w}(x), \; && \partial_{i,j,k} \mu_{0}^{w}(x), \; && \partial_{i,j,k,l} \mu_{0}^{w}(x)
                && \; \leq \; & \overline{\mathfrak{m}}^{\prime} & \; < \; & \infty \\
                %
                -\infty & \; < \; & \underline{\mathfrak{p}}^{\prime} 
                & \; \leq \; && \partial_{i,j} \pi_{0}(x), \; && \partial_{i,j,k} \pi_{0}(x), \; && \partial_{i,j,k,l} \pi_{0}(x)
                && \; \leq \; & \overline{\mathfrak{p}}^{\prime} & \; < \; & \infty
            \end{alignat*}
            % in a neighborhood of $x$
            \item $\mu(\cdot), \mu_{0}^{0}(\cdot), \mu_{0}^{1}(\cdot) \in L^{2}\left(\mathcal{X}\right)$ are square-integrable functions on $\mathcal{X}$.
		\end{itemize}
	\end{asm}
\end{boxD}
There is considerable potential to relax these assumptions at the cost of requiring both less interpretable conditions and more technically sophisticated proofs. 
For example, the bounded derivatives condition can be relaxed to hold only in a neighborhood of $x$ while requiring a weaker, more complex condition on the behavior of the derivatives beyond that neighborhood.
Furthermore, it is necessary to point out again that this setup considers a highly stylized example, as applying these conditions to the IT training program quickly runs into problems.
\begin{boxE}
    \addtocounter{exmp}{-1}
    \begin{exmp}[Average Hourly Earnings and IT Training - continued]\mbox{}\\*
        Consider two types of workers: those with 12 years of education, i.e., a high school diploma and those who drop out slightly before finishing high school and therefore have slightly less than 12 years of education.
        Given the continuous differentiability assumption on the regression function, I do not allow for a discontinuity in average earnings between high school graduates and dropouts.
        This seems unreasonable as employers would ceteris paribus prefer the worker with a high school diploma in most circumstances.
        Similarly, I would expect discontinuities in the distribution of educational attainment, for example right after the standard time necessary to achieve common educational milestones.
        For the sake of a simplified exposition, I will ignore problems such as this going forward.
    \end{exmp}    
\end{boxE}
Additionally, I require a rather standard assumption in localized regression approaches, namely that the variance changes continuously.
\begin{boxD}
	\begin{asm}[Error Distribution Assumptions]\label{asm:errors}\mbox{}\\*
		The error terms $\varepsilon$ defined in Setup~\ref{asm:npr_dgp} and Setup~\ref{asm:CATE_dgp}, respectively, fulfill the following conditions.
        \begin{itemize}
            \item $\varepsilon$ has continuously varying variance.
		      In other words, $\sigma^2_{\varepsilon}: \mathcal{X} \rightarrow \mathbb{R}_{>0}$ is a continuous function.
            \item $\sigma^2_{\varepsilon} \in L^{2}\left(\mathcal{X}\right)$ is a square-integrable function on $\mathcal{X}$
        \end{itemize}
	\end{asm}
\end{boxD}
As $\mathcal{X}$ is a compact and bounded set, this implies that there exists a $\overline{\sigma}_{\varepsilon}^2 > 0$ such that for any $x \in \mathcal{X}$ we have $\sigma^{2}_{\varepsilon}\left(x\right) \leq \overline{\sigma}_{\varepsilon}^2$.
Readers of \citet{demirkaya_optimal_2024} will recognize that this setup, in contrast to the original paper, allows for heteroskedasticity of the error terms.
This comes at basically no cost as the original proofs can be used nearly unchanged to prove the corresponding theorems on distributional approximations.
Additionally, due to the assumptions on the regression functions, this ensures the existence of seconds moments of $Y$ in both scenarios.
Furthermore, to ensure that there are a sufficient number of treated and untreated observations local to each point of interest asymptotically, I require the following condition on the treatment assignment and uptake mechanism.
\begin{boxD}
	\begin{asm}[Non-Trivial Treatment Overlap]\label{asm:treatment_overlap}\mbox{}\\*
		In the Heterogeneous Treatment Effect Setup (Assumption~\ref{asm:CATE_dgp}), I assume that there exists a constant $\mathfrak{p} \in (0, 1/2)$ such that
		\begin{equation}
			\forall x \in \mathcal{X}: \quad 
			0 < \mathfrak{p} \leq \pi_{0}\left(x\right) \leq 1 - \mathfrak{p} < 1.
		\end{equation}
	\end{asm}
\end{boxD}
This assumption seems rather strong when considering the full universe of potential treatment recipients.
In reality, we can constrain this assumption of overlap to neighborhoods of points of interest $x$.
As long as there is sufficient overlap in those neighborhoods the ideas of our identification strategy continue to hold locally.
\begin{boxE}
    \addtocounter{exmp}{-1}
    \begin{exmp}[Average Hourly Earnings and IT Training - continued]\mbox{}\\*
        In the IT training example, this condition requires that for each combination of characteristics, there are workers who choose to participate in the IT training and those who do not.
        More precisely, I assume that as I consider larger and larger data sets, there are both types of workers with characteristics that are very similar to those we are currently interested in.
        Intuitively this ensures that we have a suitable basis of workers to achieve meaningful comparisons.
    \end{exmp}    
\end{boxE}

% \begin{boxD}
% 	\begin{asm}[Stable Unit Treatment Value Assumption (SUTVA)]\label{asm:sutva}\mbox{}\\*
% 		For any $n$, let $\mathfrak{W}_{n}: \mathcal{X}^{n} \rightarrow \{0,1\}^{n}$ and $\mathfrak{W}_{n}^{\prime}: \mathcal{X}^{n} \rightarrow \{0,1\}^{n}$ be two functions characterizing treatment assignment among a group of $n$ potential observations.
% 		Fixing a collection of potential observations corresponding to a collection of feature vectors $\mathbf{X} \in  \mathcal{X}^{n}$ for the potential observations and $i \in [n]$, we impose that given $\left[\mathfrak{W}_{n}(\mathbf{X})\right]_{i} = \left[\mathfrak{W}_{n}^{\prime}(\mathbf{X})\right]_{i}$, the following holds.
% 		\begin{equation}
% 			\begin{aligned}
% 				Y_{i} = & \1\left(\left[\mathfrak{W}_{n}(\mathbf{X})\right]_{i} = 0\right)\mu_{0}^{0}(\mathbf{X}_i)
% 				+ \1\left(\left[\mathfrak{W}_{n}(\mathbf{X})\right]_{i} = 1\right)\mu_{0}^{1}(\mathbf{X}_i)
% 				+ \epsilon_i \\
% 				%
% 				&\quad  = 
% 				\1\left(\left[\mathfrak{W}_{n}^{\prime}(\mathbf{X})\right]_{i} = 0\right)\mu_{0}^{0}(\mathbf{X}_i)
% 				+\1\left(\left[\mathfrak{W}_{n}^{\prime}(\mathbf{X})\right]_{i} = 1\right)\mu_{0}^{1}(\mathbf{X}_i)
% 				+ \epsilon_i
% 				= Y_{i}^{\prime}
% 			\end{aligned}
% 		\end{equation}
% 	\end{asm}
% \end{boxD}
% Technically, since we are assuming i.i.d. observations in the characterization of the CATE setup, this is already implied.
% However, due to the importance of the SUTVA assumption in the treatment estimation literature, it seems appropriate to explicitly point out that it is implicitly assumed that the assumption holds.
% \begin{boxE}
%     \addtocounter{exmp}{-1}
%     \begin{exmp}[Average Hourly Earnings and IT Training - continued]\mbox{}\\*
%         {\color{red} LOREM IPSUM}
%     \end{exmp}    
% \end{boxE}
