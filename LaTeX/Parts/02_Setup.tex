\section{Setup}\label{sec:setup}
\hrule
Throughout this paper, we will consider two distinct setups focusing on separate but intertwined problems.
As a running example to give immediate economic meaning to the statistical problems, we consider the problem of evaluating a job-training program given only observational data in the style of .
\begin{boxE}
	\begin{exmp}[Average Earnings and Job-Training Program]\label{asm:run_exmp}\mbox{}\\*
        
        {\color{red} LOREM IPSUM}
        this DGP could describe the average hourly earnings $Y$ of a worker with characteristics $X$, say age $X_1$ and $X_2$ years of education.
        Next, consider the introduction of a job training program that potentially influences the average earnings of a worker with given characteristics.
        The evaluation of such programs given only observational data is a common econometric problem that is made difficult by the potentially endogenous choice of workers into treatment.
	\end{exmp}
\end{boxE}
To simplify the presentation, this running example ignores many widely discussed economic problems in this environment and instead focuses on a highly stylized problem with a limited set of characteristics.
The first statistical setup is a pure nonparametric regression setup that closely mirrors the structure of \citet{demirkaya_optimal_2024} and will be useful to illustrate the inner workings of the estimator of interest.
\begin{boxD}
	\begin{asm}[Nonparametric Regression DGP]\label{asm:npr_dgp}\mbox{}\\*
		The observed data consists of an i.i.d.\ sample taking the following form.
		\begin{equation}\label{DGP1}
			\mathbf{D}_n = \{Z_{i} = (X_{i}, Y_{i})\}_{i = 1}^{n}
			\quad \text{from the model} \quad
			Y = \mu(X) + \varepsilon,
		\end{equation}
		where $Y \in \mathbb{R}$ is the response, $X \in \mathcal{X} \subset \mathbb{R}^k$ is a feature vector of fixed dimension $k$ distributed according to a density function $f$ with associated probability measure $\varphi$ on $\mathcal{X}$, and $\mu(x)$ is the unknown mean regression function.
		$\varepsilon$ is the unobservable model error on which we impose the following conditions.
		\begin{equation}
			\E\left[\varepsilon \, \middle| \, X\right] = 0, \quad
			\Var\left(\varepsilon \, \middle| \, X = x\right) = \sigma_{\varepsilon}^2\left(x\right)
		\end{equation}
		Let the distribution induced by this model be denoted by $P$ and thus $Z_{i} = \left(X_{i}, Y_{i}\right) \overset{\text{iid}}{\sim} P$.
	\end{asm}
\end{boxD}
We will also consider a second statistical setting with more immediate econometric relevance: estimation of and inference on heterogeneous treatment effects in the potential outcomes framework.
This serves as a more immediately applicable version of the theoretical setup presented in \citet{ritzwoller_simultaneous_2024} and brings their results closer to practitioners in the field of economics.
\begin{boxD}
	\begin{asm}[Heterogeneous Treatment Effect DGP]\label{asm:CATE_dgp}\mbox{}\\*
		The observed data consists of an i.i.d. sample taking the following form.
		\begin{equation}\label{DGP2}
			\begin{aligned}
				\mathbf{D}_n & = \{Z_{i} = (X_{i}, W_{i}, Y_{i})\}_{i = 1}^{n}
				\quad \text{from the model} \quad
				Y = \1(W = 0)\mu_{0}(X) + \1(W = 1)\mu_1(X) + \varepsilon,	\\
				W_{i} & \sim \operatorname{Bern}\left(\pi\left(X_{i}\right)\right)
			\end{aligned}
		\end{equation}
		where $Y \in \mathbb{R}$ is the response and $W \in \{0,1\}$ is an observed treatment indicator.
		$X \in \mathcal{X} \subset \mathbb{R}^k$ is a vector of covariates of fixed dimension $k$ distributed according to a density function $f$ with an associated probability measure $\varphi$ on $\mathcal{X}$ and $\varepsilon$ is the unobservable model error on which we impose the following conditions.
		\begin{equation}
			\varepsilon \indep W \, | \, X, \quad
			\E\left[\varepsilon \, | \, X\right] = 0, \quad
			\Var\left(\varepsilon \, | \, X = x\right) = \sigma_{\varepsilon}^2\left(x\right)
		\end{equation}
		Furthermore, $\mu_0:\mathcal{X} \rightarrow \mathbb{R}$ and $\mu_1:\mathcal{X} \rightarrow \mathbb{R}$ are the two unknown potential outcome functions and $\pi:\mathcal{X} \rightarrow [0,1]$ is a function describing the probability of treatment uptake, effectively corresponding to the propensity score.
		Let the distribution induced by this model be denoted by $Q$ and thus $Z_{i} = \left(X_{i}, W_{i}, Y_{i}\right) \overset{\text{iid}}{\sim} Q$.
	\end{asm}
\end{boxD}
In this second setting, we will use the notation $\mathbf{D}^{(0)}$ and $\mathbf{D}^{(1)}$ to refer to the data subsets that contain only observations with $W = 0$ and $W = 1$, respectively.
Clearly, this model can be interpreted in the context of the potential outcomes framework in the usual way.

Throughout this paper, we will additionally rely on a number of assumptions that are more technical in nature.
\begin{boxD}
	\begin{asm}[Technical Assumptions]\label{asm:technical}\mbox{}\\*
		In both settings (Assumption~\ref{asm:npr_dgp} and Assumption~\ref{asm:CATE_dgp}) the following conditions hold:
		\begin{itemize}
			\item The feature space $\mathcal{X} = \operatorname{supp}(X)$ is a bounded, compact subset of $\mathbb{R}^k$
			\item The density $f(\cdot)$ is bounded away from 0 and $\infty$
			\item $f(\cdot)$ are $\mu(\cdot)$ are four times continuously differentiable with bounded second, third, and fourth-order partial derivatives in a neighborhood of $x$
		\end{itemize}
		In the Heterogeneous Treatment Effect setting (Assumption~\ref{asm:CATE_dgp}), the following additional condition holds:
		\begin{itemize}
			\item $\mu_0(\cdot)$ and $\mu_1(\cdot)$ are four times continuously differentiable with bounded second, third, and fourth-order partial derivatives in a neighborhood of $x$
		\end{itemize}
	\end{asm}
\end{boxD}
There is a potential to relax these assumptions at the cost of requiring both less interpretable conditions and more technically sophisticated proofs.
Additionally, we require a rather standard assumption in localized regression approaches, namely that the variance changes continuously.
\begin{boxD}
	\begin{asm}[Error Distribution Assumptions]\label{asm:errors}\mbox{}\\*
		The error terms $\varepsilon$ defined in Setup~\ref{asm:npr_dgp} and Setup~\ref{asm:CATE_dgp}, respectively, have continuously varying variance.
		In other words,$\sigma^2_{\varepsilon}: \mathcal{X} \rightarrow \mathbb{R}_{>0}$ is a continuous function.
	\end{asm}
\end{boxD}
As $\mathcal{X}$ is a compact and bounded set, this implies that there exists a $\overline{\sigma}_{\varepsilon}^2 > 0$ such that for any $x \in \mathcal{X}$ we have $\sigma^{2}_{\varepsilon}\left(x\right) \leq \overline{\sigma}_{\varepsilon}^2$.
Readers of \citet{demirkaya_optimal_2024} will recognize that this setup, in contrast to the original paper, allows for heteroskedasticity of the error terms.
This comes at basically no cost as the original proofs can be used nearly unchanged to prove the corresponding theorems on distributional approximations.
Additionally, due to the assumptions on the regression functions, this ensures the existence of seconds moments of $Y$ in both scenarios.
Furthermore, to ensure that there are a sufficient number of treated and untreated observations local to each point of interest asymptotically, we require the following condition on the treatment assignment and uptake mechanism.
\begin{boxD}
	\begin{asm}[Non-Trivial Treatment Overlap]\label{asm:treatment_overlap}\mbox{}\\*
		In the Heterogeneous Treatment Effect Setup (Assumption~\ref{asm:CATE_dgp}), we assume that there exists a constant $\mathfrak{p} \in (0, 1/2)$ such that
		\begin{equation}
			\forall x \in \mathcal{X}: \quad 
			0 < \mathfrak{p} \leq \pi\left(x\right) \leq 1 - \mathfrak{p} < 1.
		\end{equation}
	\end{asm}
\end{boxD}
This assumption seems rather strong when considering the full universe of potential treatment recipients.
In reality, we can constrain this assumption of overlap to neighborhoods of points of interest $x$.
As long as there is sufficient overlap in those neighborhoods the ideas of our identification strategy continue to hold locally.
\begin{boxE}
    \addtocounter{exmp}{-1}
    \begin{exmp}[Average Earnings and Job-Training Program - continued]\mbox{}\\*
        {\color{red} LOREM IPSUM}
    \end{exmp}    
\end{boxE}

\begin{boxD}
	\begin{asm}[Stable Unit Treatment Value Assumption (SUTVA)]\label{asm:sutva}\mbox{}\\*
		For any $n$, let $\mathfrak{W}_{n}: \mathcal{X}^{n} \rightarrow \{0,1\}^{n}$ and $\mathfrak{W}_{n}^{\prime}: \mathcal{X}^{n} \rightarrow \{0,1\}^{n}$ be two functions characterizing treatment assignment among a group of $n$ potential observations.
		Fixing a collection of potential observations corresponding to a collection of feature vectors $\mathbf{X} \in  \mathcal{X}^{n}$ for the potential observations and $i \in [n]$, we impose that given $\left[\mathfrak{W}_{n}(\mathbf{X})\right]_{i} = \left[\mathfrak{W}_{n}^{\prime}(\mathbf{X})\right]_{i}$, the following holds.
		\begin{equation}
			\begin{aligned}
				Y_{i} = & \1\left(\left[\mathfrak{W}_{n}(\mathbf{X})\right]_{i} = 0\right)\mu_{0}(\mathbf{X}_i)
				+ \1\left(\left[\mathfrak{W}_{n}(\mathbf{X})\right]_{i} = 1\right)\mu_{1}(\mathbf{X}_i)
				+ \epsilon_i \\
				%
				&\quad  = 
				\1\left(\left[\mathfrak{W}_{n}^{\prime}(\mathbf{X})\right]_{i} = 0\right)\mu_{0}(\mathbf{X}_i)
				+\1\left(\left[\mathfrak{W}_{n}^{\prime}(\mathbf{X})\right]_{i} = 1\right)\mu_{1}(\mathbf{X}_i)
				+ \epsilon_i
				= Y_{i}^{\prime}
			\end{aligned}
		\end{equation}
	\end{asm}
\end{boxD}
Technically, since we are assuming i.i.d. observations in the characterization of the CATE setup, this is already implied.
However, due to the importance of the SUTVA assumption in the treatment estimation literature, it seems appropriate to explicitly point out that it is implicitly assumed that the assumption holds.
\begin{boxE}
    \addtocounter{exmp}{-1}
    \begin{exmp}[Average Earnings and Job-Training Program - continued]\mbox{}\\*
        {\color{red} LOREM IPSUM}
    \end{exmp}    
\end{boxE}
