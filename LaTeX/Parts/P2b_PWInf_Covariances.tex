\subsection{NPR - Kernel Variances \& Covariances}
\hrule
Similar to the previous section of proofs, we will continue by analyzing the variances and covariances of the kernels under consideration.
These results will play an important role in the derivation of consistency properties for the variance estimators.
Similar to the previous part, we will first consider the nonparametric regression setup and then proceed to the conditional average treatment effect setup.
\vspace{0.5cm}
\hrule
\begin{lem}[Adapted from \citet{demirkaya_optimal_2024}]\label{lem:omega_s}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s}\}$ be a vector of i.i.d. random variables drawn from $P$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}\left(x\right)
		= \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}\left(x\right)
		= \E_{1}\left[\left(\mu\left(X_1\right)+ \varepsilon_1\right)^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]
		\lesssim \mu^2(x) + \overline{\sigma}_{\varepsilon}^2 + o(1)
		\quad \text{as} \quad s \rightarrow \infty.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:omega_s}]\mbox{}\\*
	\begin{equation}
		\begin{aligned}
			\Omega_{s}\left(x\right)
			 & = \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right]
			= \E_{D}\left[\left(\sum_{i = 1}^{s}\kappa\left(x; Z_{i}, D\right)Y_{i}\right)^2\right]
			= \E_{D}\left[\sum_{i = 1}^{s}\sum_{j = 1}^{s}\left(\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D\right)Y_{i}Y_{j}\right)\right] \\
			%
			 & = \E_{D}\left[s \kappa\left(x; Z_{1}, D\right)Y_{1}^2\right]
			= \E_{1}\left[\left(\mu\left(X_1\right) + \varepsilon_{1}\right)^2 s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]                   \\
			%
			 & = \underbrace{\E_{1}\left[\mu^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]}_{(A)}
			+ 2\underbrace{\E_{1}\left[\varepsilon_1\mu\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]}_{(B)}
			+ \underbrace{\E_{1}\left[\varepsilon_1^{2} s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]}_{(C)}
		\end{aligned}
	\end{equation}
	Using Lemma \ref{lem:dem13}, we can now make the following observation concerning the first term.
	\begin{equation}
		(A)
		= \E_{1}\left[\mu^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]
		\rightarrow \mu^2\left(x\right)
	\end{equation}
	We can approach the second term using the law of iterated expectation to obtain the following.
	\begin{equation}
		\begin{aligned}
			(B)
			 & = \E_{1}\left[\varepsilon_1\mu\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]
			= \E_{1}\left[\E\left[\varepsilon_1 \, | \, X_1 \right] \cdot \mu\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]
			= 0
		\end{aligned}
	\end{equation}
	An analogous application to the third term gives us the following.
	\begin{equation}
		\begin{aligned}
			(C)
			 & = \E_{1}\left[\varepsilon_1^{2} s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]
			= \E_{1}\left[\E\left[\varepsilon_1^{2} \, | \, X_1\right] s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right] \\
			%
			 & = \E_{1}\left[\sigma^2_{\varepsilon}\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]
			\rightarrow \sigma^2_{\varepsilon}\left(x\right)
			\leq \overline{\sigma}^2_{\varepsilon}
		\end{aligned}
	\end{equation}
	Thus, we obtain the desired result.
	Going forward, we will rely on some of the steps shown in this proof implicitly to shorten the presentation of arguments.
\end{proof}

\newpage
\begin{lem}\label{lem:omega_sc}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s}\}$ be a vector of i.i.d. random variables drawn from $P$.
	Let $D^{\prime} = \{Z_1, \dotsc, Z_{c}, Z_{c+1}^{\prime}, \dotsc,  Z_{s}^{\prime}\}$ where $Z_{c+1}^{\prime}, \dotsc,  Z_{s}^{\prime}$ are i.i.d. draws from $P$ that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		= \E\left[h_{s}\left(x; Z_1, \ldots, Z_{c}, Z_{c+1}, \ldots, Z_{s}\right) \cdot
			h_{s}\left(x; Z_1, \ldots,Z_{c}, Z_{c+1}^{\prime}, \ldots, Z_{s}^{\prime}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		\lesssim \frac{s^2 + cs  - c^2}{s^2} \mu^2(x) + (c/s) \overline{\sigma}_{\varepsilon}^2 + o(1)
		\quad \text{for} \quad s \quad \text{sufficiently large}
	\end{equation}
	and thus
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		\lesssim \mu^2(x) + \overline{\sigma}_{\varepsilon}^2 + o(1)
		\quad \text{as} \quad s \rightarrow \infty.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:omega_sc}]
	\begin{equation}
		\begin{aligned}
			\Omega_{s}^{c}\left(x\right)
			 & = \E\left[h_{s}\left(x; Z_1, \ldots, Z_{c}, Z_{c+1}, \ldots, Z_{s}\right) \cdot
			h_{s}\left(x; Z_1, \ldots,Z_{c}, Z_{c+1}^{\prime}, \ldots, Z_{s}^{\prime}\right)\right]                                                                \\
			%
			 & = \E_{D, D^{\prime}}\left[
				\left(\sum_{i = 1}^{s}\kappa\left(x; Z_{i}, D\right)Y_{i}\right)
				\left(\sum_{j = 1}^{c}\kappa\left(x; Z_{j}, D^{\prime}\right)Y_{j}
				+ \sum_{j = c+1}^{s}\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right)Y_{j}^{\prime}\right)
			\right]                                                                                                                                                                                             \\
			%
			 & = \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D^{\prime}\right)Y_{i}Y_{j}\right]
			+  \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = c+1}^{s}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right)Y_{i}Y_{j}^{\prime}\right]   \\
			 & \quad + \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s}\sum_{j = 1}^{c}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D^{\prime}\right)Y_{i}Y_{j}\right]
			+  \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s}\sum_{j = c+1}^{s}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right)Y_{i}Y_{j}^{\prime}\right] \\
			%
			 & = \underbrace{\E_{D, D^{\prime}}\left[c \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{1}^{2}\right]}_{(A)}
			+ \underbrace{\E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{1}Y_{c+1}^{\prime}\right]}_{(B)}                           \\
			 & \quad + \underbrace{\E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{c+1}Y_{1}\right]}_{(C)}                                    \\
			 & \quad + \underbrace{\E_{D, D^{\prime}}\left[(s-c)^2 \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{c+1}Y_{c+1}^{\prime}\right]}_{(D)}
		\end{aligned}
	\end{equation}
	Starting from this decomposition, we will analyze the terms one by one.
	First, by honesty and an application of Lemma \ref{lem:dem12}, we find the following.
	\begin{equation}
		\begin{aligned}
            (A) & = 
			\E_{D, D^{\prime}}\left[c\kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{1}^{2}\right]
			= \E_{D, D^{\prime}}\left[c\kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)\left(\mu^2(X_1) + \sigma_{\varepsilon}^{2}\right)\right]                                 \\
			%
			 & = \E_{1}\left[\left(\mu^2(X_1) + \sigma_{\varepsilon}^{2}(X_1)\right) c\E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)\right]\right] \\
			%
			 & = \E_{1}\left[\left(\mu^2(X_1) + \sigma_{\varepsilon}^{2}(X_1)\right) c\left\{1 - \varphi\left(B\left(x, \|X_1 - x\|\right)\right)\right\}^{2s-c-1}\right]                        \\
			%
			 & \leq (c/s) \E_{1}\left[\left(\mu^2(X_1) + \sigma_{\varepsilon}^{2}(X_1)\right) s\left\{1 - \varphi\left(B\left(x, \|X_1 - x\|\right)\right)\right\}^{s-1}\right]                  \\
			%
			 & \lesssim (c/s)\left(\mu^2(x) + \sigma_{\varepsilon}(x)\right) + o(1)
			\lesssim (c/s)\left(\mu^2(x) + \overline{\sigma}_{\varepsilon}\right) + o(1)
		\end{aligned}
	\end{equation}
	Similarly, we can find that:
	\begin{equation}
		\begin{aligned}
            (B)
			 & = \E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{1}Y_{c+1}^{\prime}\right]                                          \\
			%
			 & = \E_{D, D^{\prime}}\left[\mu(X_1)\mu(X_{c+1}^{\prime}) \, c(s-c) \, \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right] \\
			%
			 & \leq \E_{D}\left[|\mu(X_1)| \, c \, \kappa\left(x; Z_{1}, D\right)\right]
			\E_{D^{\prime}}\left[|\mu(X_{c+1}^{\prime})| \, (s-c) \, \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                               \\
			% 
			 & = \frac{c (s-c)}{s^2} \leq \E_{D}\left[|\mu(X_1)| \, s \, \kappa\left(x; Z_{1}, D\right)\right]
			\E_{D^{\prime}}\left[|\mu(X_{c+1}^{\prime})| \, s \, \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                   \\
			%
			 & = \frac{c (s-c)}{s^2} \left(\E_{D}\left[|\mu(X_1)| \, s \, \kappa\left(x; Z_{1}, D\right)\right]\right)^2                                                                       
			= \frac{c (s-c)}{s^2} \left(\E_{1}\left[|\mu(X_1)| s\left\{1 - \varphi\left(B\left(x, \|X_1 - x\|\right)\right)\right\}^{s-1}\right]\right)^2                          \\
			%
			 & \lesssim \frac{c(s-c)}{s^2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	Following analogous steps, we find the same result for the third term.
	\begin{equation}
		\begin{aligned}
			(C)
            = \E_{D, D^{\prime}}\left[c(s-c) \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)Y_{c+1}Y_{1}\right]
			\lesssim \frac{c(s-c)}{s^2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	The fourth term can be asymptotically bounded in the following way.
	\begin{equation}
		\begin{aligned}
			(D)
            & = \E_{D, D^{\prime}}\left[(s-c)^2 \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)Y_{c+1}Y_{c+1}^{\prime}\right]                                           \\
			%
			 & = \E_{D, D^{\prime}}\left[\mu(X_{c+1})\mu(X_{c+1}^{\prime})\, (s-c)^2 \, \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right] \\
			%
			 & \leq \E_{D}\left[|\mu(X_{c+1})|\, (s-c) \, \kappa\left(x; Z_{c+1}, D\right)\right]
			\E_{D^{\prime}}\left[|\mu(X_{c+1}^{\prime})|\, (s-c) \, \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                      \\
			%
			 & = \frac{(s-c)^2}{s^2} \E_{D}\left[|\mu(X_{c+1})|\, s \, \kappa\left(x; Z_{c+1}, D\right)\right]
			\E_{D^{\prime}}\left[|\mu(X_{c+1}^{\prime})|\, s \, \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)\right]                                                                                          \\
			%
			 & \quad = \frac{(s-c)^2}{s^2} \left(\E_{D}\left[|\mu(X_{c+1})|\, s \, \kappa\left(x; Z_{c+1}, D\right)\right]\right)^2                                                                        
			\lesssim \frac{(s-c)^2}{s^2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	The result of Lemma \ref{lem:omega_sc} follows immediately by summing up the asymptotic bounds for the individual terms.
\end{proof}

\newpage
\begin{lem}\label{lem:upsilon_s}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s_2}\}$ be a vector of i.i.d. random variables drawn from $P$ for $s_2 > s_1$.
	Furthermore, let
	\begin{equation}
		\Upsilon_{s_1, s_2}\left(x\right)
		= \E\left[h_{s_1}\left(x; Z_1, \ldots,  Z_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots,Z_{s_1}, \ldots, Z_{s_2}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Upsilon_{s_1, s_2}\left(x\right)
		\lesssim \mu^{2}\left(x\right) + \overline{\sigma}^2_{\varepsilon} + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:upsilon_s}]
	\begin{equation}
		\begin{aligned}
			\Upsilon_{s_1, s_2}\left(x\right)
			 & = \E\left[h_{s_1}\left(x; Z_1, \ldots,  Z_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots,Z_{s_1}, \ldots, Z_{s_2}\right)\right]                                                                             \\
			%
			 & = \E_{D}\left[
				\left(\sum_{i = 1}^{s_1} \kappa(x; Z_{i}, D_{[s_1]})Y_{i}\right)
				\left(\sum_{j = 1}^{s_1}\kappa(x; Z_j, D)Y_j + \sum_{j = s_1 + 1}^{s_2}\kappa(x; Z_j, D)Y_j\right)
			\right]                                                                                                                                                                            \\
			%
			 & = \E_{D}\left[\sum_{i = 1}^{s_1} \kappa(x; Z_{i}, D) Y_{i}^2\right]
			+ \E_{D}\left[\sum_{i = 1}^{s_1}\sum_{j = s_1 + 1}^{s_2}\kappa(x; Z_{i}, D_{[s_1]})\kappa(x; Z_j, D) Y_{i} Y_j\right]                              \\
			%
			 & = \E_{D}\left[Y_1^2 \, s_1 \, \kappa(x; Z_1, D)\right]
			+ \E_{D}\left[Y_{1} Y_{s_2} \, s_1 (s_2 - s_1) \, \kappa(x; Z_1, D_{[s_1]})\kappa(x; Z_{s_2}, D)\right]                                        \\
			%
			 & = \E_{D}\left[\left(\mu^2(X_1) + \sigma^2_{\varepsilon}(X_1)\right) \, s_1 \, \kappa(x; Z_1, D)\right]
			+ \E_{D}\left[\mu(X_1) \mu(X_{s_2}) \, s_1 (s_2 - s_1) \, \kappa(x; Z_1, D_{[s_1]})\kappa(x; Z_{s_2}, D)\right]              \\
			%
			 & = \frac{s_1}{s_2}\E_{D}\left[\left(\mu^2(X_1) + \sigma^2_{\varepsilon}(X_1)\right) \, s_1 \, \kappa(x; Z_1, D)\right]
			+ \frac{s_2 - s_1}{s_2}\E_{D}\left[\mu(X_1) \mu(X_{s_2}) \, s_1 s_2 \, \kappa(x; Z_1, D_{[s_1]})\kappa(x; Z_{s_2}, D)\right] \\
			%
			 & \leq \frac{s_1}{s_2} \E_{D}\left[\left(\mu^2(X_1) + \sigma^2_{\varepsilon}(X_1)\right) \, s_2 \, \kappa(x; Z_1, D)\right]                   \\
			 & \quad \quad + \frac{s_2 - s_1}{s_2}\E_{D}\left[|\mu(X_1)| \, s_1 \, \kappa(x; Z_1, D_{[s_1]})\right]
			\E_{D}\left[|\mu(X_{s_2})| \, s_2 \, \kappa(x; Z_{s_2}, D)\right]                                                                                       \\
			%
			 & \lesssim \mu^{2}\left(x\right) + \sigma^2_{\varepsilon}(x) + o(1)
			\leq \mu^{2}\left(x\right) + \overline{\sigma}^2_{\varepsilon} + o(1).
		\end{aligned}
	\end{equation}
\end{proof}

\newpage
\begin{lem}\label{lem:upsilon_sc}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s_2}\}$ be a vector of i.i.d. random variables drawn from $P$ for $s_2 > s_1$.
	Let $D^{\prime} = \{Z_1, \dotsc, Z_{c}, Z_{c+1}^{\prime}, \dotsc,  Z_{s_1}^{\prime}\}$ where $Z_{c+1}^{\prime}, \dotsc,  Z_{s_1}^{\prime}$ are i.i.d. draws from $P$ that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Upsilon_{s_1, s_2}^{c}\left(x\right)
		= \E\left[h_{s_1}\left(x; Z_1, \ldots, Z_c, Z^{\prime}_{c+1}, \ldots,  Z^{\prime}_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots, Z_{s_2}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\begin{aligned}
			 & \Upsilon_{s_1, s_2}^{c}\left(x\right)
			\lesssim \frac{c s_2 - c^2 + s_1 s_2}{s_1 s_2}\mu^2(x) + (c/s_1) \overline{\sigma}^2_{\varepsilon} + o(1) \\
			%
			 & \text{for} \quad s_1, s_2 \quad \text{sufficiently large}
			\quad \text{with} \quad
			0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1
		\end{aligned}
	\end{equation}
	and thus
	\begin{equation}
		\Upsilon_{s_1, s_2}^{c}\left(x\right)
		\lesssim \mu^2(x) + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:upsilon_sc}]
	\begin{equation}
		\begin{aligned}
			\Upsilon_{s_1, s_2}^{c}\left(x\right)
			 & = \E\left[h_{s_1}\left(x; Z_1, \ldots, Z_c, Z^{\prime}_{c+1}, \ldots,  Z^{\prime}_{s_1}\right) \cdot
			h_{s_2}\left(x; Z_1, \ldots, Z_{s_2}\right)\right]                                                                                                                       \\
			%                                                                     
			 & = \E_{D, D^{\prime}}\left[
				\left(\sum_{i = 1}^{c} \kappa(x; Z_{i}, D^{\prime})Y_{i} + \sum_{i = c+1}^{s_1} \kappa(x; Z_{i}^{\prime}, D^{\prime})Y_{i}^{\prime}\right)
				\left(\sum_{j = 1}^{c} \kappa(x; Z_j, D)Y_j + \sum_{j = c+1}^{s_2}\kappa(x; Z_j, D)Y_j \right)
			\right]                                                                                                                                                                                             \\
			%
			 & = \underbrace{\E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c} \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_j, D)Y_{i} Y_j\right]}_{(A)}
			+ \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c}\kappa(x; Z_{i}, D^{\prime}) Y_{i}\right) \left(\sum_{j = c+1}^{s_2} \kappa(x; Z_j, D) Y_j\right)\right]}_{(B)} \\
			 & \quad + \underbrace{\E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s_1}\sum_{j = 1}^{c} \kappa(x; Z_{i}^{\prime}, D^{\prime})\kappa(x; Z_j, D)Y_{i}^{\prime} Y_j\right]}_{(C)}
			+ \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = c+1}^{s_1}\kappa(x; Z_{i}^{\prime}, D^{\prime}) Y_{i}^{\prime}\right)
					\left(\sum_{j = c+1}^{s_2} \kappa(x; Z_j, D) Y_j\right)\right]}_{(D)}
		\end{aligned}
	\end{equation}
	Again, we have four terms to analyze individually.
	\begin{equation}
		\begin{aligned}
			(A)
			 & = \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = 1}^{c} \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_j, D)Y_{i} Y_j\right]
			= \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c} Y_{i}^2 \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_{i}, D)\right]                                                                          \\
			%
			 & = \E_{D, D^{\prime}}\left[Y_{1}^2 \, c \, \kappa(x; Z_1, D^{\prime})\kappa(x; Z_1, D)\right]
			\E_{D, D^{\prime}}\left[\left(\mu^2(X_{1}) + \sigma^2_{\varepsilon}(X_{1})\right) \, c \, \kappa(x; Z_1, D_{[c]})\kappa(x; Z_1, D^{\prime}_{c+1:s_1})\right] \\
			%
			 & = \E_{D}\left[\left(\mu^2(X_{1}) + \sigma^2_{\varepsilon}(X_{1})\right) \, c \, \kappa(x; Z_1, D)\right]
			= \frac{c}{s_1} \E_{D}\left[\left(\mu^2(X_{1}) + \sigma^2_{\varepsilon}(X_{1})\right) \, s_1 \, \kappa(x; Z_1, D)\right]                                               \\
			%
			 & \lesssim (c/s_1)(\mu^2(x) + \sigma^2_{\varepsilon}(x)) + o(1)
			\leq (c/s_1)(\mu^2(x) + \overline{\sigma}^2_{\varepsilon}) + o(1)
		\end{aligned}
	\end{equation}
	Considering the second term, we find the following.
	\begin{equation}
		\begin{aligned}
			(B)
			 & = \E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c}\kappa(x; Z_{i}, D^{\prime}) Y_{i}\right)\left(\sum_{j = c+1}^{s_2} \kappa(x; Z_j, D) Y_j\right)\right]
			= \E_{D, D^{\prime}}\left[\sum_{i = 1}^{c}\sum_{j = c+1}^{s_2}Y_{i} Y_j \kappa(x; Z_{i}, D^{\prime})\kappa(x; Z_j, D)\right]                               \\
			%
			 & = \E_{D, D^{\prime}}\left[c(s_2 - c) \, Y_1 Y_{s_1} \kappa(x; Z_1, D^{\prime})\kappa(x; Z_{s_2}, D)\right]
			= \frac{c (s_2 - c)}{s_1 s_2}\E_{D, D^{\prime}}\left[Y_1 Y_{s_2} \, s_1 s_2 \,\kappa(x; Z_1, D^{\prime})\kappa(x; Z_{s_2}, D)\right]                   \\
			%
			 & \leq \frac{c (s_2 - c)}{s_1 s_2}
			\E_{D^{\prime}}\left[|\mu(X_1)| \, s_1  \,\kappa(x; Z_1, D^{\prime})\right]
			\E_{D}\left[|\mu(X_{s_2})| \, s_2  \,\kappa(x; Z_{s_2}, D)\right]                                                                                       \\
			%
			 & \lesssim \frac{c(s_2 - c)}{s_1 s_2} \mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	Similarly, by simplifying the third term, we find the following.
	\begin{equation}
		\begin{aligned}
			(C)
			 & = \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s_1}\sum_{j = 1}^{c} \kappa(x; Z_{i}^{\prime}, D^{\prime})\kappa(x; Z_j, D)Y_{i}^{\prime} Y_j\right]
			= \E_{D, D^{\prime}}\left[Y_{s_1}^{\prime} Y_{1} \, (s_1 - c)c \, \kappa(x; Z_{s_1}^{\prime}, D^{\prime})\kappa(x; Z_1, D)\right]                                                    \\
			% 
			 & = \frac{(s_1 - c)c}{s_1 s_2}\E_{D, D^{\prime}}\left[\mu(X_{s_1}^{\prime})\mu(X_1) \, s_1 s_2 \, \kappa(x; Z_{s_1}^{\prime}, D^{\prime})\kappa(x; Z_1, D)\right] \\
			%
			 & \leq \frac{(s_1 - c)c}{s_1 s_2}
			\E_{D}\left[|\mu(X_{s_1}^{\prime})| \, s_1 \, \kappa(x; Z_{s_1}^{\prime}, D^{\prime})\right]
			\E_{D}\left[|\mu(X_1)| \, s_2 \, \kappa(x; Z_1, D)\right]                                                                                                                             \\
			%
			 & \lesssim \frac{(s_1 - c)c}{s_1 s_2} \mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
	Lastly, concerning the fourth term, observe the following.
	\begin{equation}
		\begin{aligned}
			(D)
			 & = \E_{D, D^{\prime}}\left[\left(\sum_{i = c+1}^{s_1}\kappa(x; Z_{i}^{\prime}, D^{\prime}) Y_{i}^{\prime}\right)
				\left(\sum_{j = c+1}^{s_2} \kappa(x; Z_j, D) Y_j\right)\right]
			= \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s_1}\sum_{j = c+1}^{s_2}\kappa(x; Z_{i}^{\prime}, D^{\prime}) \kappa(x; Z_j, D)  Y_{i}^{\prime}Y_j\right]                                                        \\
			%
			 & = \E_{D, D^{\prime}}\left[\mu(X_{s_1}^{\prime}) \mu(X_{s_2}) \, (s_1 - c)(s_2 -c) \,\kappa(x; Z_{s_1}^{\prime}, D^{\prime}) \kappa(x; Z_{s_2}, D)  \right]                        \\                                                                                                                                                                       \\
			%
			 & = \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}\E_{D, D^{\prime}}\left[\mu(X_{s_1}^{\prime}) \mu(X_{s_2}) \, s_1 s_2 \,\kappa(x; Z_{s_1}^{\prime}, D^{\prime}) \kappa(x; Z_{s_2}, D)  \right] \\                                                                                                                                                                       \\
			%
			 & \leq \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}
			\E_{D^{\prime}}\left[|\mu(X_{s_1}^{\prime})| \, s_1 \,\kappa(x; Z_{s_1}^{\prime}, D^{\prime})   \right]
			\E_{D}\left[ |\mu(X_{s_2})| \, s_2 \, \kappa(x; Z_{s_2}, D)  \right]                                                                                                                                    \\
			%
			 & \lesssim \frac{(s_1 - c)(s_2 -c)}{s_1 s_2}\mu^2(x) + o(1)
		\end{aligned}
	\end{equation}
\end{proof}

\newpage
\begin{lem}[Kernel Variance of the TDNN Kernel]\label{lem:Var_TDNN_k}
	For the kernel of the TDNN estimator with subsampling scales $s_1$ and $s_2$, it holds that
	\begin{equation}
		\zeta_{s_1, s_2}^{s_2}\left(x\right)
		\lesssim \mu^2(x) + \overline{\sigma}_{\varepsilon} + o(1)
		\quad \text{as} \quad s_1, s_2 \rightarrow \infty
		\quad \text{with} \quad
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1.
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:Var_TDNN_k}]
	Consider first the following decomposition.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_1, s_2}^{s_2}\left(x\right)
			 & = \Var\left(h_{s_1, s_2}\left(x; Z_1, \ldots, Z_{s_2}\right)\right)
			= \Var_{D}\left(h_{s_1, s_2}\left(x; D\right)\right)                                     \\
			%
			 & \leq \E_{D}\left[h_{s_1, s_2}^{2}\left(x; D\right)\right]
			= \E_{D}\left[
				\left(w_{1}^{*}\tilde{\mu}_{s_1}\left(x; D\right) + w_{2}^{*} h_{s_2}\left(x; D\right)\right)^2
			\right]                                                                                           \\
			%
			 & = \left(w_{1}^{*}\right)^2\E_{D}\left[\tilde{\mu}_{s_1}^{2}\left(x; D\right)\right]
			+ 2 w_{1}^{*}w_{2}^{*} \E_{D}\left[\tilde{\mu}_{s_1}\left(x; D\right) h_{s_2}\left(x; D\right)\right]
			+ \left(w_{2}^{*}\right)^2\Omega_{s_2}
		\end{aligned}
	\end{equation}

	Then, observe the following.
	\begin{equation}
		\begin{aligned}
			\E_{D}\left[\tilde{\mu}_{s_1}^{2}\left(x; D\right)\right]
			 & = \E_D\left[\left(\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(x; D_{\ell}\right)\right)^2\right]
			= \binom{s_2}{s_1}^{-2} \E_{D}\left[\sum_{\iota, \iota' \in L_{s_2, s_1}}h_{s_1}\left(x; D_{\iota}\right)h_{s_1}\left(x; D_{\iota'}\right)\right] \\
			%
			 & = \binom{s_2}{s_1}^{-2} \sum_{c = 0}^{s_1} \binom{s_2}{s_1}\binom{s_1}{c}\binom{s_2 - s_1}{s_1 - c} \Omega_{s_1}^{c}
			= \binom{s_2}{s_1}^{-1} \sum_{c = 0}^{s_1} \binom{s_1}{c}\binom{s_2 - s_1}{s_1 - c} \Omega_{s_1}^{c}                                                                \\
			%
			 & \lesssim \Omega_{s_1}
			\lesssim \mu(x)^2 + \sigma_{\varepsilon}^2 + o(1)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}

	Recall that by Lemma \ref{lem:omega_s}, we have the following.
	\begin{equation}
		\begin{aligned}
			\Omega_{s_2}
			\lesssim \mu(x)^2 + \sigma_{\varepsilon}^2 + o(1)
			\quad \text{as} \quad s \rightarrow \infty
		\end{aligned}
	\end{equation}

	Lastly, consider the following.
	\begin{equation}
		\begin{aligned}
			\E_{D}\left[\tilde{\mu}_{s_1}\left(x; D\right) h_{s_2}\left(x; D\right)\right]
			 & = \E_D\left[\binom{s_2}{s_1}^{-1}\sum_{\ell \in L_{s_2, s_1}} h_{s_1}\left(x; D_{\ell}\right)h_{s_2}\left(x; D\right)\right] \\
			%
			 & = \E_D\left[h_{s_1}\left(x; D_{[s_1]}\right)h_{s_2}\left(x; D\right)\right]
			= \Upsilon_{s_1, s_2}\left(x\right)
		\end{aligned}
	\end{equation}

	Thus, we find the following.
	\begin{equation}
		\begin{aligned}
			\zeta_{s_2, s_2}\left(x\right)
			 & \lesssim \left(w_{1}^{*}\right)^2 \Omega_{s_1}
			+ 2 w_{1}^{*}w_{2}^{*} \Upsilon_{s_1, s_2}\left(x\right)
			+ \left(w_{1}^{*}\right)^2 \Omega_{s_2}                                                                       \\
			%
			 & \lesssim \left(w_{1}^{*} + w_{2}^{*}\right)^2 \left(\mu^2(x) + \sigma_{\varepsilon}\right) + o(1)
			= \mu^2(x) + \sigma_{\varepsilon} + o(1).
		\end{aligned}
	\end{equation}
\end{proof}

\newpage

\begin{lem}[Lemma 10 - \citet{demirkaya_optimal_2024}]\label{lem:dem10}
	For the kernel of the TDNN estimator with subsampling scales $s_1$ and $s_2$ satisfying
	\begin{equation}
		0 < \mathfrak{c} \leq s_1 / s_2 \leq 1 - \mathfrak{c} < 1
		\quad \text{and} \quad
		s_2 = o(n),
	\end{equation}
	it holds that
	\begin{equation}
		\zeta_{s_1, s_2}^{1}\left(x\right)
		\sim s_2^{-1}.
	\end{equation}
\end{lem}

\newpage
\subsection{CATE - Kernel Variances \& Covariances}
\hrule
Next, we will continue by showing analogous properties in the CATE setting.
Similar to before, we will start under the assumption that the functional nuisance parameters are known a priori, to then show that the estimation of said parameters does not impact the asymptotic behavior of the estimator.
\vspace{0.5cm}
\hrule

\begin{lem}\label{lem:CATE_omega_s}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s}\}$ be a vector of i.i.d. random variables generated by the setup shown in Assumption \ref{asm:hte_dgp}.
	Furthermore, let
	\begin{equation}
		\Omega_{s}\left(x\right)
		= \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}\left(x\right)
		\lesssim \left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2 + \frac{\overline{\sigma}^2_{\varepsilon}}{\mathfrak{p}\left(1 - \mathfrak{p}\right)} + o(1)
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:CATE_omega_s}]\mbox{}\\*
    First, notice that we can decompose the quantity of interest in the following way.
	\begin{equation}
		\begin{aligned}
			\Omega_{s}\left(x\right)
			 & = \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right]
             = \E_{D}\left[\left(\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right) + \beta\left(W_{i}, X_{i}\right)\left(Y_{i} - \mu_{W_{i}}\left(X_{i}\right)\right)\right)\right)^2\right]\\
             %
             & = \E_{D}\Bigg[\sum_{i = 1}^{s}\sum_{j = 1}^{s}\left[\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_j, D\right) \right.
             \left(\mu_{1}\left(X_{i}\right) - \mu_{0}\left(X_{i}\right) + \beta\left(W_{i}, X_{i}\right)\left(Y_{i} - \mu_{W_{i}}\left(X_{i}\right)\right)\right)\\
             & \quad \quad \quad \left. \left(\mu_{1}\left(X_j\right) - \mu_{0}\left(X_j\right) + \beta\left(W_j, X_j\right)\left(Y_{j} - \mu_{W_j}\left(X_j\right)\right)\right)
             \right]\Bigg] \\
             %
             & = \E_{D}\left[s \kappa\left(x; Z_1, D\right) \left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\left(Y_{1} - \mu_{W_{1}}\left(X_{i}\right)\right)\right)^2\right]\\
             %
             & = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right) + \beta\left(W_{1}, X_1\right)\varepsilon_1\right)^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]\\
             %
             & = \underbrace{\E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right)\right)^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]}_{(A)}
             + 2 \underbrace{\E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right)\right)\beta\left(W_{1}, X_1\right)\varepsilon_1 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]}_{(B)}\\
             & \quad + \underbrace{\E_{1}\left[\beta^2\left(W_{1}, X_1\right)\varepsilon_1^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]}_{(C)}
		\end{aligned}
	\end{equation}
    This allows us to consider the three components individually.
    \begin{equation}
        \begin{aligned}
            (A) 
            & = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right)\right)^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]
            \longrightarrow \left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2
            \quad \text{as} \quad s \rightarrow \infty
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            (B)
            & = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right)\right)\beta\left(W_{1}, X_1\right)\varepsilon_1 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\
            %
            & = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right)\right) \cdot \E\left[\beta\left(W_{1}, X_1\right)\varepsilon_1 \; \middle| \; X_1\right] s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\
            %
            & = \E_{1}\left[\left(\mu_{1}\left(X_1\right) - \mu_{0}\left(X_1\right)\right) \cdot 0 \cdot s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] 
            = 0
        \end{aligned}
    \end{equation}
    \newpage
    \begin{equation}
        \begin{aligned}
            (C)
            & = \E_{1}\left[\beta^2\left(W_{1}, X_1\right)\varepsilon_1^2 s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] 
            = \E_{1}\left[\E\left[\beta^2\left(W_{1}, X_1\right)\varepsilon_1^2 \; \middle| \; X_1\right] s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\
            %
            & = \E_{1}\left[\E\left[\left(\frac{W_{1}}{\pi(X_1)} - \frac{1 - W_{1}}{1 - \pi(X_1)}\right)^2 \; \middle| \; X_1\right] \sigma_{\varepsilon}^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\
            %
            & = \E_{1}\left[\E\left[\left(\frac{W_{1}}{\pi(X_1)}\right)^2 - 2 \frac{(1 - W_{1})W_{1}}{\pi(X_1)(1 - \pi(X_1))} + \left(\frac{1 - W_{1}}{\pi(X_1)}\right)^2 \; \middle| \; X_1\right] \sigma_{\varepsilon}^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\ 
            %
            & = \E_{1}\left[\left(\frac{\E\left[W_{1}^2 \; \middle| \; X_1\right]}{\pi^2(X_1)} 
            - 2 \underbrace{\frac{\E\left[(1 - W_{1})W_{1}\; \middle| \; X_1\right]}{\pi(X_1)(1 - \pi(X_1))}}_{ = 0} 
            + \frac{\E\left[(1 - W_{1})^2\; \middle| \; X_1\right]}{(1 - \pi(X_1))^2}\right)
             \sigma_{\varepsilon}^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\ 
            %
            & = \E_{1}\left[\left(\frac{\E\left[W_{1} \; \middle| \; X_1\right]}{\pi^2(X_1)} 
            + \frac{\E\left[1 - W_{1}\; \middle| \; X_1\right]}{(1 - \pi(X_1))^2}\right)
             \sigma_{\varepsilon}^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\
             %
            & = \E_{1}\left[\left(\frac{1}{\pi(X_1)} + \frac{1}{1 - \pi(X_1)}\right)
             \sigma_{\varepsilon}^2\left(X_1\right) s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right] \\
             %
            & = \E_{1}\left[\frac{\sigma_{\varepsilon}^2\left(X_1\right)}{\pi(X_1)\left(1 - \pi(X_1)\right)} s \E_{2:s}\left[\kappa\left(x; Z_1, D\right)\right]\right]
            \longrightarrow \frac{\sigma_{\varepsilon}^2\left(x\right)}{\pi(x)\left(1 - \pi(x)\right)}
            \quad \text{as} \quad
            s \rightarrow \infty
        \end{aligned}
    \end{equation}
    Thus, we find the following.
    \begin{equation}
        (C) \lesssim \frac{\overline{\sigma}^2_{\varepsilon}}{\mathfrak{p}\left(1 - \mathfrak{p}\right)} + o(1)
    \end{equation}
    Combining these findings, we obtain the following.
    \begin{equation}
        \begin{aligned}
            \Omega_{s}\left(x\right)
            & = \E\left[h_{s}^{2}\left(x; Z_1, \ldots,  Z_{s}\right)\right]
            \longrightarrow \left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2 + \frac{\sigma_{\varepsilon}^2\left(x\right)}{\pi(x)\left(1 - \pi(x)\right)}
            \quad \text{as} \quad
            s \rightarrow \infty
        \end{aligned}
    \end{equation}
    This gives us the desired result.
    \begin{equation}
        \Omega_{s}\left(x\right)
        \lesssim \left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2 + \frac{\overline{\sigma}^2_{\varepsilon}}{\mathfrak{p}\left(1 - \mathfrak{p}\right)} + o(1)
    \end{equation}
\end{proof}

\newpage
\begin{lem}\label{lem:CATE_omega_sc}\mbox{}\\*
	Let $D = \{Z_1, \dotsc, Z_{s}\}$ be a vector of i.i.d. random variables drawn from as described in Setup \ref{asm:hte_dgp}.
	Let $D^{\prime} = \{Z_1, \dotsc, Z_{c}, Z_{c+1}^{\prime}, \dotsc,  Z_{s}^{\prime}\}$ where $Z_{c+1}^{\prime}, \dotsc,  Z_{s}^{\prime}$ are i.i.d. draws from the model that are independent of $D$.
	Furthermore, let
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		= \E\left[h_{s}\left(x; Z_1, \ldots, Z_{c}, Z_{c+1}, \ldots, Z_{s}\right) \cdot
			h_{s}\left(x; Z_1, \ldots,Z_{c}, Z_{c+1}^{\prime}, \ldots, Z_{s}^{\prime}\right)\right].
	\end{equation}
	Then,
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		\lesssim 
	\end{equation}
	and thus
	\begin{equation}
		\Omega_{s}^{c}\left(x\right)
		\lesssim 
	\end{equation}
\end{lem}
\hrule
\begin{proof}[Proof of Lemma \ref{lem:CATE_omega_sc}]
    First, we decompose the term of interest in a similar fashion to before.
	\begin{equation}
        \begin{aligned}
            \Omega_{s}^{c}\left(x\right)
		    & = \E\left[h_{s}\left(x; Z_1, \ldots, Z_{c}, Z_{c+1}, \ldots, Z_{s}\right) \cdot
			h_{s}\left(x; Z_1, \ldots,Z_{c}, Z_{c+1}^{\prime}, \ldots, Z_{s}^{\prime}\right)\right]\\
            %
            & = \E_{D, D^{\prime}}\left[
                \left(\sum_{i = 1}^{s} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right) 
                \left(\sum_{j = 1}^{c} \kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{j}; \mu, p\right) + \sum_{j = c + 1}^{s} \kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right) m\left(Z_{j}^{\prime}; \mu, p\right)\right)
            \right]\\
            %
            & = \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = 1}^{c} \kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{j}; \mu, p\right)\right)\right]}_{(A)} \\
            & \quad + \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = c + 1}^{s} \kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right) m\left(Z_{j}^{\prime}; \mu, p\right)\right)\right]}_{(B)} \\
            & \quad + \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = c + 1}^{s} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = 1}^{c} \kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{j}; \mu, p\right)\right)\right]}_{(C)} \\
            & \quad + \underbrace{\E_{D, D^{\prime}}\left[\left(\sum_{i = c + 1}^{s} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = c + 1}^{s} \kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right) m\left(Z_{j}^{\prime}; \mu, p\right)\right)\right]}_{(D)} 
        \end{aligned}
    \end{equation}
    Considering these terms one by one, we can make the following observations.
    \begin{equation}
        \begin{aligned}
            (A)
            & = \E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = 1}^{c} \kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{j}; \mu, p\right)\right)\right] \\
			%
			& = \E_{D, D^{\prime}}\left[
				\sum_{i = 1}^{c} \sum_{j = 1}^{c} \kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{i}; \mu, p\right)m\left(Z_{j}; \mu, p\right)
			\right] \\
			& = \E_{D,D^{\prime}}\left[c \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)m^{2}\left(Z_{1}; \mu, p\right)\right]
			= \E_{1}\left[m^{2}\left(Z_{1}; \mu, p\right) c \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right)\right]\right]\\
			%
			& = \frac{c}{s} \cdot \E_{1}\left[\E\left[m^{2}\left(Z_{1}; \mu, p\right) \; \middle| \; X_1\right] s \left\{1 - \varphi\left(B\left(x, \|X_1 - x\|\right)\right)\right\}^{2s - c- 1}\right]\\
			%
			& \leq \frac{c}{s} \cdot \E_{1}\left[\E\left[m^{2}\left(Z_{1}; \mu, p\right) \; \middle| \; X_1\right] s \left\{1 - \varphi\left(B\left(x, \|X_1 - x\|\right)\right)\right\}^{s - 1}\right]
        \end{aligned}
    \end{equation}
	Using results established in the proof of Lemma \ref{lem:CATE_omega_s}, we can find the following.
	\begin{equation}
		\begin{aligned}
			(A) 
			& \lesssim \frac{c}{s} \left[\left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2 + \frac{\overline{\sigma}^2_{\varepsilon}}{\mathfrak{p}\left(1 - \mathfrak{p}\right)}\right]  + o(1)
		\end{aligned}
	\end{equation}
	Similarly, for the second term, we can make the following observation.
	\begin{equation}
		\begin{aligned}
			(B) 
			& = \E_{D, D^{\prime}}\left[\left(\sum_{i = 1}^{c} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = c + 1}^{s} \kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right) m\left(Z_{j}^{\prime}; \mu, p\right)\right)\right] \\
			%
			& = \E_{D,D^{\prime}}\left[\sum_{i = 1}^{c} \sum_{j = c + 1}^{s}\kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right) m\left(Z_{i}; \mu, p\right)m\left(Z_{j}^{\prime}; \mu, p\right)\right]\\
			%
			& = \E_{D, D^{\prime}}\left[c (s-c) \kappa\left(x; Z_{1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right) m\left(Z_{1}; \mu, p\right) m\left(Z_{c+1}^{\prime}; \mu, p\right)\right]\\
			%
			& \leq \E_{D}\left[c \kappa\left(x; Z_{1}, D\right)  \left|m\left(Z_{1}; \mu, p\right)\right|\right]
			\E_{D^{\prime}}\left[(s-c) \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)  \left| m\left(Z_{c+1}^{\prime}; \mu, p\right) \right|\right]\\
			%
			& = \frac{c(s-c)}{s^2} \cdot \left(\E_{1}\left[\left|m\left(Z_{1}; \mu, p\right)\right| s \E_{2:s}\left[\kappa\left(x; Z_{1}, D\right)\right]\right]\right)^2\\
			%
			& \lesssim \frac{c(s-c)}{s^2}\left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2  + o(1)
		\end{aligned}
	\end{equation}
	Applying the same principles to the third term we find a similar result.
	\begin{equation}
		\begin{aligned}
			(C)
			& = \E_{D, D^{\prime}}\left[\left(\sum_{i = c + 1}^{s} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = 1}^{c} \kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{j}; \mu, p\right)\right)\right]\\
			%
			% & = \E_{D, D^{\prime}}\left[\sum_{i = c+1}^{s}\sum_{j = 1}^{c}
			% \kappa\left(x; Z_{i}, D\right)\kappa\left(x; Z_{j}, D^{\prime}\right) m\left(Z_{i}; \mu, p\right)m\left(Z_{j}; \mu, p\right)\right] \\
			% %
			% & = \E_{D, D^{\prime}}\left[(s-c)c
			% \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{1}, D^{\prime}\right) m\left(Z_{c+1}; \mu, p\right)m\left(Z_{1}; \mu, p\right)\right] \\
			% %
			% & \leq \frac{(s-c)c}{s^2} \cdot \E_{D}\left[\left| m\left(Z_{c+1}; \mu, p\right) \right| s\kappa\left(x; Z_{c+1}, D\right)\right]
			% \E_{D^{\prime}}\left[\left|m\left(Z_{1}; \mu, p\right)\right| s\kappa\left(x; Z_{1}, D^{\prime}\right) \right] \\
			% %
			% & = \frac{(s-c)c}{s^2} \cdot \left(\E_{1}\left[\left| m\left(Z_{c+1}; \mu, p\right) \right| s \E_{2:s}\left[\kappa\left(x; Z_{c+1}, D\right)\right]\right]\right)^2\\
			% %
			& \lesssim \frac{c(s-c)}{s^2}\left(\mu_{1}\left(x\right) - \mu_{0}\left(x\right)\right)^2  + o(1)
		\end{aligned}
	\end{equation}
	\begin{equation}
		\begin{aligned}
			(D)
			& = \E_{D, D^{\prime}}\left[\left(\sum_{i = c + 1}^{s} \kappa\left(x; Z_{i}, D\right) m\left(Z_{i}; \mu, p\right)\right)
            \left(\sum_{j = c + 1}^{s} \kappa\left(x; Z_{j}^{\prime}, D^{\prime}\right) m\left(Z_{j}^{\prime}; \mu, p\right)\right)\right] \\
			%
			& = \E_{D, D^{\prime}}\left[(s-c)^2 \kappa\left(x; Z_{c+1}, D\right)\kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right)
			 m\left(Z_{c+1}; \mu, p\right)  m\left(Z_{c+1}^{\prime}; \mu, p\right)\right] \\
			 %
			 & \leq \frac{(s-c)^2}{s^2} \cdot \E_{D}\left[\left|m\left(Z_{c+1}; \mu, p\right)\right| s \kappa\left(x; Z_{c+1}, D\right)\right]
			 \E_{D^{\prime}}\left[\left|m\left(Z_{c+1}^{\prime}; \mu, p\right)\right| s \kappa\left(x; Z_{c+1}^{\prime}, D^{\prime}\right) \right]\\
			 %
			 & = \frac{(s-c)^2}{s^2} \left(\E_{1}\left[\left|m\left(Z_{1}; \mu, p\right)\right| s \E_{2:s}\left[\kappa\left(x; Z_{c+1}, D\right)\right]\right]\right)^2
		\end{aligned}
	\end{equation}
\end{proof}